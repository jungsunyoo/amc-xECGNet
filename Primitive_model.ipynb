{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numba/errors.py:137: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow.contrib.eager as tfe\n",
    "\n",
    "\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import keras\n",
    "# import datetime as dt\n",
    "from datetime import datetime\n",
    "import time\n",
    "# import datetime.datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "# from keras import optimizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from keras.applications.densenet import DenseNet121, DenseNet169\n",
    "#from keras.applications.nasnet import NASNetLarge\n",
    "# from keras_efficientnets import EfficientNetB7\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "random.seed(100)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.24290386e-03 -4.58280585e-05  4.31697309e-03 -3.00174693e-03\n",
      " -2.36609229e-04  1.28997408e-03  2.17347589e-04 -7.99152384e-04\n",
      " -3.42993744e-03 -1.69711686e-03  1.27138164e-03  1.94670545e-03]\n",
      "(4864,) (256,) (1281,)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def score_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "def get_unique_classes(input_directory,files):\n",
    "\n",
    "    unique_classes=set()\n",
    "    for f in files:\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    for c in tmp:\n",
    "                        unique_classes.add(c.strip())\n",
    "    return sorted(unique_classes)\n",
    "\n",
    "def one_hot_encoding(one_hot_vector,y, class2index):\n",
    "    ind=class2index[y]\n",
    "    one_hot_vector[ind]=1\n",
    "    return one_hot_vector\n",
    "\n",
    "# Search for multi-label subjects\n",
    "def searching_overlap(input_directory,class2index, input_file_names):\n",
    "    multiclasses=[]\n",
    "    multisubjects=[]\n",
    "    number = []\n",
    "    for file in input_file_names:\n",
    "        f=file\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    if len(tmp)>1:\n",
    "                        one_hot_vector = [0]*(len(class2index))\n",
    "                        for c in tmp:\n",
    "                            one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                        multiclasses.append(one_hot_vector)\n",
    "                        multisubjects.append(g)\n",
    "                        number.append(len(tmp))\n",
    "    return multisubjects, multiclasses, number\n",
    "\n",
    "def block_feature(sequence_en, minimum_len): \n",
    "    new_en = []\n",
    "    if len(sequence_en) > minimum_len:  # 길이가 minimum보다 긴 경우\n",
    "        start = random.randint(0,len(sequence_en)-minimum_len)\n",
    "        #print(start)\n",
    "        new_en = sequence_en[start:start+minimum_len]\n",
    "    elif len(sequence_en) == minimum_len: # 길이가 minimum\n",
    "        new_en = sequence_en\n",
    "    else: \n",
    "        assert len(sequence_en) <= minimum_len\n",
    "    return new_en\n",
    "\n",
    "def exploratory_look(input_directory,file, class2index):\n",
    "    classes = []\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                print(tmp, len(tmp))\n",
    "    return tmp     \n",
    "\n",
    "# Get classes of sorted file names\n",
    "def get_labels(input_directory,file, class2index):\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                one_hot_vector = [0]*(len(class2index))\n",
    "                for c in tmp:\n",
    "                    one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                \n",
    "    return one_hot_vector\n",
    "\n",
    "def randextract_mels(curr_step, batch_size, data, mel_directory, class2index, minimum_len, x_mean_final, x_std_final):\n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    start = batch_size*curr_step\n",
    "    end = batch_size*(curr_step+1)\n",
    "    curr_file_indices = data[start:end]\n",
    "    for file in curr_file_indices:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        clip_file = block_feature(tmp_file, minimum_len)\n",
    "        #print(clip_file.shape)\n",
    "        #clip_file = tmp_file[:minimum_len]\n",
    "        clip_file -= x_mean_final\n",
    "        clip_file /= x_std_final\n",
    "        mel_files.append(clip_file)\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        classes.append(label)\n",
    "    \n",
    "    # YJS added for ABN -> should calculate 2 losses\n",
    "#     classes_abn = [classes,classes]\n",
    "    \n",
    "    concat = list(zip(mel_files, classes))\n",
    "#     concat = list(zip(mel_files, classes_abn))\n",
    "    random.shuffle(concat)\n",
    "    mel_files, classes = zip(*concat)\n",
    "    return mel_files, classes\n",
    "\n",
    "def train(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final): \n",
    "    loss=[]\n",
    "#     acc = []\n",
    "    f1 = []\n",
    "\n",
    "    total_steps = int(np.ceil(len(data_train)/batch_size))\n",
    "    for curr_step in range(total_steps):\n",
    "        batch_mels, batch_labels = randextract_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "        batch_mels = np.asarray(batch_mels)\n",
    "        batch_labels2 = np.asarray(np.squeeze(batch_labels))\n",
    "        batch_labels = [batch_labels2, batch_labels2]\n",
    "#         print(batch_labels.shape)\n",
    "#         print(batch_labels)\n",
    "\n",
    "# return of train\n",
    "# 0 = total loss (attention branch + perception branch)\n",
    "# 1 = loss of attention pred\n",
    "# 2 = loss of perception pred\n",
    "# 3 = f1 of attention pred\n",
    "# 4 = f1 of perception pred     \n",
    "\n",
    "        train_tmp = model.train_on_batch(batch_mels, batch_labels)\n",
    "#         print(train_tmp)\n",
    "        loss_ = train_tmp[0]/2\n",
    "        f1_ = np.mean((train_tmp[3], train_tmp[4]))\n",
    "        loss.append(loss_)\n",
    "#         acc.append(train_tmp[1])\n",
    "        f1.append(f1_)\n",
    "\n",
    "    loss = np.mean(np.array(loss))\n",
    "#     acc = np.mean(np.array(acc))\n",
    "    f1 = np.mean(np.array(f1))\n",
    "    return loss, f1\n",
    "\n",
    "def train_cam(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final): \n",
    "    loss=[]\n",
    "#     acc = []\n",
    "    f1 = []\n",
    "\n",
    "    total_steps = int(np.ceil(len(data_train)/batch_size))\n",
    "    for curr_step in range(total_steps):\n",
    "        batch_mels, batch_labels = randextract_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "        batch_mels = np.asarray(batch_mels)\n",
    "        \n",
    "        batch_labels = np.asarray(np.squeeze(batch_labels))\n",
    "#         batch_labels2 = np.asarray(np.squeeze(batch_labels))\n",
    "#         batch_labels = [batch_labels2, batch_labels2]\n",
    "# #         print(batch_labels.shape)\n",
    "#         print(batch_labels)\n",
    "\n",
    "# return of train\n",
    "# 0 = total loss (attention branch + perception branch)\n",
    "# 1 = loss of attention pred\n",
    "# 2 = loss of perception pred\n",
    "# 3 = f1 of attention pred\n",
    "# 4 = f1 of perception pred     \n",
    "\n",
    "        train_tmp = model.train_on_batch(batch_mels, batch_labels)\n",
    "#         print(train_tmp)\n",
    "        loss_ = train_tmp[0]\n",
    "        f1_ = train_tmp[1]\n",
    "        loss.append(loss_)\n",
    "#         acc.append(train_tmp[1])\n",
    "        f1.append(f1_)\n",
    "\n",
    "    loss = np.mean(np.array(loss))\n",
    "#     acc = np.mean(np.array(acc))\n",
    "    f1 = np.mean(np.array(f1))\n",
    "    return loss, f1\n",
    "\n",
    "def test_cam(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    for i, file in enumerate(data):\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "        for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "            mel_files.append(clip_file)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "        logit = model.predict(mel_files)\n",
    "#         print(len(logit))\n",
    "        logit = np.mean(logit, axis=0)\n",
    "#         logit = np.mean(logit, axis=0)\n",
    "#         print(logit)\n",
    "        pred = np.argmax(logit)\n",
    "#         print('Pred={}'.format(pred))\n",
    "        \n",
    "        label = np.argmax(get_labels(input_directory, file, class2index))\n",
    "#         print('Label={}'.format(label))\n",
    "        #f1 = f1_score(label, logit)\n",
    "        #print(pred, label)\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        #total_f1 += f1\n",
    "    final_acc = total_acc / i\n",
    "    #final_f1 = total_f1 / i\n",
    "    return final_acc#, final_f1\n",
    "def test_short(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    n_channels=12\n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    for i, file in enumerate(data):\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "#         gradcam_files = []\n",
    "        heatmap_files=[]\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        label = [label]\n",
    "#         for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "        steps=1\n",
    "        block = 0\n",
    "        start = block*minimum_len\n",
    "        end = (block+1)*minimum_len\n",
    "        clip_file = tmp_file[start:end]\n",
    "        clip_file -= x_mean_final\n",
    "        clip_file /= x_std_final\n",
    "\n",
    "        clip_file = clip_file.reshape(1,minimum_len,n_channels)\n",
    "#         heatmap = CAM_conv1D(p_model, conv_layer, softmax_layer,  x_mean_final, x_std_final, minimum_len, \n",
    "#                                       n_channels, clip_file, label, out_len)\n",
    "\n",
    "        mel_files.append(clip_file)    \n",
    "#         heatmap_files.append(heatmap)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "#         heatmap_files = np.asarray(heatmap_files)\n",
    "#         heatmap_files = heatmap_files.reshape(steps,out_len,1) # changed for modified attention editting\n",
    "        mel_files = mel_files.reshape(steps,minimum_len,n_channels)\n",
    "\n",
    "        logit = model.predict(mel_files)\n",
    "        # YJS changed on 2020-06-02: input으로 두개 들어가야하니까 predict도 수정?\n",
    "\n",
    "        \n",
    "        logit = np.mean(logit, axis=0)\n",
    "#         logit = np.mean(logit, axis=0)\n",
    "        pred = np.argmax(logit)\n",
    "        \n",
    "        label = np.argmax(get_labels(input_directory, file, class2index))\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "\n",
    "        total_acc += acc\n",
    "\n",
    "    final_acc = total_acc / i\n",
    "\n",
    "    return final_acc#, final_f1\n",
    "batch_size = 32#16#20#32#5#2#1#10#32\n",
    "\n",
    "minimum_len = 2880\n",
    "epochs = 1000\n",
    "loss_function = 'categorical_crossentropy'\n",
    "activation_function = 'softmax'\n",
    "rootdir = '../'\n",
    "date = datetime.today().strftime(\"%Y%m%d\")\n",
    "input_directory = os.path.join(rootdir, 'Training_WFDB')\n",
    "mel_name = 'Raw_data_20200424' \n",
    "mel_directory = os.path.join(rootdir, mel_name)\n",
    "results_directory = os.path.join(rootdir, 'results_'+date+'_1_CAM_primitive_model')\n",
    "if not os.path.isdir(input_directory):\n",
    "    os.mkdir(input_directory)\n",
    "if not os.path.isdir(mel_directory):\n",
    "    os.mkdir(mel_directory)\n",
    "if not os.path.isdir(results_directory):\n",
    "    os.mkdir(results_directory)\n",
    "        \n",
    "input_files = []\n",
    "for f in os.listdir(input_directory):\n",
    "    if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('mat'):\n",
    "        input_files.append(f)\n",
    "input_file_names = sorted(input_files)\n",
    "\n",
    "unique_classes = get_unique_classes(input_directory, input_files)\n",
    "class2index = {}\n",
    "for a, b in enumerate(unique_classes):\n",
    "    class2index[b] = a\n",
    "    \n",
    "classes_orig= [x.replace('.mat', '.hea') for x in input_file_names] # total subjects\n",
    "classes_multi, _, _ = searching_overlap(input_directory,class2index, input_file_names)\n",
    "classes_single = [x for x in classes_orig if x not in classes_multi]\n",
    "classes_single = [x.replace('.hea', '.mat') for x in classes_single]\n",
    "\n",
    "# double-checking if classes_single have single-label\n",
    "a, b, c  = searching_overlap(input_directory,class2index,classes_single)\n",
    "\n",
    "# we can safely use classes_single as input_file_names\n",
    "input_file_names = classes_single\n",
    "random.shuffle(input_file_names)\n",
    "np.shape(input_file_names)\n",
    "\n",
    "x_mean_all = []\n",
    "x_std_all = []\n",
    "for file in input_file_names:\n",
    "    x = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "    x_mean = [np.mean(x[:,0]), np.mean(x[:,1]), np.mean(x[:,2]), np.mean(x[:,3]), np.mean(x[:,4]), np.mean(x[:,5]),\n",
    "             np.mean(x[:,6]), np.mean(x[:,7]), np.mean(x[:,8]), np.mean(x[:,9]), np.mean(x[:,10]), np.mean(x[:,11])]\n",
    "    \n",
    "    x_std = [np.std(x[:,0]), np.std(x[:,1]), np.std(x[:,2]), np.std(x[:,3]), np.std(x[:,4]), np.std(x[:,5]),\n",
    "             np.std(x[:,6]), np.std(x[:,7]), np.std(x[:,8]), np.std(x[:,9]), np.std(x[:,10]), np.std(x[:,11])]\n",
    "    #print(x_mean)\n",
    "    x_mean_all.append(x_mean)\n",
    "    x_std_all.append(x_std) # yjs corrected on 2020-05-25\n",
    "x_mean_final = np.mean(x_mean_all, axis=0)\n",
    "x_std_final = np.mean(x_std_all, axis=0)\n",
    "print(x_mean_final)\n",
    "\n",
    "data, data_test = train_test_split(input_file_names, test_size = 0.2, train_size = 0.8, shuffle=True)\n",
    "# data_train, data_val = train_test_split(data, test_size = 0.25, train_size = 0.75, shuffle=True)\n",
    "data_train, data_val = train_test_split(data, test_size = 0.05, train_size = 0.95, shuffle=True)\n",
    "print(np.shape(data_train), np.shape(data_val), np.shape(data_test))\n",
    "\n",
    "\n",
    "main_input = Input(shape=(minimum_len,12), dtype='float32', name='main_input')\n",
    "\n",
    "branch_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_image (InputLayer)     [(None, None, 12)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 64)          2368      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, None, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, None, 256)         98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, None, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, None, 256)         196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, None, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, None, 256)         196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, None, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, None, 512)         393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, None, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, None, 512)         786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, None, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, None, 512)         786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, None, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, None, 512)         786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, None, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, None, 256)         393472    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, None, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, None, 128)         98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_final (Dense)          (None, 9)                 1161      \n",
      "_________________________________________________________________\n",
      "output (Softmax)             (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 3,842,953\n",
      "Trainable params: 3,835,785\n",
      "Non-trainable params: 7,168\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from Primitive_modules import *\n",
    "\n",
    "# def get_custom_model(input_shape, n_classes, minimum_len, target_classes, out_ch=256, n=18):\n",
    "# model = get_custom_model((None, 12), 9, minimum_len, 1, out_ch=256, n=18)\n",
    "# model = get_model((None, 12), 9, n=7)\n",
    "\n",
    "model = cam_model((None, 12), 9, minimum_len, out_ch=256, n=1)\n",
    "model.summary()\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=optimizers.Adam(lr=1e-5),           \n",
    "              metrics=[score_f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results_20200612_1_CAM_primitive_model\n",
      "\n",
      "Epoch 1 train_loss: 1.660 train_f1: 0.246 \t\n",
      "\n",
      "Validation 1 valid_acc: 0.463 best_acc: 0.463 \t\n",
      "\n",
      "Epoch 2 train_loss: 1.341 train_f1: 0.432 \t\n",
      "\n",
      "Validation 2 valid_acc: 0.529 best_acc: 0.529 \t\n",
      "\n",
      "Epoch 3 train_loss: 1.196 train_f1: 0.516 \t\n",
      "\n",
      "Validation 3 valid_acc: 0.584 best_acc: 0.584 \t\n",
      "\n",
      "Epoch 4 train_loss: 1.100 train_f1: 0.582 \t\n",
      "\n",
      "Validation 4 valid_acc: 0.643 best_acc: 0.643 \t\n",
      "\n",
      "Epoch 5 train_loss: 1.034 train_f1: 0.624 \t\n",
      "\n",
      "Validation 5 valid_acc: 0.671 best_acc: 0.671 \t\n",
      "\n",
      "Epoch 6 train_loss: 0.976 train_f1: 0.655 \t\n",
      "\n",
      "Validation 6 valid_acc: 0.702 best_acc: 0.702 \t\n",
      "\n",
      "Epoch 7 train_loss: 0.941 train_f1: 0.674 \t\n",
      "\n",
      "Validation 7 valid_acc: 0.702 best_acc: 0.702 \t\n",
      "\n",
      "Epoch 8 train_loss: 0.909 train_f1: 0.680 \t\n",
      "\n",
      "Validation 8 valid_acc: 0.710 best_acc: 0.710 \t\n",
      "\n",
      "Epoch 9 train_loss: 0.882 train_f1: 0.685 \t\n",
      "\n",
      "Validation 9 valid_acc: 0.714 best_acc: 0.714 \t\n",
      "\n",
      "Epoch 10 train_loss: 0.874 train_f1: 0.694 \t\n",
      "\n",
      "Validation 10 valid_acc: 0.722 best_acc: 0.722 \t\n",
      "\n",
      "Epoch 11 train_loss: 0.846 train_f1: 0.705 \t\n",
      "\n",
      "Validation 11 valid_acc: 0.761 best_acc: 0.761 \t\n",
      "\n",
      "Epoch 12 train_loss: 0.816 train_f1: 0.711 \t\n",
      "\n",
      "Validation 12 valid_acc: 0.682 best_acc: 0.761 \t\n",
      "\n",
      "Epoch 13 train_loss: 0.824 train_f1: 0.709 \t\n",
      "\n",
      "Validation 13 valid_acc: 0.737 best_acc: 0.761 \t\n",
      "\n",
      "Epoch 14 train_loss: 0.809 train_f1: 0.717 \t\n",
      "\n",
      "Validation 14 valid_acc: 0.737 best_acc: 0.761 \t\n",
      "\n",
      "Epoch 15 train_loss: 0.760 train_f1: 0.737 \t\n",
      "\n",
      "Validation 15 valid_acc: 0.741 best_acc: 0.761 \t\n",
      "\n",
      "Epoch 16 train_loss: 0.771 train_f1: 0.735 \t\n",
      "\n",
      "Validation 16 valid_acc: 0.702 best_acc: 0.761 \t\n",
      "\n",
      "Epoch 17 train_loss: 0.753 train_f1: 0.743 \t\n",
      "\n",
      "Validation 17 valid_acc: 0.706 best_acc: 0.761 \t\n",
      "\n",
      "Epoch 18 train_loss: 0.739 train_f1: 0.745 \t\n",
      "\n",
      "Validation 18 valid_acc: 0.749 best_acc: 0.761 \t\n",
      "\n",
      "Epoch 19 train_loss: 0.720 train_f1: 0.749 \t\n",
      "\n",
      "Validation 19 valid_acc: 0.733 best_acc: 0.761 \t\n",
      "\n",
      "Epoch 20 train_loss: 0.725 train_f1: 0.757 \t\n",
      "\n",
      "Validation 20 valid_acc: 0.706 best_acc: 0.761 \t\n",
      "\n",
      "Epoch 21 train_loss: 0.714 train_f1: 0.750 \t\n",
      "\n",
      "Validation 21 valid_acc: 0.733 best_acc: 0.761 \t\n",
      "\n",
      "Epoch 22 train_loss: 0.693 train_f1: 0.763 \t\n",
      "\n",
      "Validation 22 valid_acc: 0.773 best_acc: 0.773 \t\n",
      "\n",
      "Epoch 23 train_loss: 0.686 train_f1: 0.766 \t\n",
      "\n",
      "Validation 23 valid_acc: 0.741 best_acc: 0.773 \t\n",
      "\n",
      "Epoch 24 train_loss: 0.687 train_f1: 0.762 \t\n",
      "\n",
      "Validation 24 valid_acc: 0.725 best_acc: 0.773 \t\n",
      "\n",
      "Epoch 25 train_loss: 0.682 train_f1: 0.772 \t\n",
      "\n",
      "Validation 25 valid_acc: 0.741 best_acc: 0.773 \t\n",
      "\n",
      "Epoch 26 train_loss: 0.669 train_f1: 0.771 \t\n",
      "\n",
      "Validation 26 valid_acc: 0.761 best_acc: 0.773 \t\n",
      "\n",
      "Epoch 27 train_loss: 0.659 train_f1: 0.775 \t\n",
      "\n",
      "Validation 27 valid_acc: 0.745 best_acc: 0.773 \t\n",
      "\n",
      "Epoch 28 train_loss: 0.646 train_f1: 0.783 \t\n",
      "\n",
      "Validation 28 valid_acc: 0.753 best_acc: 0.773 \t\n",
      "\n",
      "Epoch 29 train_loss: 0.640 train_f1: 0.780 \t\n",
      "\n",
      "Validation 29 valid_acc: 0.725 best_acc: 0.773 \t\n",
      "\n",
      "Epoch 30 train_loss: 0.647 train_f1: 0.780 \t\n",
      "\n",
      "Validation 30 valid_acc: 0.745 best_acc: 0.773 \t\n",
      "\n",
      "Epoch 31 train_loss: 0.610 train_f1: 0.789 \t\n",
      "\n",
      "Validation 31 valid_acc: 0.718 best_acc: 0.773 \t\n",
      "\n",
      "Epoch 32 train_loss: 0.622 train_f1: 0.786 \t\n",
      "\n",
      "Validation 32 valid_acc: 0.710 best_acc: 0.773 \t\n",
      "\n",
      "Epoch 33 train_loss: 0.612 train_f1: 0.793 \t\n",
      "\n",
      "Validation 33 valid_acc: 0.749 best_acc: 0.773 \t\n",
      "\n",
      "Epoch 34 train_loss: 0.608 train_f1: 0.794 \t\n",
      "\n",
      "Validation 34 valid_acc: 0.776 best_acc: 0.776 \t\n",
      "\n",
      "Epoch 35 train_loss: 0.590 train_f1: 0.800 \t\n",
      "\n",
      "Validation 35 valid_acc: 0.769 best_acc: 0.776 \t\n",
      "\n",
      "Epoch 36 train_loss: 0.590 train_f1: 0.797 \t\n",
      "\n",
      "Validation 36 valid_acc: 0.757 best_acc: 0.776 \t\n",
      "\n",
      "Epoch 37 train_loss: 0.581 train_f1: 0.802 \t\n",
      "\n",
      "Validation 37 valid_acc: 0.761 best_acc: 0.776 \t\n",
      "\n",
      "Epoch 38 train_loss: 0.579 train_f1: 0.804 \t\n",
      "\n",
      "Validation 38 valid_acc: 0.769 best_acc: 0.776 \t\n",
      "\n",
      "Epoch 39 train_loss: 0.575 train_f1: 0.803 \t\n",
      "\n",
      "Validation 39 valid_acc: 0.757 best_acc: 0.776 \t\n",
      "\n",
      "Epoch 40 train_loss: 0.568 train_f1: 0.806 \t\n",
      "\n",
      "Validation 40 valid_acc: 0.776 best_acc: 0.776 \t\n",
      "\n",
      "Epoch 41 train_loss: 0.555 train_f1: 0.812 \t\n",
      "\n",
      "Validation 41 valid_acc: 0.757 best_acc: 0.776 \t\n",
      "\n",
      "Epoch 42 train_loss: 0.546 train_f1: 0.816 \t\n",
      "\n",
      "Validation 42 valid_acc: 0.761 best_acc: 0.776 \t\n",
      "\n",
      "Epoch 43 train_loss: 0.565 train_f1: 0.809 \t\n",
      "\n",
      "Validation 43 valid_acc: 0.761 best_acc: 0.776 \t\n",
      "\n",
      "Epoch 44 train_loss: 0.537 train_f1: 0.814 \t\n",
      "\n",
      "Validation 44 valid_acc: 0.745 best_acc: 0.776 \t\n",
      "\n",
      "Epoch 45 train_loss: 0.536 train_f1: 0.817 \t\n",
      "\n",
      "Validation 45 valid_acc: 0.761 best_acc: 0.776 \t\n",
      "\n",
      "Epoch 46 train_loss: 0.526 train_f1: 0.824 \t\n",
      "\n",
      "Validation 46 valid_acc: 0.745 best_acc: 0.776 \t\n",
      "\n",
      "Epoch 47 train_loss: 0.537 train_f1: 0.821 \t\n",
      "\n",
      "Validation 47 valid_acc: 0.753 best_acc: 0.776 \t\n",
      "\n",
      "Epoch 48 train_loss: 0.513 train_f1: 0.821 \t\n",
      "\n",
      "Validation 48 valid_acc: 0.729 best_acc: 0.776 \t\n",
      "\n",
      "Epoch 49 train_loss: 0.503 train_f1: 0.830 \t\n",
      "\n",
      "Validation 49 valid_acc: 0.780 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 50 train_loss: 0.498 train_f1: 0.829 \t\n",
      "\n",
      "Validation 50 valid_acc: 0.761 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 51 train_loss: 0.503 train_f1: 0.830 \t\n",
      "\n",
      "Validation 51 valid_acc: 0.765 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 52 train_loss: 0.506 train_f1: 0.829 \t\n",
      "\n",
      "Validation 52 valid_acc: 0.749 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 53 train_loss: 0.494 train_f1: 0.833 \t\n",
      "\n",
      "Validation 53 valid_acc: 0.780 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 54 train_loss: 0.480 train_f1: 0.841 \t\n",
      "\n",
      "Validation 54 valid_acc: 0.757 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 55 train_loss: 0.499 train_f1: 0.830 \t\n",
      "\n",
      "Validation 55 valid_acc: 0.737 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 56 train_loss: 0.491 train_f1: 0.835 \t\n",
      "\n",
      "Validation 56 valid_acc: 0.761 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 57 train_loss: 0.481 train_f1: 0.835 \t\n",
      "\n",
      "Validation 57 valid_acc: 0.773 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 58 train_loss: 0.491 train_f1: 0.834 \t\n",
      "\n",
      "Validation 58 valid_acc: 0.741 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 59 train_loss: 0.471 train_f1: 0.839 \t\n",
      "\n",
      "Validation 59 valid_acc: 0.753 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 60 train_loss: 0.450 train_f1: 0.847 \t\n",
      "\n",
      "Validation 60 valid_acc: 0.773 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 61 train_loss: 0.435 train_f1: 0.855 \t\n",
      "\n",
      "Validation 61 valid_acc: 0.773 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 62 train_loss: 0.457 train_f1: 0.846 \t\n",
      "\n",
      "Validation 62 valid_acc: 0.749 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 63 train_loss: 0.435 train_f1: 0.852 \t\n",
      "\n",
      "Validation 63 valid_acc: 0.769 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 64 train_loss: 0.448 train_f1: 0.848 \t\n",
      "\n",
      "Validation 64 valid_acc: 0.761 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 65 train_loss: 0.442 train_f1: 0.856 \t\n",
      "\n",
      "Validation 65 valid_acc: 0.765 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 66 train_loss: 0.444 train_f1: 0.848 \t\n",
      "\n",
      "Validation 66 valid_acc: 0.776 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 67 train_loss: 0.436 train_f1: 0.853 \t\n",
      "\n",
      "Validation 67 valid_acc: 0.745 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 68 train_loss: 0.432 train_f1: 0.853 \t\n",
      "\n",
      "Validation 68 valid_acc: 0.741 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 69 train_loss: 0.430 train_f1: 0.857 \t\n",
      "\n",
      "Validation 69 valid_acc: 0.737 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 70 train_loss: 0.417 train_f1: 0.855 \t\n",
      "\n",
      "Validation 70 valid_acc: 0.753 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 71 train_loss: 0.427 train_f1: 0.856 \t\n",
      "\n",
      "Validation 71 valid_acc: 0.741 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 72 train_loss: 0.402 train_f1: 0.864 \t\n",
      "\n",
      "Validation 72 valid_acc: 0.722 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 73 train_loss: 0.412 train_f1: 0.863 \t\n",
      "\n",
      "Validation 73 valid_acc: 0.753 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 74 train_loss: 0.406 train_f1: 0.867 \t\n",
      "\n",
      "Validation 74 valid_acc: 0.741 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 75 train_loss: 0.400 train_f1: 0.867 \t\n",
      "\n",
      "Validation 75 valid_acc: 0.737 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 76 train_loss: 0.415 train_f1: 0.858 \t\n",
      "\n",
      "Validation 76 valid_acc: 0.749 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 77 train_loss: 0.390 train_f1: 0.869 \t\n",
      "\n",
      "Validation 77 valid_acc: 0.718 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 78 train_loss: 0.391 train_f1: 0.869 \t\n",
      "\n",
      "Validation 78 valid_acc: 0.741 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 79 train_loss: 0.389 train_f1: 0.865 \t\n",
      "\n",
      "Validation 79 valid_acc: 0.749 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 80 train_loss: 0.386 train_f1: 0.870 \t\n",
      "\n",
      "Validation 80 valid_acc: 0.749 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 81 train_loss: 0.370 train_f1: 0.874 \t\n",
      "\n",
      "Validation 81 valid_acc: 0.753 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 82 train_loss: 0.385 train_f1: 0.867 \t\n",
      "\n",
      "Validation 82 valid_acc: 0.745 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 83 train_loss: 0.374 train_f1: 0.875 \t\n",
      "\n",
      "Validation 83 valid_acc: 0.745 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 84 train_loss: 0.362 train_f1: 0.879 \t\n",
      "\n",
      "Validation 84 valid_acc: 0.745 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 85 train_loss: 0.353 train_f1: 0.882 \t\n",
      "\n",
      "Validation 85 valid_acc: 0.725 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 86 train_loss: 0.374 train_f1: 0.878 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 86 valid_acc: 0.737 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 87 train_loss: 0.355 train_f1: 0.878 \t\n",
      "\n",
      "Validation 87 valid_acc: 0.733 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 88 train_loss: 0.377 train_f1: 0.871 \t\n",
      "\n",
      "Validation 88 valid_acc: 0.769 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 89 train_loss: 0.368 train_f1: 0.879 \t\n",
      "\n",
      "Validation 89 valid_acc: 0.753 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 90 train_loss: 0.348 train_f1: 0.882 \t\n",
      "\n",
      "Validation 90 valid_acc: 0.725 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 91 train_loss: 0.356 train_f1: 0.880 \t\n",
      "\n",
      "Validation 91 valid_acc: 0.745 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 92 train_loss: 0.371 train_f1: 0.877 \t\n",
      "\n",
      "Validation 92 valid_acc: 0.741 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 93 train_loss: 0.342 train_f1: 0.888 \t\n",
      "\n",
      "Validation 93 valid_acc: 0.753 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 94 train_loss: 0.358 train_f1: 0.881 \t\n",
      "\n",
      "Validation 94 valid_acc: 0.765 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 95 train_loss: 0.358 train_f1: 0.879 \t\n",
      "\n",
      "Validation 95 valid_acc: 0.745 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 96 train_loss: 0.332 train_f1: 0.888 \t\n",
      "\n",
      "Validation 96 valid_acc: 0.749 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 97 train_loss: 0.307 train_f1: 0.900 \t\n",
      "\n",
      "Validation 97 valid_acc: 0.741 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 98 train_loss: 0.320 train_f1: 0.891 \t\n",
      "\n",
      "Validation 98 valid_acc: 0.741 best_acc: 0.780 \t\n",
      "\n",
      "Epoch 99 train_loss: 0.323 train_f1: 0.890 \t\n",
      "\n",
      "Validation 99 valid_acc: 0.761 best_acc: 0.780 \t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-057f68b6e0f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum_epoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_cam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass2index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_mean_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_std_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEpoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train_loss:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf'{train_loss:.3f}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train_f1:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf'{train_f1:.3f}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ECG_ABN_E%02dL%.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4ae3b9b827fd>\u001b[0m in \u001b[0;36mtrain_cam\u001b[0;34m(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;31m# 4 = f1 of perception pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mtrain_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;31m#         print(train_tmp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DISP DATETIME FOR CHECKING TIME\n",
    "start = time.time()\n",
    "val_acc_sum=[]\n",
    "train_loss_sum=[]\n",
    "train_acc_sum=[]\n",
    "val_loss_sum=[]\n",
    "val_acc_min = 0\n",
    "print(results_directory)\n",
    "\n",
    "for num_epoch in range(epochs):\n",
    "    random.shuffle(data_train)\n",
    "    train_loss, train_f1 = train_cam(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "    print('\\nEpoch',num_epoch+1,'train_loss:',f'{train_loss:.3f}','train_f1:',f'{train_f1:.3f}',\"\\t\")\n",
    "    model_output = \"ECG_ABN_E%02dL%.2f\" % (num_epoch, train_loss)\n",
    "    save_name = os.path.join(results_directory, model_output)\n",
    "    \n",
    "    val_acc = test_short(data_val, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)\n",
    "\n",
    "    if val_acc > val_acc_min:\n",
    "        val_acc_min = val_acc\n",
    "        model.save(save_name)\n",
    "    print('\\nValidation', num_epoch+1, 'valid_acc:',f'{val_acc:.3f}', 'best_acc:',f'{val_acc_min:.3f}', \"\\t\")\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
