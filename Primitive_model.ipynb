{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numba/errors.py:137: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow.contrib.eager as tfe\n",
    "\n",
    "\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import keras\n",
    "# import datetime as dt\n",
    "from datetime import datetime\n",
    "import time\n",
    "# import datetime.datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "# from keras import optimizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from keras.applications.densenet import DenseNet121, DenseNet169\n",
    "#from keras.applications.nasnet import NASNetLarge\n",
    "# from keras_efficientnets import EfficientNetB7\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "random.seed(100)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.24290386e-03 -4.58280585e-05  4.31697309e-03 -3.00174693e-03\n",
      " -2.36609229e-04  1.28997408e-03  2.17347589e-04 -7.99152384e-04\n",
      " -3.42993744e-03 -1.69711686e-03  1.27138164e-03  1.94670545e-03]\n",
      "(3840,) (1280,) (1281,)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def score_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "def get_unique_classes(input_directory,files):\n",
    "\n",
    "    unique_classes=set()\n",
    "    for f in files:\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    for c in tmp:\n",
    "                        unique_classes.add(c.strip())\n",
    "    return sorted(unique_classes)\n",
    "\n",
    "def one_hot_encoding(one_hot_vector,y, class2index):\n",
    "    ind=class2index[y]\n",
    "    one_hot_vector[ind]=1\n",
    "    return one_hot_vector\n",
    "\n",
    "# Search for multi-label subjects\n",
    "def searching_overlap(input_directory,class2index, input_file_names):\n",
    "    multiclasses=[]\n",
    "    multisubjects=[]\n",
    "    number = []\n",
    "    for file in input_file_names:\n",
    "        f=file\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    if len(tmp)>1:\n",
    "                        one_hot_vector = [0]*(len(class2index))\n",
    "                        for c in tmp:\n",
    "                            one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                        multiclasses.append(one_hot_vector)\n",
    "                        multisubjects.append(g)\n",
    "                        number.append(len(tmp))\n",
    "    return multisubjects, multiclasses, number\n",
    "\n",
    "def block_feature(sequence_en, minimum_len): \n",
    "    new_en = []\n",
    "    if len(sequence_en) > minimum_len:  # 길이가 minimum보다 긴 경우\n",
    "        start = random.randint(0,len(sequence_en)-minimum_len)\n",
    "        #print(start)\n",
    "        new_en = sequence_en[start:start+minimum_len]\n",
    "    elif len(sequence_en) == minimum_len: # 길이가 minimum\n",
    "        new_en = sequence_en\n",
    "    else: \n",
    "        assert len(sequence_en) <= minimum_len\n",
    "    return new_en\n",
    "\n",
    "def exploratory_look(input_directory,file, class2index):\n",
    "    classes = []\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                print(tmp, len(tmp))\n",
    "    return tmp     \n",
    "\n",
    "# Get classes of sorted file names\n",
    "def get_labels(input_directory,file, class2index):\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                one_hot_vector = [0]*(len(class2index))\n",
    "                for c in tmp:\n",
    "                    one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                \n",
    "    return one_hot_vector\n",
    "\n",
    "def randextract_mels(curr_step, batch_size, data, mel_directory, class2index, minimum_len, x_mean_final, x_std_final):\n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    start = batch_size*curr_step\n",
    "    end = batch_size*(curr_step+1)\n",
    "    curr_file_indices = data[start:end]\n",
    "    for file in curr_file_indices:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        clip_file = block_feature(tmp_file, minimum_len)\n",
    "        #print(clip_file.shape)\n",
    "        #clip_file = tmp_file[:minimum_len]\n",
    "        clip_file -= x_mean_final\n",
    "        clip_file /= x_std_final\n",
    "        mel_files.append(clip_file)\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        classes.append(label)\n",
    "    \n",
    "    # YJS added for ABN -> should calculate 2 losses\n",
    "#     classes_abn = [classes,classes]\n",
    "    \n",
    "    concat = list(zip(mel_files, classes))\n",
    "#     concat = list(zip(mel_files, classes_abn))\n",
    "    random.shuffle(concat)\n",
    "    mel_files, classes = zip(*concat)\n",
    "    return mel_files, classes\n",
    "\n",
    "def train(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final): \n",
    "    loss=[]\n",
    "#     acc = []\n",
    "    f1 = []\n",
    "\n",
    "    total_steps = int(np.ceil(len(data_train)/batch_size))\n",
    "    for curr_step in range(total_steps):\n",
    "        batch_mels, batch_labels = randextract_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "        batch_mels = np.asarray(batch_mels)\n",
    "        batch_labels2 = np.asarray(np.squeeze(batch_labels))\n",
    "        batch_labels = [batch_labels2, batch_labels2]\n",
    "#         print(batch_labels.shape)\n",
    "#         print(batch_labels)\n",
    "\n",
    "# return of train\n",
    "# 0 = total loss (attention branch + perception branch)\n",
    "# 1 = loss of attention pred\n",
    "# 2 = loss of perception pred\n",
    "# 3 = f1 of attention pred\n",
    "# 4 = f1 of perception pred     \n",
    "\n",
    "        train_tmp = model.train_on_batch(batch_mels, batch_labels)\n",
    "#         print(train_tmp)\n",
    "        loss_ = train_tmp[0]/2\n",
    "        f1_ = np.mean((train_tmp[3], train_tmp[4]))\n",
    "        loss.append(loss_)\n",
    "#         acc.append(train_tmp[1])\n",
    "        f1.append(f1_)\n",
    "\n",
    "    loss = np.mean(np.array(loss))\n",
    "#     acc = np.mean(np.array(acc))\n",
    "    f1 = np.mean(np.array(f1))\n",
    "    return loss, f1\n",
    "\n",
    "def train_cam(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final): \n",
    "    loss=[]\n",
    "#     acc = []\n",
    "    f1 = []\n",
    "\n",
    "    total_steps = int(np.ceil(len(data_train)/batch_size))\n",
    "    for curr_step in range(total_steps):\n",
    "        batch_mels, batch_labels = randextract_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "        batch_mels = np.asarray(batch_mels)\n",
    "        \n",
    "        batch_labels = np.asarray(np.squeeze(batch_labels))\n",
    "#         batch_labels2 = np.asarray(np.squeeze(batch_labels))\n",
    "#         batch_labels = [batch_labels2, batch_labels2]\n",
    "# #         print(batch_labels.shape)\n",
    "#         print(batch_labels)\n",
    "\n",
    "# return of train\n",
    "# 0 = total loss (attention branch + perception branch)\n",
    "# 1 = loss of attention pred\n",
    "# 2 = loss of perception pred\n",
    "# 3 = f1 of attention pred\n",
    "# 4 = f1 of perception pred     \n",
    "\n",
    "        train_tmp = model.train_on_batch(batch_mels, batch_labels)\n",
    "#         print(train_tmp)\n",
    "        loss_ = train_tmp[0]\n",
    "        f1_ = train_tmp[1]\n",
    "        loss.append(loss_)\n",
    "#         acc.append(train_tmp[1])\n",
    "        f1.append(f1_)\n",
    "\n",
    "    loss = np.mean(np.array(loss))\n",
    "#     acc = np.mean(np.array(acc))\n",
    "    f1 = np.mean(np.array(f1))\n",
    "    return loss, f1\n",
    "\n",
    "def test_cam(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    for i, file in enumerate(data):\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "        for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "            mel_files.append(clip_file)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "        logit = model.predict(mel_files)\n",
    "#         print(len(logit))\n",
    "        logit = np.mean(logit, axis=0)\n",
    "#         logit = np.mean(logit, axis=0)\n",
    "#         print(logit)\n",
    "        pred = np.argmax(logit)\n",
    "#         print('Pred={}'.format(pred))\n",
    "        \n",
    "        label = np.argmax(get_labels(input_directory, file, class2index))\n",
    "#         print('Label={}'.format(label))\n",
    "        #f1 = f1_score(label, logit)\n",
    "        #print(pred, label)\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        #total_f1 += f1\n",
    "    final_acc = total_acc / i\n",
    "    #final_f1 = total_f1 / i\n",
    "    return final_acc#, final_f1\n",
    "\n",
    "batch_size = 32#16#20#32#5#2#1#10#32\n",
    "\n",
    "minimum_len = 2880\n",
    "epochs = 1000\n",
    "loss_function = 'categorical_crossentropy'\n",
    "activation_function = 'softmax'\n",
    "rootdir = '../'\n",
    "date = datetime.today().strftime(\"%Y%m%d\")\n",
    "input_directory = os.path.join(rootdir, 'Training_WFDB')\n",
    "mel_name = 'Raw_data_20200424' \n",
    "mel_directory = os.path.join(rootdir, mel_name)\n",
    "results_directory = os.path.join(rootdir, 'results_'+date+'_1_CAM_primitive_model')\n",
    "if not os.path.isdir(input_directory):\n",
    "    os.mkdir(input_directory)\n",
    "if not os.path.isdir(mel_directory):\n",
    "    os.mkdir(mel_directory)\n",
    "if not os.path.isdir(results_directory):\n",
    "    os.mkdir(results_directory)\n",
    "        \n",
    "input_files = []\n",
    "for f in os.listdir(input_directory):\n",
    "    if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('mat'):\n",
    "        input_files.append(f)\n",
    "input_file_names = sorted(input_files)\n",
    "\n",
    "unique_classes = get_unique_classes(input_directory, input_files)\n",
    "class2index = {}\n",
    "for a, b in enumerate(unique_classes):\n",
    "    class2index[b] = a\n",
    "    \n",
    "classes_orig= [x.replace('.mat', '.hea') for x in input_file_names] # total subjects\n",
    "classes_multi, _, _ = searching_overlap(input_directory,class2index, input_file_names)\n",
    "classes_single = [x for x in classes_orig if x not in classes_multi]\n",
    "classes_single = [x.replace('.hea', '.mat') for x in classes_single]\n",
    "\n",
    "# double-checking if classes_single have single-label\n",
    "a, b, c  = searching_overlap(input_directory,class2index,classes_single)\n",
    "\n",
    "# we can safely use classes_single as input_file_names\n",
    "input_file_names = classes_single\n",
    "random.shuffle(input_file_names)\n",
    "np.shape(input_file_names)\n",
    "\n",
    "x_mean_all = []\n",
    "x_std_all = []\n",
    "for file in input_file_names:\n",
    "    x = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "    x_mean = [np.mean(x[:,0]), np.mean(x[:,1]), np.mean(x[:,2]), np.mean(x[:,3]), np.mean(x[:,4]), np.mean(x[:,5]),\n",
    "             np.mean(x[:,6]), np.mean(x[:,7]), np.mean(x[:,8]), np.mean(x[:,9]), np.mean(x[:,10]), np.mean(x[:,11])]\n",
    "    \n",
    "    x_std = [np.std(x[:,0]), np.std(x[:,1]), np.std(x[:,2]), np.std(x[:,3]), np.std(x[:,4]), np.std(x[:,5]),\n",
    "             np.std(x[:,6]), np.std(x[:,7]), np.std(x[:,8]), np.std(x[:,9]), np.std(x[:,10]), np.std(x[:,11])]\n",
    "    #print(x_mean)\n",
    "    x_mean_all.append(x_mean)\n",
    "    x_std_all.append(x_std) # yjs corrected on 2020-05-25\n",
    "x_mean_final = np.mean(x_mean_all, axis=0)\n",
    "x_std_final = np.mean(x_std_all, axis=0)\n",
    "print(x_mean_final)\n",
    "\n",
    "data, data_test = train_test_split(input_file_names, test_size = 0.2, train_size = 0.8, shuffle=True)\n",
    "data_train, data_val = train_test_split(data, test_size = 0.25, train_size = 0.75, shuffle=True)\n",
    "print(np.shape(data_train), np.shape(data_val), np.shape(data_test))\n",
    "\n",
    "\n",
    "main_input = Input(shape=(minimum_len,12), dtype='float32', name='main_input')\n",
    "\n",
    "branch_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_image (InputLayer)     [(None, None, 12)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 64)          2368      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, None, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 64)          12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 128)         24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 128)         49280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, None, 256)         98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, None, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, None, 256)         196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, None, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, None, 256)         196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, None, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, None, 512)         393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, None, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, None, 512)         786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, None, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, None, 512)         786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, None, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, None, 512)         786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, None, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, None, 256)         393472    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, None, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, None, 128)         98432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_final (Dense)          (None, 9)                 1161      \n",
      "_________________________________________________________________\n",
      "output (Softmax)             (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 3,842,953\n",
      "Trainable params: 3,835,785\n",
      "Non-trainable params: 7,168\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from Primitive_modules import *\n",
    "\n",
    "# def get_custom_model(input_shape, n_classes, minimum_len, target_classes, out_ch=256, n=18):\n",
    "# model = get_custom_model((None, 12), 9, minimum_len, 1, out_ch=256, n=18)\n",
    "# model = get_model((None, 12), 9, n=7)\n",
    "\n",
    "model = cam_model((None, 12), 9, minimum_len, out_ch=256, n=1)\n",
    "model.summary()\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=optimizers.Adam(lr=1e-5),           \n",
    "              metrics=[score_f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results_20200601_1_CAM_primitive_model\n",
      "\n",
      "Epoch 1 train_loss: 1.700 train_f1: 0.209 \t\n",
      "\n",
      "Validation 1 valid_acc: 0.414 best_acc: 0.414 \t\n",
      "\n",
      "Epoch 2 train_loss: 1.387 train_f1: 0.418 \t\n",
      "\n",
      "Validation 2 valid_acc: 0.542 best_acc: 0.542 \t\n",
      "\n",
      "Epoch 3 train_loss: 1.256 train_f1: 0.473 \t\n",
      "\n",
      "Validation 3 valid_acc: 0.602 best_acc: 0.602 \t\n",
      "\n",
      "Epoch 4 train_loss: 1.150 train_f1: 0.535 \t\n",
      "\n",
      "Validation 4 valid_acc: 0.636 best_acc: 0.636 \t\n",
      "\n",
      "Epoch 5 train_loss: 1.073 train_f1: 0.588 \t\n",
      "\n",
      "Validation 5 valid_acc: 0.661 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 6 train_loss: 1.005 train_f1: 0.631 \t\n",
      "\n",
      "Validation 6 valid_acc: 0.679 best_acc: 0.679 \t\n",
      "\n",
      "Epoch 7 train_loss: 0.978 train_f1: 0.642 \t\n",
      "\n",
      "Validation 7 valid_acc: 0.700 best_acc: 0.700 \t\n",
      "\n",
      "Epoch 8 train_loss: 0.953 train_f1: 0.654 \t\n",
      "\n",
      "Validation 8 valid_acc: 0.694 best_acc: 0.700 \t\n",
      "\n",
      "Epoch 9 train_loss: 0.906 train_f1: 0.672 \t\n",
      "\n",
      "Validation 9 valid_acc: 0.704 best_acc: 0.704 \t\n",
      "\n",
      "Epoch 10 train_loss: 0.887 train_f1: 0.683 \t\n",
      "\n",
      "Validation 10 valid_acc: 0.705 best_acc: 0.705 \t\n",
      "\n",
      "Epoch 11 train_loss: 0.870 train_f1: 0.693 \t\n",
      "\n",
      "Validation 11 valid_acc: 0.698 best_acc: 0.705 \t\n",
      "\n",
      "Epoch 12 train_loss: 0.851 train_f1: 0.696 \t\n",
      "\n",
      "Validation 12 valid_acc: 0.729 best_acc: 0.729 \t\n",
      "\n",
      "Epoch 13 train_loss: 0.808 train_f1: 0.724 \t\n",
      "\n",
      "Validation 13 valid_acc: 0.719 best_acc: 0.729 \t\n",
      "\n",
      "Epoch 14 train_loss: 0.810 train_f1: 0.714 \t\n",
      "\n",
      "Validation 14 valid_acc: 0.710 best_acc: 0.729 \t\n",
      "\n",
      "Epoch 15 train_loss: 0.801 train_f1: 0.718 \t\n",
      "\n",
      "Validation 15 valid_acc: 0.729 best_acc: 0.729 \t\n",
      "\n",
      "Epoch 16 train_loss: 0.783 train_f1: 0.729 \t\n",
      "\n",
      "Validation 16 valid_acc: 0.710 best_acc: 0.729 \t\n",
      "\n",
      "Epoch 17 train_loss: 0.776 train_f1: 0.729 \t\n",
      "\n",
      "Validation 17 valid_acc: 0.714 best_acc: 0.729 \t\n",
      "\n",
      "Epoch 18 train_loss: 0.750 train_f1: 0.742 \t\n",
      "\n",
      "Validation 18 valid_acc: 0.719 best_acc: 0.729 \t\n",
      "\n",
      "Epoch 19 train_loss: 0.736 train_f1: 0.746 \t\n",
      "\n",
      "Validation 19 valid_acc: 0.727 best_acc: 0.729 \t\n",
      "\n",
      "Epoch 20 train_loss: 0.741 train_f1: 0.749 \t\n",
      "\n",
      "Validation 20 valid_acc: 0.720 best_acc: 0.729 \t\n",
      "\n",
      "Epoch 21 train_loss: 0.711 train_f1: 0.753 \t\n",
      "\n",
      "Validation 21 valid_acc: 0.719 best_acc: 0.729 \t\n",
      "\n",
      "Epoch 22 train_loss: 0.685 train_f1: 0.762 \t\n",
      "\n",
      "Validation 22 valid_acc: 0.734 best_acc: 0.734 \t\n",
      "\n",
      "Epoch 23 train_loss: 0.696 train_f1: 0.762 \t\n",
      "\n",
      "Validation 23 valid_acc: 0.744 best_acc: 0.744 \t\n",
      "\n",
      "Epoch 24 train_loss: 0.687 train_f1: 0.760 \t\n",
      "\n",
      "Validation 24 valid_acc: 0.737 best_acc: 0.744 \t\n",
      "\n",
      "Epoch 25 train_loss: 0.659 train_f1: 0.774 \t\n",
      "\n",
      "Validation 25 valid_acc: 0.725 best_acc: 0.744 \t\n",
      "\n",
      "Epoch 26 train_loss: 0.671 train_f1: 0.767 \t\n",
      "\n",
      "Validation 26 valid_acc: 0.720 best_acc: 0.744 \t\n",
      "\n",
      "Epoch 27 train_loss: 0.670 train_f1: 0.762 \t\n",
      "\n",
      "Validation 27 valid_acc: 0.743 best_acc: 0.744 \t\n",
      "\n",
      "Epoch 28 train_loss: 0.651 train_f1: 0.776 \t\n",
      "\n",
      "Validation 28 valid_acc: 0.725 best_acc: 0.744 \t\n",
      "\n",
      "Epoch 29 train_loss: 0.669 train_f1: 0.773 \t\n",
      "\n",
      "Validation 29 valid_acc: 0.745 best_acc: 0.745 \t\n",
      "\n",
      "Epoch 30 train_loss: 0.646 train_f1: 0.779 \t\n",
      "\n",
      "Validation 30 valid_acc: 0.738 best_acc: 0.745 \t\n",
      "\n",
      "Epoch 31 train_loss: 0.620 train_f1: 0.787 \t\n",
      "\n",
      "Validation 31 valid_acc: 0.737 best_acc: 0.745 \t\n",
      "\n",
      "Epoch 32 train_loss: 0.641 train_f1: 0.785 \t\n",
      "\n",
      "Validation 32 valid_acc: 0.735 best_acc: 0.745 \t\n",
      "\n",
      "Epoch 33 train_loss: 0.609 train_f1: 0.799 \t\n",
      "\n",
      "Validation 33 valid_acc: 0.741 best_acc: 0.745 \t\n",
      "\n",
      "Epoch 34 train_loss: 0.601 train_f1: 0.787 \t\n",
      "\n",
      "Validation 34 valid_acc: 0.748 best_acc: 0.748 \t\n",
      "\n",
      "Epoch 35 train_loss: 0.585 train_f1: 0.799 \t\n",
      "\n",
      "Validation 35 valid_acc: 0.720 best_acc: 0.748 \t\n",
      "\n",
      "Epoch 36 train_loss: 0.575 train_f1: 0.805 \t\n",
      "\n",
      "Validation 36 valid_acc: 0.731 best_acc: 0.748 \t\n",
      "\n",
      "Epoch 37 train_loss: 0.561 train_f1: 0.811 \t\n",
      "\n",
      "Validation 37 valid_acc: 0.744 best_acc: 0.748 \t\n",
      "\n",
      "Epoch 38 train_loss: 0.573 train_f1: 0.807 \t\n",
      "\n",
      "Validation 38 valid_acc: 0.744 best_acc: 0.748 \t\n",
      "\n",
      "Epoch 39 train_loss: 0.549 train_f1: 0.817 \t\n",
      "\n",
      "Validation 39 valid_acc: 0.747 best_acc: 0.748 \t\n",
      "\n",
      "Epoch 40 train_loss: 0.548 train_f1: 0.812 \t\n",
      "\n",
      "Validation 40 valid_acc: 0.737 best_acc: 0.748 \t\n",
      "\n",
      "Epoch 41 train_loss: 0.546 train_f1: 0.810 \t\n",
      "\n",
      "Validation 41 valid_acc: 0.729 best_acc: 0.748 \t\n",
      "\n",
      "Epoch 42 train_loss: 0.536 train_f1: 0.820 \t\n",
      "\n",
      "Validation 42 valid_acc: 0.749 best_acc: 0.749 \t\n",
      "\n",
      "Epoch 43 train_loss: 0.540 train_f1: 0.819 \t\n",
      "\n",
      "Validation 43 valid_acc: 0.739 best_acc: 0.749 \t\n",
      "\n",
      "Epoch 44 train_loss: 0.507 train_f1: 0.823 \t\n",
      "\n",
      "Validation 44 valid_acc: 0.746 best_acc: 0.749 \t\n",
      "\n",
      "Epoch 45 train_loss: 0.524 train_f1: 0.819 \t\n",
      "\n",
      "Validation 45 valid_acc: 0.741 best_acc: 0.749 \t\n",
      "\n",
      "Epoch 46 train_loss: 0.510 train_f1: 0.828 \t\n",
      "\n",
      "Validation 46 valid_acc: 0.747 best_acc: 0.749 \t\n",
      "\n",
      "Epoch 47 train_loss: 0.513 train_f1: 0.828 \t\n",
      "\n",
      "Validation 47 valid_acc: 0.751 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 48 train_loss: 0.504 train_f1: 0.828 \t\n",
      "\n",
      "Validation 48 valid_acc: 0.732 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 49 train_loss: 0.520 train_f1: 0.823 \t\n",
      "\n",
      "Validation 49 valid_acc: 0.739 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 50 train_loss: 0.489 train_f1: 0.837 \t\n",
      "\n",
      "Validation 50 valid_acc: 0.734 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 51 train_loss: 0.494 train_f1: 0.838 \t\n",
      "\n",
      "Validation 51 valid_acc: 0.744 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 52 train_loss: 0.500 train_f1: 0.826 \t\n",
      "\n",
      "Validation 52 valid_acc: 0.750 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 53 train_loss: 0.502 train_f1: 0.832 \t\n",
      "\n",
      "Validation 53 valid_acc: 0.744 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 54 train_loss: 0.476 train_f1: 0.840 \t\n",
      "\n",
      "Validation 54 valid_acc: 0.744 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 55 train_loss: 0.460 train_f1: 0.844 \t\n",
      "\n",
      "Validation 55 valid_acc: 0.739 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 56 train_loss: 0.486 train_f1: 0.838 \t\n",
      "\n",
      "Validation 56 valid_acc: 0.714 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 57 train_loss: 0.470 train_f1: 0.846 \t\n",
      "\n",
      "Validation 57 valid_acc: 0.738 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 58 train_loss: 0.450 train_f1: 0.851 \t\n",
      "\n",
      "Validation 58 valid_acc: 0.739 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 59 train_loss: 0.458 train_f1: 0.844 \t\n",
      "\n",
      "Validation 59 valid_acc: 0.730 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 60 train_loss: 0.450 train_f1: 0.853 \t\n",
      "\n",
      "Validation 60 valid_acc: 0.737 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 61 train_loss: 0.460 train_f1: 0.849 \t\n",
      "\n",
      "Validation 61 valid_acc: 0.740 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 62 train_loss: 0.462 train_f1: 0.843 \t\n",
      "\n",
      "Validation 62 valid_acc: 0.733 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 63 train_loss: 0.442 train_f1: 0.849 \t\n",
      "\n",
      "Validation 63 valid_acc: 0.746 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 64 train_loss: 0.425 train_f1: 0.855 \t\n",
      "\n",
      "Validation 64 valid_acc: 0.750 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 65 train_loss: 0.421 train_f1: 0.857 \t\n",
      "\n",
      "Validation 65 valid_acc: 0.725 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 66 train_loss: 0.434 train_f1: 0.856 \t\n",
      "\n",
      "Validation 66 valid_acc: 0.746 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 67 train_loss: 0.440 train_f1: 0.847 \t\n",
      "\n",
      "Validation 67 valid_acc: 0.743 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 68 train_loss: 0.393 train_f1: 0.866 \t\n",
      "\n",
      "Validation 68 valid_acc: 0.745 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 69 train_loss: 0.426 train_f1: 0.854 \t\n",
      "\n",
      "Validation 69 valid_acc: 0.735 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 70 train_loss: 0.395 train_f1: 0.870 \t\n",
      "\n",
      "Validation 70 valid_acc: 0.733 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 71 train_loss: 0.416 train_f1: 0.863 \t\n",
      "\n",
      "Validation 71 valid_acc: 0.748 best_acc: 0.751 \t\n",
      "\n",
      "Epoch 72 train_loss: 0.411 train_f1: 0.865 \t\n",
      "\n",
      "Validation 72 valid_acc: 0.753 best_acc: 0.753 \t\n",
      "\n",
      "Epoch 73 train_loss: 0.384 train_f1: 0.875 \t\n",
      "\n",
      "Validation 73 valid_acc: 0.754 best_acc: 0.754 \t\n",
      "\n",
      "Epoch 74 train_loss: 0.395 train_f1: 0.865 \t\n",
      "\n",
      "Validation 74 valid_acc: 0.754 best_acc: 0.754 \t\n",
      "\n",
      "Epoch 75 train_loss: 0.386 train_f1: 0.870 \t\n",
      "\n",
      "Validation 75 valid_acc: 0.726 best_acc: 0.754 \t\n",
      "\n",
      "Epoch 76 train_loss: 0.370 train_f1: 0.877 \t\n",
      "\n",
      "Validation 76 valid_acc: 0.755 best_acc: 0.755 \t\n",
      "\n",
      "Epoch 77 train_loss: 0.393 train_f1: 0.873 \t\n",
      "\n",
      "Validation 77 valid_acc: 0.746 best_acc: 0.755 \t\n",
      "\n",
      "Epoch 78 train_loss: 0.383 train_f1: 0.872 \t\n",
      "\n",
      "Validation 78 valid_acc: 0.747 best_acc: 0.755 \t\n",
      "\n",
      "Epoch 79 train_loss: 0.365 train_f1: 0.879 \t\n",
      "\n",
      "Validation 79 valid_acc: 0.723 best_acc: 0.755 \t\n",
      "\n",
      "Epoch 80 train_loss: 0.375 train_f1: 0.878 \t\n",
      "\n",
      "Validation 80 valid_acc: 0.743 best_acc: 0.755 \t\n",
      "\n",
      "Epoch 81 train_loss: 0.356 train_f1: 0.880 \t\n",
      "\n",
      "Validation 81 valid_acc: 0.732 best_acc: 0.755 \t\n",
      "\n",
      "Epoch 82 train_loss: 0.356 train_f1: 0.880 \t\n",
      "\n",
      "Validation 82 valid_acc: 0.758 best_acc: 0.758 \t\n",
      "\n",
      "Epoch 83 train_loss: 0.353 train_f1: 0.877 \t\n",
      "\n",
      "Validation 83 valid_acc: 0.751 best_acc: 0.758 \t\n",
      "\n",
      "Epoch 84 train_loss: 0.360 train_f1: 0.881 \t\n",
      "\n",
      "Validation 84 valid_acc: 0.733 best_acc: 0.758 \t\n",
      "\n",
      "Epoch 85 train_loss: 0.358 train_f1: 0.886 \t\n",
      "\n",
      "Validation 85 valid_acc: 0.757 best_acc: 0.758 \t\n",
      "\n",
      "Epoch 86 train_loss: 0.337 train_f1: 0.888 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 86 valid_acc: 0.739 best_acc: 0.758 \t\n",
      "\n",
      "Epoch 87 train_loss: 0.361 train_f1: 0.882 \t\n",
      "\n",
      "Validation 87 valid_acc: 0.735 best_acc: 0.758 \t\n",
      "\n",
      "Epoch 88 train_loss: 0.330 train_f1: 0.894 \t\n",
      "\n",
      "Validation 88 valid_acc: 0.759 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 89 train_loss: 0.320 train_f1: 0.893 \t\n",
      "\n",
      "Validation 89 valid_acc: 0.753 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 90 train_loss: 0.329 train_f1: 0.891 \t\n",
      "\n",
      "Validation 90 valid_acc: 0.754 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 91 train_loss: 0.324 train_f1: 0.892 \t\n",
      "\n",
      "Validation 91 valid_acc: 0.727 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 92 train_loss: 0.336 train_f1: 0.892 \t\n",
      "\n",
      "Validation 92 valid_acc: 0.726 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 93 train_loss: 0.327 train_f1: 0.891 \t\n",
      "\n",
      "Validation 93 valid_acc: 0.750 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 94 train_loss: 0.303 train_f1: 0.901 \t\n",
      "\n",
      "Validation 94 valid_acc: 0.739 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 95 train_loss: 0.326 train_f1: 0.891 \t\n",
      "\n",
      "Validation 95 valid_acc: 0.726 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 96 train_loss: 0.328 train_f1: 0.889 \t\n",
      "\n",
      "Validation 96 valid_acc: 0.726 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 97 train_loss: 0.324 train_f1: 0.891 \t\n",
      "\n",
      "Validation 97 valid_acc: 0.748 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 98 train_loss: 0.309 train_f1: 0.897 \t\n",
      "\n",
      "Validation 98 valid_acc: 0.728 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 99 train_loss: 0.304 train_f1: 0.901 \t\n",
      "\n",
      "Validation 99 valid_acc: 0.751 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 100 train_loss: 0.322 train_f1: 0.897 \t\n",
      "\n",
      "Validation 100 valid_acc: 0.724 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 101 train_loss: 0.285 train_f1: 0.906 \t\n",
      "\n",
      "Validation 101 valid_acc: 0.745 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 102 train_loss: 0.305 train_f1: 0.900 \t\n",
      "\n",
      "Validation 102 valid_acc: 0.747 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 103 train_loss: 0.290 train_f1: 0.910 \t\n",
      "\n",
      "Validation 103 valid_acc: 0.747 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 104 train_loss: 0.288 train_f1: 0.905 \t\n",
      "\n",
      "Validation 104 valid_acc: 0.747 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 105 train_loss: 0.312 train_f1: 0.899 \t\n",
      "\n",
      "Validation 105 valid_acc: 0.741 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 106 train_loss: 0.292 train_f1: 0.902 \t\n",
      "\n",
      "Validation 106 valid_acc: 0.723 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 107 train_loss: 0.280 train_f1: 0.912 \t\n",
      "\n",
      "Validation 107 valid_acc: 0.737 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 108 train_loss: 0.280 train_f1: 0.905 \t\n",
      "\n",
      "Validation 108 valid_acc: 0.745 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 109 train_loss: 0.267 train_f1: 0.916 \t\n",
      "\n",
      "Validation 109 valid_acc: 0.750 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 110 train_loss: 0.270 train_f1: 0.912 \t\n",
      "\n",
      "Validation 110 valid_acc: 0.720 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 111 train_loss: 0.270 train_f1: 0.915 \t\n",
      "\n",
      "Validation 111 valid_acc: 0.740 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 112 train_loss: 0.276 train_f1: 0.909 \t\n",
      "\n",
      "Validation 112 valid_acc: 0.722 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 113 train_loss: 0.267 train_f1: 0.911 \t\n",
      "\n",
      "Validation 113 valid_acc: 0.733 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 114 train_loss: 0.263 train_f1: 0.916 \t\n",
      "\n",
      "Validation 114 valid_acc: 0.725 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 115 train_loss: 0.266 train_f1: 0.913 \t\n",
      "\n",
      "Validation 115 valid_acc: 0.746 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 116 train_loss: 0.261 train_f1: 0.913 \t\n",
      "\n",
      "Validation 116 valid_acc: 0.727 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 117 train_loss: 0.235 train_f1: 0.927 \t\n",
      "\n",
      "Validation 117 valid_acc: 0.752 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 118 train_loss: 0.251 train_f1: 0.922 \t\n",
      "\n",
      "Validation 118 valid_acc: 0.749 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 119 train_loss: 0.236 train_f1: 0.928 \t\n",
      "\n",
      "Validation 119 valid_acc: 0.713 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 120 train_loss: 0.257 train_f1: 0.916 \t\n",
      "\n",
      "Validation 120 valid_acc: 0.737 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 121 train_loss: 0.263 train_f1: 0.917 \t\n",
      "\n",
      "Validation 121 valid_acc: 0.724 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 122 train_loss: 0.231 train_f1: 0.927 \t\n",
      "\n",
      "Validation 122 valid_acc: 0.732 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 123 train_loss: 0.261 train_f1: 0.915 \t\n",
      "\n",
      "Validation 123 valid_acc: 0.738 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 124 train_loss: 0.244 train_f1: 0.919 \t\n",
      "\n",
      "Validation 124 valid_acc: 0.731 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 125 train_loss: 0.244 train_f1: 0.921 \t\n",
      "\n",
      "Validation 125 valid_acc: 0.750 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 126 train_loss: 0.243 train_f1: 0.921 \t\n",
      "\n",
      "Validation 126 valid_acc: 0.736 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 127 train_loss: 0.229 train_f1: 0.925 \t\n",
      "\n",
      "Validation 127 valid_acc: 0.735 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 128 train_loss: 0.234 train_f1: 0.925 \t\n",
      "\n",
      "Validation 128 valid_acc: 0.731 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 129 train_loss: 0.249 train_f1: 0.920 \t\n",
      "\n",
      "Validation 129 valid_acc: 0.726 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 130 train_loss: 0.229 train_f1: 0.928 \t\n",
      "\n",
      "Validation 130 valid_acc: 0.736 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 131 train_loss: 0.223 train_f1: 0.924 \t\n",
      "\n",
      "Validation 131 valid_acc: 0.747 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 132 train_loss: 0.226 train_f1: 0.931 \t\n",
      "\n",
      "Validation 132 valid_acc: 0.736 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 133 train_loss: 0.213 train_f1: 0.934 \t\n",
      "\n",
      "Validation 133 valid_acc: 0.740 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 134 train_loss: 0.217 train_f1: 0.930 \t\n",
      "\n",
      "Validation 134 valid_acc: 0.725 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 135 train_loss: 0.221 train_f1: 0.931 \t\n",
      "\n",
      "Validation 135 valid_acc: 0.745 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 136 train_loss: 0.221 train_f1: 0.926 \t\n",
      "\n",
      "Validation 136 valid_acc: 0.730 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 137 train_loss: 0.202 train_f1: 0.934 \t\n",
      "\n",
      "Validation 137 valid_acc: 0.758 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 138 train_loss: 0.219 train_f1: 0.930 \t\n",
      "\n",
      "Validation 138 valid_acc: 0.750 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 139 train_loss: 0.215 train_f1: 0.937 \t\n",
      "\n",
      "Validation 139 valid_acc: 0.728 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 140 train_loss: 0.223 train_f1: 0.928 \t\n",
      "\n",
      "Validation 140 valid_acc: 0.737 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 141 train_loss: 0.219 train_f1: 0.930 \t\n",
      "\n",
      "Validation 141 valid_acc: 0.747 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 142 train_loss: 0.202 train_f1: 0.935 \t\n",
      "\n",
      "Validation 142 valid_acc: 0.738 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 143 train_loss: 0.203 train_f1: 0.932 \t\n",
      "\n",
      "Validation 143 valid_acc: 0.748 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 144 train_loss: 0.220 train_f1: 0.928 \t\n",
      "\n",
      "Validation 144 valid_acc: 0.735 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 145 train_loss: 0.199 train_f1: 0.939 \t\n",
      "\n",
      "Validation 145 valid_acc: 0.738 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 146 train_loss: 0.189 train_f1: 0.941 \t\n",
      "\n",
      "Validation 146 valid_acc: 0.745 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 147 train_loss: 0.213 train_f1: 0.930 \t\n",
      "\n",
      "Validation 147 valid_acc: 0.729 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 148 train_loss: 0.208 train_f1: 0.934 \t\n",
      "\n",
      "Validation 148 valid_acc: 0.723 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 149 train_loss: 0.207 train_f1: 0.934 \t\n",
      "\n",
      "Validation 149 valid_acc: 0.731 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 150 train_loss: 0.205 train_f1: 0.938 \t\n",
      "\n",
      "Validation 150 valid_acc: 0.739 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 151 train_loss: 0.199 train_f1: 0.939 \t\n",
      "\n",
      "Validation 151 valid_acc: 0.722 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 152 train_loss: 0.197 train_f1: 0.940 \t\n",
      "\n",
      "Validation 152 valid_acc: 0.736 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 153 train_loss: 0.176 train_f1: 0.946 \t\n",
      "\n",
      "Validation 153 valid_acc: 0.733 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 154 train_loss: 0.178 train_f1: 0.947 \t\n",
      "\n",
      "Validation 154 valid_acc: 0.726 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 155 train_loss: 0.194 train_f1: 0.940 \t\n",
      "\n",
      "Validation 155 valid_acc: 0.749 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 156 train_loss: 0.191 train_f1: 0.938 \t\n",
      "\n",
      "Validation 156 valid_acc: 0.729 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 157 train_loss: 0.182 train_f1: 0.939 \t\n",
      "\n",
      "Validation 157 valid_acc: 0.733 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 158 train_loss: 0.203 train_f1: 0.936 \t\n",
      "\n",
      "Validation 158 valid_acc: 0.722 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 159 train_loss: 0.173 train_f1: 0.945 \t\n",
      "\n",
      "Validation 159 valid_acc: 0.722 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 160 train_loss: 0.176 train_f1: 0.944 \t\n",
      "\n",
      "Validation 160 valid_acc: 0.730 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 161 train_loss: 0.184 train_f1: 0.941 \t\n",
      "\n",
      "Validation 161 valid_acc: 0.741 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 162 train_loss: 0.189 train_f1: 0.942 \t\n",
      "\n",
      "Validation 162 valid_acc: 0.699 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 163 train_loss: 0.180 train_f1: 0.944 \t\n",
      "\n",
      "Validation 163 valid_acc: 0.746 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 164 train_loss: 0.174 train_f1: 0.945 \t\n",
      "\n",
      "Validation 164 valid_acc: 0.730 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 165 train_loss: 0.184 train_f1: 0.945 \t\n",
      "\n",
      "Validation 165 valid_acc: 0.747 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 166 train_loss: 0.179 train_f1: 0.945 \t\n",
      "\n",
      "Validation 166 valid_acc: 0.710 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 167 train_loss: 0.175 train_f1: 0.945 \t\n",
      "\n",
      "Validation 167 valid_acc: 0.738 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 168 train_loss: 0.176 train_f1: 0.946 \t\n",
      "\n",
      "Validation 168 valid_acc: 0.717 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 169 train_loss: 0.166 train_f1: 0.950 \t\n",
      "\n",
      "Validation 169 valid_acc: 0.731 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 170 train_loss: 0.167 train_f1: 0.950 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 170 valid_acc: 0.735 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 171 train_loss: 0.162 train_f1: 0.949 \t\n",
      "\n",
      "Validation 171 valid_acc: 0.737 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 172 train_loss: 0.176 train_f1: 0.943 \t\n",
      "\n",
      "Validation 172 valid_acc: 0.740 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 173 train_loss: 0.156 train_f1: 0.953 \t\n",
      "\n",
      "Validation 173 valid_acc: 0.715 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 174 train_loss: 0.175 train_f1: 0.946 \t\n",
      "\n",
      "Validation 174 valid_acc: 0.723 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 175 train_loss: 0.160 train_f1: 0.949 \t\n",
      "\n",
      "Validation 175 valid_acc: 0.744 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 176 train_loss: 0.169 train_f1: 0.946 \t\n",
      "\n",
      "Validation 176 valid_acc: 0.725 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 177 train_loss: 0.180 train_f1: 0.943 \t\n",
      "\n",
      "Validation 177 valid_acc: 0.735 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 178 train_loss: 0.177 train_f1: 0.942 \t\n",
      "\n",
      "Validation 178 valid_acc: 0.711 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 179 train_loss: 0.148 train_f1: 0.955 \t\n",
      "\n",
      "Validation 179 valid_acc: 0.727 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 180 train_loss: 0.163 train_f1: 0.954 \t\n",
      "\n",
      "Validation 180 valid_acc: 0.735 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 181 train_loss: 0.168 train_f1: 0.944 \t\n",
      "\n",
      "Validation 181 valid_acc: 0.716 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 182 train_loss: 0.145 train_f1: 0.955 \t\n",
      "\n",
      "Validation 182 valid_acc: 0.735 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 183 train_loss: 0.155 train_f1: 0.953 \t\n",
      "\n",
      "Validation 183 valid_acc: 0.729 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 184 train_loss: 0.161 train_f1: 0.951 \t\n",
      "\n",
      "Validation 184 valid_acc: 0.732 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 185 train_loss: 0.166 train_f1: 0.946 \t\n",
      "\n",
      "Validation 185 valid_acc: 0.710 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 186 train_loss: 0.146 train_f1: 0.955 \t\n",
      "\n",
      "Validation 186 valid_acc: 0.727 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 187 train_loss: 0.169 train_f1: 0.953 \t\n",
      "\n",
      "Validation 187 valid_acc: 0.727 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 188 train_loss: 0.159 train_f1: 0.950 \t\n",
      "\n",
      "Validation 188 valid_acc: 0.724 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 189 train_loss: 0.151 train_f1: 0.953 \t\n",
      "\n",
      "Validation 189 valid_acc: 0.725 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 190 train_loss: 0.140 train_f1: 0.956 \t\n",
      "\n",
      "Validation 190 valid_acc: 0.747 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 191 train_loss: 0.145 train_f1: 0.956 \t\n",
      "\n",
      "Validation 191 valid_acc: 0.721 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 192 train_loss: 0.155 train_f1: 0.954 \t\n",
      "\n",
      "Validation 192 valid_acc: 0.721 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 193 train_loss: 0.142 train_f1: 0.956 \t\n",
      "\n",
      "Validation 193 valid_acc: 0.700 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 194 train_loss: 0.151 train_f1: 0.959 \t\n",
      "\n",
      "Validation 194 valid_acc: 0.726 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 195 train_loss: 0.134 train_f1: 0.961 \t\n",
      "\n",
      "Validation 195 valid_acc: 0.742 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 196 train_loss: 0.137 train_f1: 0.960 \t\n",
      "\n",
      "Validation 196 valid_acc: 0.740 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 197 train_loss: 0.159 train_f1: 0.949 \t\n",
      "\n",
      "Validation 197 valid_acc: 0.720 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 198 train_loss: 0.143 train_f1: 0.957 \t\n",
      "\n",
      "Validation 198 valid_acc: 0.733 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 199 train_loss: 0.147 train_f1: 0.956 \t\n",
      "\n",
      "Validation 199 valid_acc: 0.732 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 200 train_loss: 0.136 train_f1: 0.959 \t\n",
      "\n",
      "Validation 200 valid_acc: 0.737 best_acc: 0.759 \t\n",
      "\n",
      "Epoch 201 train_loss: 0.146 train_f1: 0.952 \t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3e15d2b7344a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msave_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_cam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass2index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_mean_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_std_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mval_acc_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-03e5174e37b9>\u001b[0m in \u001b[0;36mtest_cam\u001b[0;34m(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mmel_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mmel_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;31m#         print(len(logit))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1076\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m           callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;31m# Setup work for each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'metrics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;31m# Reset the state of loss metric wrappers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \"\"\"\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3069\u001b[0m           \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3071\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1337\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1339\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1341\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/lock_util.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/lock_util.py\u001b[0m in \u001b[0;36macquire\u001b[0;34m(self, group_id)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_another_group_active\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_member_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroup_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DISP DATETIME FOR CHECKING TIME\n",
    "start = time.time()\n",
    "val_acc_sum=[]\n",
    "train_loss_sum=[]\n",
    "train_acc_sum=[]\n",
    "val_loss_sum=[]\n",
    "val_acc_min = 0\n",
    "print(results_directory)\n",
    "\n",
    "for num_epoch in range(epochs):\n",
    "    random.shuffle(data_train)\n",
    "    train_loss, train_f1 = train_cam(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "    print('\\nEpoch',num_epoch+1,'train_loss:',f'{train_loss:.3f}','train_f1:',f'{train_f1:.3f}',\"\\t\")\n",
    "    model_output = \"ECG_ABN_E%02dL%.2f\" % (num_epoch, train_loss)\n",
    "    save_name = os.path.join(results_directory, model_output)\n",
    "    \n",
    "    val_acc = test_cam(data_val, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)\n",
    "\n",
    "    if val_acc > val_acc_min:\n",
    "        val_acc_min = val_acc\n",
    "        model.save(save_name)\n",
    "    print('\\nValidation', num_epoch+1, 'valid_acc:',f'{val_acc:.3f}', 'best_acc:',f'{val_acc_min:.3f}', \"\\t\")\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
