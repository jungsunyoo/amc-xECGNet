{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numba/errors.py:137: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.7606254 2.7811065 2.746315  2.7405128 2.706551  2.7243118 2.6449137\n",
      " 2.733338  2.7534912 2.760266  2.7354655 2.6915622]\n",
      "(3840,) (1280,) (1281,)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\n",
      "Epoch 1 train_loss: 1.898 train_acc: 0.351 \t\n",
      "\n",
      "Validation 1 valid_acc: 0.149 best_acc: 0.149 \t\n",
      "\n",
      "Epoch 2 train_loss: 1.594 train_acc: 0.455 \t\n",
      "\n",
      "Validation 2 valid_acc: 0.260 best_acc: 0.260 \t\n",
      "\n",
      "Epoch 3 train_loss: 1.437 train_acc: 0.509 \t\n",
      "\n",
      "Validation 3 valid_acc: 0.345 best_acc: 0.345 \t\n",
      "\n",
      "Epoch 4 train_loss: 1.315 train_acc: 0.549 \t\n",
      "\n",
      "Validation 4 valid_acc: 0.379 best_acc: 0.379 \t\n",
      "\n",
      "Epoch 5 train_loss: 1.262 train_acc: 0.564 \t\n",
      "\n",
      "Validation 5 valid_acc: 0.352 best_acc: 0.379 \t\n",
      "\n",
      "Epoch 6 train_loss: 1.177 train_acc: 0.599 \t\n",
      "\n",
      "Validation 6 valid_acc: 0.193 best_acc: 0.379 \t\n",
      "\n",
      "Epoch 7 train_loss: 1.114 train_acc: 0.629 \t\n",
      "\n",
      "Validation 7 valid_acc: 0.289 best_acc: 0.379 \t\n",
      "\n",
      "Epoch 8 train_loss: 1.030 train_acc: 0.654 \t\n",
      "\n",
      "Validation 8 valid_acc: 0.516 best_acc: 0.516 \t\n",
      "\n",
      "Epoch 9 train_loss: 1.006 train_acc: 0.658 \t\n",
      "\n",
      "Validation 9 valid_acc: 0.414 best_acc: 0.516 \t\n",
      "\n",
      "Epoch 10 train_loss: 0.932 train_acc: 0.679 \t\n",
      "\n",
      "Validation 10 valid_acc: 0.564 best_acc: 0.564 \t\n",
      "\n",
      "Epoch 11 train_loss: 0.903 train_acc: 0.695 \t\n",
      "\n",
      "Validation 11 valid_acc: 0.513 best_acc: 0.564 \t\n",
      "\n",
      "Epoch 12 train_loss: 0.856 train_acc: 0.709 \t\n",
      "\n",
      "Validation 12 valid_acc: 0.400 best_acc: 0.564 \t\n",
      "\n",
      "Epoch 13 train_loss: 0.813 train_acc: 0.729 \t\n",
      "\n",
      "Validation 13 valid_acc: 0.599 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 14 train_loss: 0.796 train_acc: 0.731 \t\n",
      "\n",
      "Validation 14 valid_acc: 0.503 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 15 train_loss: 0.763 train_acc: 0.736 \t\n",
      "\n",
      "Validation 15 valid_acc: 0.460 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 16 train_loss: 0.705 train_acc: 0.754 \t\n",
      "\n",
      "Validation 16 valid_acc: 0.569 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 17 train_loss: 0.701 train_acc: 0.755 \t\n",
      "\n",
      "Validation 17 valid_acc: 0.647 best_acc: 0.647 \t\n",
      "\n",
      "Epoch 18 train_loss: 0.653 train_acc: 0.778 \t\n",
      "\n",
      "Validation 18 valid_acc: 0.538 best_acc: 0.647 \t\n",
      "\n",
      "Epoch 19 train_loss: 0.656 train_acc: 0.772 \t\n",
      "\n",
      "Validation 19 valid_acc: 0.573 best_acc: 0.647 \t\n",
      "\n",
      "Epoch 20 train_loss: 0.624 train_acc: 0.790 \t\n",
      "\n",
      "Validation 20 valid_acc: 0.679 best_acc: 0.679 \t\n",
      "\n",
      "Epoch 21 train_loss: 0.570 train_acc: 0.799 \t\n",
      "\n",
      "Validation 21 valid_acc: 0.646 best_acc: 0.679 \t\n",
      "\n",
      "Epoch 22 train_loss: 0.566 train_acc: 0.805 \t\n",
      "\n",
      "Validation 22 valid_acc: 0.646 best_acc: 0.679 \t\n",
      "\n",
      "Epoch 23 train_loss: 0.538 train_acc: 0.811 \t\n",
      "\n",
      "Validation 23 valid_acc: 0.364 best_acc: 0.679 \t\n",
      "\n",
      "Epoch 24 train_loss: 0.513 train_acc: 0.818 \t\n",
      "\n",
      "Validation 24 valid_acc: 0.494 best_acc: 0.679 \t\n",
      "\n",
      "Epoch 25 train_loss: 0.487 train_acc: 0.832 \t\n",
      "\n",
      "Validation 25 valid_acc: 0.661 best_acc: 0.679 \t\n",
      "\n",
      "Epoch 26 train_loss: 0.486 train_acc: 0.831 \t\n",
      "\n",
      "Validation 26 valid_acc: 0.529 best_acc: 0.679 \t\n",
      "\n",
      "Epoch 27 train_loss: 0.463 train_acc: 0.837 \t\n",
      "\n",
      "Validation 27 valid_acc: 0.557 best_acc: 0.679 \t\n",
      "\n",
      "Epoch 28 train_loss: 0.403 train_acc: 0.866 \t\n",
      "\n",
      "Validation 28 valid_acc: 0.603 best_acc: 0.679 \t\n",
      "\n",
      "Epoch 29 train_loss: 0.405 train_acc: 0.863 \t\n",
      "\n",
      "Validation 29 valid_acc: 0.679 best_acc: 0.679 \t\n",
      "\n",
      "Epoch 30 train_loss: 0.404 train_acc: 0.856 \t\n",
      "\n",
      "Validation 30 valid_acc: 0.624 best_acc: 0.679 \t\n",
      "\n",
      "Epoch 31 train_loss: 0.370 train_acc: 0.877 \t\n",
      "\n",
      "Validation 31 valid_acc: 0.641 best_acc: 0.679 \t\n",
      "\n",
      "Epoch 32 train_loss: 0.351 train_acc: 0.880 \t\n",
      "\n",
      "Validation 32 valid_acc: 0.687 best_acc: 0.687 \t\n",
      "\n",
      "Epoch 33 train_loss: 0.317 train_acc: 0.897 \t\n",
      "\n",
      "Validation 33 valid_acc: 0.525 best_acc: 0.687 \t\n",
      "\n",
      "Epoch 34 train_loss: 0.297 train_acc: 0.892 \t\n",
      "\n",
      "Validation 34 valid_acc: 0.672 best_acc: 0.687 \t\n",
      "\n",
      "Epoch 35 train_loss: 0.289 train_acc: 0.899 \t\n",
      "\n",
      "Validation 35 valid_acc: 0.602 best_acc: 0.687 \t\n",
      "\n",
      "Epoch 36 train_loss: 0.283 train_acc: 0.907 \t\n",
      "\n",
      "Validation 36 valid_acc: 0.643 best_acc: 0.687 \t\n",
      "\n",
      "Epoch 37 train_loss: 0.247 train_acc: 0.909 \t\n",
      "\n",
      "Validation 37 valid_acc: 0.639 best_acc: 0.687 \t\n",
      "\n",
      "Epoch 38 train_loss: 0.242 train_acc: 0.921 \t\n",
      "\n",
      "Validation 38 valid_acc: 0.561 best_acc: 0.687 \t\n",
      "\n",
      "Epoch 39 train_loss: 0.273 train_acc: 0.907 \t\n",
      "\n",
      "Validation 39 valid_acc: 0.597 best_acc: 0.687 \t\n",
      "\n",
      "Epoch 40 train_loss: 0.278 train_acc: 0.898 \t\n",
      "\n",
      "Validation 40 valid_acc: 0.645 best_acc: 0.687 \t\n",
      "\n",
      "Epoch 41 train_loss: 0.205 train_acc: 0.931 \t\n",
      "\n",
      "Validation 41 valid_acc: 0.647 best_acc: 0.687 \t\n",
      "\n",
      "Epoch 42 train_loss: 0.225 train_acc: 0.926 \t\n",
      "\n",
      "Validation 42 valid_acc: 0.611 best_acc: 0.687 \t\n",
      "\n",
      "Epoch 43 train_loss: 0.208 train_acc: 0.929 \t\n",
      "\n",
      "Validation 43 valid_acc: 0.666 best_acc: 0.687 \t\n",
      "\n",
      "Epoch 44 train_loss: 0.223 train_acc: 0.927 \t\n",
      "\n",
      "Validation 44 valid_acc: 0.596 best_acc: 0.687 \t\n",
      "\n",
      "Epoch 45 train_loss: 0.190 train_acc: 0.934 \t\n",
      "\n",
      "Validation 45 valid_acc: 0.625 best_acc: 0.687 \t\n",
      "\n",
      "Epoch 46 train_loss: 0.208 train_acc: 0.932 \t\n",
      "\n",
      "Validation 46 valid_acc: 0.564 best_acc: 0.687 \t\n",
      "\n",
      "Epoch 47 train_loss: 0.189 train_acc: 0.934 \t\n",
      "\n",
      "Validation 47 valid_acc: 0.686 best_acc: 0.687 \t\n",
      "\n",
      "Epoch 48 train_loss: 0.198 train_acc: 0.929 \t\n",
      "\n",
      "Validation 48 valid_acc: 0.550 best_acc: 0.687 \t\n",
      "\n",
      "Epoch 49 train_loss: 0.180 train_acc: 0.934 \t\n",
      "\n",
      "Validation 49 valid_acc: 0.547 best_acc: 0.687 \t\n",
      "\n",
      "Epoch 50 train_loss: 0.164 train_acc: 0.944 \t\n",
      "\n",
      "Validation 50 valid_acc: 0.695 best_acc: 0.695 \t\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.applications.densenet import DenseNet121, DenseNet169\n",
    "from keras.layers import Input, GlobalAveragePooling2D, Dense, Activation, GaussianNoise\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "random.seed(100)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "# Find unique number of classes  \n",
    "def get_unique_classes(input_directory,files):\n",
    "\n",
    "    unique_classes=set()\n",
    "    for f in files:\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    for c in tmp:\n",
    "                        unique_classes.add(c.strip())\n",
    "    return sorted(unique_classes)\n",
    "\n",
    "def one_hot_encoding(one_hot_vector,y, class2index):\n",
    "    ind=class2index[y]\n",
    "    one_hot_vector[ind]=1\n",
    "    return one_hot_vector\n",
    "\n",
    "# Search for multi-label subjects\n",
    "def searching_overlap(input_directory,class2index, input_file_names):\n",
    "    multiclasses=[]\n",
    "    multisubjects=[]\n",
    "    number = []\n",
    "    for file in input_file_names:\n",
    "        f=file\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    if len(tmp)>1:\n",
    "                        one_hot_vector = [0]*(len(class2index))\n",
    "                        for c in tmp:\n",
    "                            one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                        multiclasses.append(one_hot_vector)\n",
    "                        multisubjects.append(g)\n",
    "                        number.append(len(tmp))\n",
    "    return multisubjects, multiclasses, number\n",
    "\n",
    "def block_feature(sequence_en, minimum_len): \n",
    "    new_en = []\n",
    "    if len(sequence_en) > minimum_len:  \n",
    "        start = random.randint(0,len(sequence_en)-minimum_len)\n",
    "        #print(start)\n",
    "        new_en = sequence_en[start:start+minimum_len]\n",
    "    elif len(sequence_en) == minimum_len: \n",
    "        new_en = sequence_en\n",
    "    else: \n",
    "        assert len(sequence_en) <= minimum_len\n",
    "    return new_en\n",
    "\n",
    "def exploratory_look(input_directory,file, class2index):\n",
    "    classes = []\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                print(tmp, len(tmp))\n",
    "    return tmp     \n",
    "\n",
    "# Get classes of sorted file names\n",
    "def get_labels(input_directory,file, class2index):\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                one_hot_vector = [0]*(len(class2index))\n",
    "                for c in tmp:\n",
    "                    one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                \n",
    "    return one_hot_vector\n",
    "\n",
    "def randextract_mels(curr_step, batch_size, data, mel_directory, class2index, minimum_len, x_mean_final, x_std_final):\n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    start = batch_size*curr_step\n",
    "    end = batch_size*(curr_step+1)\n",
    "    curr_file_indices = data[start:end]\n",
    "    for file in curr_file_indices:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        clip_file = block_feature(tmp_file, minimum_len)\n",
    "        #print(clip_file.shape)\n",
    "        #clip_file = tmp_file[:minimum_len]\n",
    "        clip_file -= x_mean_final\n",
    "        clip_file /= x_std_final\n",
    "        mel_files.append(clip_file)\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        classes.append(label)\n",
    "    concat = list(zip(mel_files, classes))\n",
    "    random.shuffle(concat)\n",
    "    mel_files, classes = zip(*concat)\n",
    "    return mel_files, classes\n",
    "\n",
    "def train(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final): \n",
    "    loss=[]\n",
    "    acc = []\n",
    "\n",
    "    total_steps = int(np.ceil(len(data_train)/batch_size))\n",
    "    for curr_step in range(total_steps):\n",
    "        batch_mels, batch_labels = randextract_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "        batch_mels = np.asarray(batch_mels)\n",
    "        batch_labels = np.asarray(np.squeeze(batch_labels))\n",
    "        train_tmp = model.train_on_batch(batch_mels, batch_labels)\n",
    "        loss.append(train_tmp[0])\n",
    "        acc.append(train_tmp[1])\n",
    "\n",
    "    loss = np.mean(np.array(loss))\n",
    "    acc = np.mean(np.array(acc))\n",
    "    return loss, acc\n",
    "\n",
    "def test(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    for i, file in enumerate(data):\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "        for block in range(steps): \n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "            mel_files.append(clip_file)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "        logit = model.predict(mel_files)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        #print(logit.shape)\n",
    "        pred = np.argmax(logit)\n",
    "        \n",
    "        #clip_file = tmp_file[:minimum_len]\n",
    "        #clip_file = np.expand_dims(clip_file,0)\n",
    "        #clip_file -= x_mean_final\n",
    "        #clip_file /= x_std_final\n",
    "        #mel_files.append(clip_file)\n",
    "        label = np.argmax(get_labels(input_directory, file, class2index))\n",
    "        #print(pred, label)\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "    final_acc = total_acc / i\n",
    "    return final_acc\n",
    "\n",
    "batch_size = 32\n",
    "minimum_len = 128\n",
    "epochs = 50\n",
    "loss_function = 'categorical_crossentropy'\n",
    "activation_function = 'softmax'\n",
    "rootdir = '../'\n",
    "date = datetime.today().strftime(\"%Y%m%d\")\n",
    "input_directory = os.path.join(rootdir, 'Training_WFDB')\n",
    "# mel_name = 'Mel_data_20200402_128' \n",
    "# mel_name = 'Mel_data_20200421_melbin128_window1024_hop12'\n",
    "# mel_name = 'Mel_data_20200421_melbin128_window512_hop12'\n",
    "mel_name = 'Mel_data_20200421_melbin64_concatwindow16_512_hop12'\n",
    "mel_directory = os.path.join(rootdir, mel_name)\n",
    "results_directory = os.path.join(rootdir, 'results_'+date)\n",
    "if not os.path.isdir(input_directory):\n",
    "    os.mkdir(input_directory)\n",
    "if not os.path.isdir(mel_directory):\n",
    "    os.mkdir(mel_directory)\n",
    "if not os.path.isdir(results_directory):\n",
    "    os.mkdir(results_directory)\n",
    "        \n",
    "input_files = []\n",
    "for f in os.listdir(input_directory):\n",
    "    if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('mat'):\n",
    "        input_files.append(f)\n",
    "input_file_names = sorted(input_files)\n",
    "\n",
    "unique_classes = get_unique_classes(input_directory, input_files)\n",
    "class2index = {}\n",
    "for a, b in enumerate(unique_classes):\n",
    "    class2index[b] = a\n",
    "    \n",
    "classes_orig= [x.replace('.mat', '.hea') for x in input_file_names] # total subjects\n",
    "classes_multi, _, _ = searching_overlap(input_directory,class2index, input_file_names)\n",
    "classes_single = [x for x in classes_orig if x not in classes_multi]\n",
    "classes_single = [x.replace('.hea', '.mat') for x in classes_single]\n",
    "\n",
    "# double-checking if classes_single have single-label\n",
    "a, b, c  = searching_overlap(input_directory,class2index,classes_single)\n",
    "\n",
    "# we can safely use classes_single as input_file_names\n",
    "input_file_names = classes_single\n",
    "random.shuffle(input_file_names)\n",
    "np.shape(input_file_names)\n",
    "\n",
    "x_mean_all = []\n",
    "x_std_all = []\n",
    "for file in input_file_names:\n",
    "    x = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "    x_mean = [np.mean(x[:,:,0]), np.mean(x[:,:,1]), np.mean(x[:,:,2]), np.mean(x[:,:,3]), np.mean(x[:,:,4]), np.mean(x[:,:,5]),\n",
    "             np.mean(x[:,:,6]), np.mean(x[:,:,7]), np.mean(x[:,:,8]), np.mean(x[:,:,9]), np.mean(x[:,:,10]), np.mean(x[:,:,11])]\n",
    "    \n",
    "    x_std = [np.std(x[:,:,0]), np.std(x[:,:,1]), np.std(x[:,:,2]), np.std(x[:,:,3]), np.std(x[:,:,4]), np.std(x[:,:,5]),\n",
    "             np.std(x[:,:,6]), np.std(x[:,:,7]), np.std(x[:,:,8]), np.std(x[:,:,9]), np.std(x[:,:,10]), np.std(x[:,:,11])]\n",
    "    #print(x_mean)\n",
    "    x_mean_all.append(x_mean)\n",
    "    x_std_all.append(x_mean)\n",
    "x_mean_final = np.mean(x_mean_all, axis=0)\n",
    "x_std_final = np.mean(x_std_all, axis=0)\n",
    "print(x_mean_final)\n",
    "\n",
    "data, data_test = train_test_split(input_file_names, test_size = 0.2, train_size = 0.8, shuffle=True)\n",
    "data_train, data_val = train_test_split(data, test_size = 0.25, train_size = 0.75, shuffle=True)\n",
    "print(np.shape(data_train), np.shape(data_val), np.shape(data_test))\n",
    "\n",
    "input_tensor = Input(shape=(minimum_len, minimum_len, 12))\n",
    "input_tensor = GaussianNoise(0.01)(input_tensor)\n",
    "base_model = DenseNet121(input_tensor=input_tensor, weights=None, include_top=False)\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation=None)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(activation='relu')(x)\n",
    "pred = Dense(9, activation=activation_function)(x)\n",
    "model = Model(inputs=base_model.input, outputs=pred)\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=optimizers.Adam(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "val_acc_sum=[]\n",
    "train_loss_sum=[]\n",
    "train_acc_sum=[]\n",
    "val_loss_sum=[]\n",
    "val_acc_min = 0\n",
    "for num_epoch in range(epochs):\n",
    "    random.shuffle(data_train)\n",
    "    train_loss, train_acc = train(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "    print('\\nEpoch',num_epoch+1,'train_loss:',f'{train_loss:.3f}','train_acc:',f'{train_acc:.3f}',\"\\t\")\n",
    "    model_output = \"ecg_mel_E%02dL%.2f\" % (num_epoch, train_loss)\n",
    "    save_name = os.path.join(results_directory, model_output)\n",
    "    val_acc = test(data_val, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)\n",
    "    #print('\\nValidation', num_epoch+1,'valid_loss:',f'{val_loss:.3f}','valid_acc:',f'{val_acc:.3f}',\"\\t\", dt.datetime.now())\n",
    "    if val_acc > val_acc_min:\n",
    "        val_acc_min = val_acc\n",
    "        model.save(save_name)\n",
    "        #print(\"Validation loss is improved\")\n",
    "    print('\\nValidation', num_epoch+1, 'valid_acc:',f'{val_acc:.3f}', 'best_acc:',f'{val_acc_min:.3f}', \"\\t\")\n",
    "    #val_acc_sum.append(val_acc)\n",
    "    #val_loss_sum.append(val_loss)\n",
    "    #train_loss_sum.append(train_loss)\n",
    "    #train_acc_sum.append(train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
