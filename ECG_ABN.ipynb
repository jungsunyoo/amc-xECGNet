{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numba/errors.py:137: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow.contrib.eager as tfe\n",
    "\n",
    "\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import keras\n",
    "# import datetime as dt\n",
    "from datetime import datetime\n",
    "import time\n",
    "# import datetime.datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "# from keras import optimizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from keras.applications.densenet import DenseNet121, DenseNet169\n",
    "#from keras.applications.nasnet import NASNetLarge\n",
    "# from keras_efficientnets import EfficientNetB7\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "# from keras import backend as K\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "random.seed(100)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.24290386e-03 -4.58280585e-05  4.31697309e-03 -3.00174693e-03\n",
      " -2.36609229e-04  1.28997408e-03  2.17347589e-04 -7.99152384e-04\n",
      " -3.42993744e-03 -1.69711686e-03  1.27138164e-03  1.94670545e-03]\n",
      "(4864,) (256,) (1281,)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def score_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "def get_unique_classes(input_directory,files):\n",
    "\n",
    "    unique_classes=set()\n",
    "    for f in files:\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    for c in tmp:\n",
    "                        unique_classes.add(c.strip())\n",
    "    return sorted(unique_classes)\n",
    "\n",
    "def one_hot_encoding(one_hot_vector,y, class2index):\n",
    "    ind=class2index[y]\n",
    "    one_hot_vector[ind]=1\n",
    "    return one_hot_vector\n",
    "\n",
    "# Search for multi-label subjects\n",
    "def searching_overlap(input_directory,class2index, input_file_names):\n",
    "    multiclasses=[]\n",
    "    multisubjects=[]\n",
    "    number = []\n",
    "    for file in input_file_names:\n",
    "        f=file\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    if len(tmp)>1:\n",
    "                        one_hot_vector = [0]*(len(class2index))\n",
    "                        for c in tmp:\n",
    "                            one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                        multiclasses.append(one_hot_vector)\n",
    "                        multisubjects.append(g)\n",
    "                        number.append(len(tmp))\n",
    "    return multisubjects, multiclasses, number\n",
    "\n",
    "def block_feature(sequence_en, minimum_len): \n",
    "    new_en = []\n",
    "    if len(sequence_en) > minimum_len:  # 길이가 minimum보다 긴 경우\n",
    "        start = random.randint(0,len(sequence_en)-minimum_len)\n",
    "        #print(start)\n",
    "        new_en = sequence_en[start:start+minimum_len]\n",
    "    elif len(sequence_en) == minimum_len: # 길이가 minimum\n",
    "        new_en = sequence_en\n",
    "    else: \n",
    "        assert len(sequence_en) <= minimum_len\n",
    "    return new_en\n",
    "\n",
    "def exploratory_look(input_directory,file, class2index):\n",
    "    classes = []\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                print(tmp, len(tmp))\n",
    "    return tmp     \n",
    "\n",
    "# Get classes of sorted file names\n",
    "def get_labels(input_directory,file, class2index):\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                one_hot_vector = [0]*(len(class2index))\n",
    "                for c in tmp:\n",
    "                    one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                \n",
    "    return one_hot_vector\n",
    "\n",
    "def randextract_mels(curr_step, batch_size, data, mel_directory, class2index, minimum_len, x_mean_final, x_std_final):\n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    start = batch_size*curr_step\n",
    "    end = batch_size*(curr_step+1)\n",
    "    curr_file_indices = data[start:end]\n",
    "    for file in curr_file_indices:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        clip_file = block_feature(tmp_file, minimum_len)\n",
    "        #print(clip_file.shape)\n",
    "        #clip_file = tmp_file[:minimum_len]\n",
    "        clip_file -= x_mean_final\n",
    "        clip_file /= x_std_final\n",
    "        mel_files.append(clip_file)\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        classes.append(label)\n",
    "    \n",
    "    # YJS added for ABN -> should calculate 2 losses\n",
    "#     classes_abn = [classes,classes]\n",
    "    \n",
    "    concat = list(zip(mel_files, classes))\n",
    "#     concat = list(zip(mel_files, classes_abn))\n",
    "    random.shuffle(concat)\n",
    "    mel_files, classes = zip(*concat)\n",
    "    return mel_files, classes\n",
    "\n",
    "def train(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final): \n",
    "    loss=[]\n",
    "#     acc = []\n",
    "    f1 = []\n",
    "\n",
    "    total_steps = int(np.ceil(len(data_train)/batch_size))\n",
    "    for curr_step in range(total_steps): # loops over batches\n",
    "        batch_mels, batch_labels = randextract_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "\n",
    "        batch_mels = np.asarray(batch_mels)\n",
    "        batch_labels = np.asarray(np.squeeze(batch_labels))\n",
    "        \n",
    "        batch_labels = [batch_labels, batch_labels]\n",
    "\n",
    "\n",
    "# return of train\n",
    "# 0 = total loss (attention branch + perception branch)\n",
    "# 1 = loss of attention pred\n",
    "# 2 = loss of perception pred\n",
    "# 3 = f1 of attention pred\n",
    "# 4 = f1 of perception pred     \n",
    "\n",
    "\n",
    "        train_tmp = model.train_on_batch(batch_mels, batch_labels)\n",
    "        loss_ = train_tmp[0]/2\n",
    "        f1_ = np.mean((train_tmp[3], train_tmp[4]))\n",
    "        loss.append(loss_)\n",
    "        f1.append(f1_)\n",
    "\n",
    "    loss = np.mean(np.array(loss))\n",
    "    f1 = np.mean(np.array(f1))\n",
    "    return loss, f1\n",
    "\n",
    "def train_edit(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final): \n",
    "    loss=[]\n",
    "#     acc = []\n",
    "    f1 = []\n",
    "\n",
    "    total_steps = int(np.ceil(len(data_train)/batch_size))\n",
    "    for curr_step in range(total_steps): # loops over batches\n",
    "        batch_mels, batch_labels = randextract_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "\n",
    "        batch_mels = np.asarray(batch_mels)\n",
    "        batch_labels = np.asarray(np.squeeze(batch_labels))\n",
    "\n",
    "\n",
    "        heatmap = CAM_conv1D(p_model, conv_layer, softmax_layer,  x_mean_final, x_std_final, minimum_len, \n",
    "                                  n_channels, batch_mels, batch_labels, out_len)        \n",
    "        heatmap = np.asarray(heatmap)\n",
    "        \n",
    "        batch_labels = [batch_labels, batch_labels]\n",
    "\n",
    "\n",
    "# return of train\n",
    "# 0 = total loss (attention branch + perception branch)\n",
    "# 1 = loss of attention pred\n",
    "# 2 = loss of perception pred\n",
    "# 3 = f1 of attention pred\n",
    "# 4 = f1 of perception pred     \n",
    "\n",
    "        train_tmp = model.train_on_batch([batch_mels, heatmap], batch_labels)\n",
    "        loss_ = train_tmp[0]/2\n",
    "        f1_ = np.mean((train_tmp[3], train_tmp[4]))\n",
    "        loss.append(loss_)\n",
    "        f1.append(f1_)\n",
    "\n",
    "    loss = np.mean(np.array(loss))\n",
    "    f1 = np.mean(np.array(f1))\n",
    "    return loss, f1\n",
    "\n",
    "def test(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    for i, file in enumerate(data):\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "#         gradcam_files = []\n",
    "        heatmap_files=[]\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        label = [label]\n",
    "        for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "\n",
    "            clip_file = clip_file.reshape(1,minimum_len,n_channels)\n",
    "\n",
    "\n",
    "            mel_files.append(clip_file)    \n",
    "#             gradcam_files.append(gradcam)\n",
    "\n",
    "        mel_files = np.asarray(mel_files)\n",
    "\n",
    "        mel_files = mel_files.reshape(steps,minimum_len,n_channels)\n",
    "\n",
    "        logit = model.predict(mel_files)\n",
    "        # YJS changed on 2020-06-02: input으로 두개 들어가야하니까 predict도 수정?\n",
    "#         print(len(logit))\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "#         print(logit)\n",
    "        pred = np.argmax(logit)\n",
    "#         print('Pred={}'.format(pred))\n",
    "        \n",
    "        label = np.argmax(get_labels(input_directory, file, class2index))\n",
    "#         print('Label={}'.format(label))\n",
    "        #f1 = f1_score(label, logit)\n",
    "        #print(pred, label)\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        #total_f1 += f1\n",
    "    final_acc = total_acc / i\n",
    "    #final_f1 = total_f1 / i\n",
    "    return final_acc#, final_f1\n",
    "\n",
    "def test_edit(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    for i, file in enumerate(data):\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "#         gradcam_files = []\n",
    "        heatmap_files=[]\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        label = [label]\n",
    "        for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "#             heatmap = CAM_conv1D(p_model, conv_layer, softmax_layer,  x_mean_final, x_std_final, minimum_len, \n",
    "#                                       n_channels, clip_file, label, out_len)\n",
    "            \n",
    "#             heatmap = grad_cam_conv1D(p_model, layer_nm, x_mean_final, x_std_final, minimum_len, n_channels, \n",
    "#                                      clip_file, label, out_len, sample_weight=1, keras_phase=0)\n",
    "            clip_file = clip_file.reshape(1,minimum_len,n_channels)\n",
    "            heatmap = CAM_conv1D(p_model, conv_layer, softmax_layer,  x_mean_final, x_std_final, minimum_len, \n",
    "                                      n_channels, clip_file, label, out_len)\n",
    "#             print(len(heatmap))\n",
    "#             print(len(heatmap[0]))\n",
    "#             print(len(heatmap[0][0]))\n",
    "#             print(len(heatmap[0][0][0]))\n",
    "            mel_files.append(clip_file)    \n",
    "#             gradcam_files.append(gradcam)\n",
    "            heatmap_files.append(heatmap)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "        heatmap_files = np.asarray(heatmap_files)\n",
    "        \n",
    "#         print(heatmap_files.shape)\n",
    "        heatmap_files = heatmap_files.reshape(steps, out_len,1)\n",
    "        mel_files = mel_files.reshape(steps,minimum_len,n_channels)\n",
    "#         heatmap = np.asarray(heatmap)\n",
    "#         gradcam_files = np.asarray(gradcam_files)\n",
    "# grad_cam_conv1D(model, layer_nm,x_mean_final, x_std_final, minimum_len, n_channels, x, y,sample_weight=1,  keras_phase=0)        \n",
    "        \n",
    "# get_labels(input_directory,file, class2index)\n",
    "\n",
    "\n",
    "\n",
    "#         logit = model.predict(mel_files)\n",
    "#         logit = model.predict([mel_files, gradcam_files])\n",
    "        logit = model.predict([mel_files, heatmap_files])\n",
    "        # YJS changed on 2020-06-02: input으로 두개 들어가야하니까 predict도 수정?\n",
    "#         print(len(logit))\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "#         print(logit)\n",
    "        pred = np.argmax(logit)\n",
    "#         print('Pred={}'.format(pred))\n",
    "        \n",
    "        label = np.argmax(get_labels(input_directory, file, class2index))\n",
    "#         print('Label={}'.format(label))\n",
    "        #f1 = f1_score(label, logit)\n",
    "        #print(pred, label)\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        #total_f1 += f1\n",
    "    final_acc = total_acc / i\n",
    "    #final_f1 = total_f1 / i\n",
    "    return final_acc#, final_f1\n",
    "\n",
    "\n",
    "def test_short(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    for i, file in enumerate(data):\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "#         gradcam_files = []\n",
    "        heatmap_files=[]\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        label = [label]\n",
    "#         for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "        steps=1\n",
    "        block = 0\n",
    "        start = block*minimum_len\n",
    "        end = (block+1)*minimum_len\n",
    "        clip_file = tmp_file[start:end]\n",
    "        clip_file -= x_mean_final\n",
    "        clip_file /= x_std_final\n",
    "\n",
    "        clip_file = clip_file.reshape(1,minimum_len,n_channels)\n",
    "#         heatmap = CAM_conv1D(p_model, conv_layer, softmax_layer,  x_mean_final, x_std_final, minimum_len, \n",
    "#                                       n_channels, clip_file, label, out_len)\n",
    "\n",
    "        mel_files.append(clip_file)    \n",
    "#         heatmap_files.append(heatmap)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "#         heatmap_files = np.asarray(heatmap_files)\n",
    "#         heatmap_files = heatmap_files.reshape(steps,out_len,1) # changed for modified attention editting\n",
    "        mel_files = mel_files.reshape(steps,minimum_len,n_channels)\n",
    "\n",
    "        logit = model.predict([mel_files, heatmap_files])\n",
    "        # YJS changed on 2020-06-02: input으로 두개 들어가야하니까 predict도 수정?\n",
    "\n",
    "        \n",
    "        logit = np.mean(logit, axis=0)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        pred = np.argmax(logit)\n",
    "        \n",
    "        label = np.argmax(get_labels(input_directory, file, class2index))\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "\n",
    "        total_acc += acc\n",
    "\n",
    "    final_acc = total_acc / i\n",
    "\n",
    "    return final_acc#, final_f1\n",
    "batch_size = 32#16#20#32#5#2#1#10#32\n",
    "minimum_len = 2880\n",
    "epochs = 1000\n",
    "loss_function = 'categorical_crossentropy'\n",
    "activation_function = 'softmax'\n",
    "rootdir = '../'\n",
    "date = datetime.today().strftime(\"%Y%m%d\")\n",
    "input_directory = os.path.join(rootdir, 'Training_WFDB')\n",
    "mel_name = 'Raw_data_20200424' \n",
    "mel_directory = os.path.join(rootdir, mel_name)\n",
    "# results_directory = os.path.join(rootdir, 'results_'+date+'_0_IEEE_n=1')\n",
    "results_directory = os.path.join(rootdir, 'results_'+date+'_editting_CAM_changed_splitratio_testlong')\n",
    "if not os.path.isdir(input_directory):\n",
    "    os.mkdir(input_directory)\n",
    "if not os.path.isdir(mel_directory):\n",
    "    os.mkdir(mel_directory)\n",
    "if not os.path.isdir(results_directory):\n",
    "    os.mkdir(results_directory)\n",
    "        \n",
    "input_files = []\n",
    "for f in os.listdir(input_directory):\n",
    "    if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('mat'):\n",
    "        input_files.append(f)\n",
    "input_file_names = sorted(input_files)\n",
    "\n",
    "unique_classes = get_unique_classes(input_directory, input_files)\n",
    "class2index = {}\n",
    "for a, b in enumerate(unique_classes):\n",
    "    class2index[b] = a\n",
    "    \n",
    "classes_orig= [x.replace('.mat', '.hea') for x in input_file_names] # total subjects\n",
    "classes_multi, _, _ = searching_overlap(input_directory,class2index, input_file_names)\n",
    "classes_single = [x for x in classes_orig if x not in classes_multi]\n",
    "classes_single = [x.replace('.hea', '.mat') for x in classes_single]\n",
    "\n",
    "# double-checking if classes_single have single-label\n",
    "a, b, c  = searching_overlap(input_directory,class2index,classes_single)\n",
    "\n",
    "# we can safely use classes_single as input_file_names\n",
    "input_file_names = classes_single\n",
    "random.shuffle(input_file_names)\n",
    "np.shape(input_file_names)\n",
    "\n",
    "x_mean_all = []\n",
    "x_std_all = []\n",
    "for file in input_file_names:\n",
    "    x = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "    x_mean = [np.mean(x[:,0]), np.mean(x[:,1]), np.mean(x[:,2]), np.mean(x[:,3]), np.mean(x[:,4]), np.mean(x[:,5]),\n",
    "             np.mean(x[:,6]), np.mean(x[:,7]), np.mean(x[:,8]), np.mean(x[:,9]), np.mean(x[:,10]), np.mean(x[:,11])]\n",
    "    \n",
    "    x_std = [np.std(x[:,0]), np.std(x[:,1]), np.std(x[:,2]), np.std(x[:,3]), np.std(x[:,4]), np.std(x[:,5]),\n",
    "             np.std(x[:,6]), np.std(x[:,7]), np.std(x[:,8]), np.std(x[:,9]), np.std(x[:,10]), np.std(x[:,11])]\n",
    "    #print(x_mean)\n",
    "    x_mean_all.append(x_mean)\n",
    "    x_std_all.append(x_std) # yjs corrected on 2020-05-25\n",
    "x_mean_final = np.mean(x_mean_all, axis=0)\n",
    "x_std_final = np.mean(x_std_all, axis=0)\n",
    "print(x_mean_final)\n",
    "\n",
    "data, data_test = train_test_split(input_file_names, test_size = 0.2, train_size = 0.8, shuffle=True)\n",
    "data_train, data_val = train_test_split(data, test_size = 0.05, train_size = 0.95, shuffle=True)\n",
    "print(np.shape(data_train), np.shape(data_val), np.shape(data_test))\n",
    "\n",
    "\n",
    "main_input = Input(shape=(minimum_len,12), dtype='float32', name='main_input')\n",
    "\n",
    "branch_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# val_acc = test_short(data_val, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention editting by CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_mels, batch_labels = randextract_mels(1, batch_size, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_x = batch_mels[0]\n",
    "# curr_x = curr_x.reshape(1,minimum_len,n_channels)\n",
    "# print(curr_x.shape)\n",
    "# softmax_layer = 'dense_final'\n",
    "# softmax_out = p_model.get_layer(softmax_layer)\n",
    "# softmax_weights = softmax_out.weights\n",
    "\n",
    "# conv_layer = 'batch_normalization_12'\n",
    "# conv_out = p_model.get_layer(conv_layer).output\n",
    "# get_conv_out = K.function(p_model.input, [conv_out, softmax_weights[0]])\n",
    "# conv_out, softmax_weights = get_conv_out(curr_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Batch-wise to reduce time\n",
    "\n",
    "CAMdir = '/home/taejoon/PhysioNetChallenge/results_20200601_1_CAM_primitive_model'\n",
    "bestmodel = 'ECG_ABN_E87L0.33'\n",
    "p_model = tf.keras.models.load_model(os.path.join(CAMdir, bestmodel), custom_objects={'score_f1' : score_f1}) # primitive model\n",
    "conv_layer = 'batch_normalization_12'\n",
    "softmax_layer = 'dense_final'\n",
    "n_channels=12\n",
    "out_len=12#128\n",
    "softmax_weights = p_model.get_layer('dense_final').weights[0]\n",
    "# model.get_layer(softmax_layer).weights[0]\n",
    "def CAM_conv1D(model, conv_layer, softmax_layer, x_mean_final, x_std_final, minimum_len, \n",
    "                    n_channels, x, y, out_len):\n",
    "    \n",
    "    # x랑 y는 batch size만큼의 리스트 (32)\n",
    "    heatmaps=[]    \n",
    "    \n",
    "    curr_x = np.asarray(x)\n",
    "    curr_x = curr_x.reshape(len(x),minimum_len,n_channels)\n",
    "    get_conv_out = K.function(model.input, [model.get_layer(conv_layer).output, model.get_layer(softmax_layer).weights[0]])\n",
    "    conv_out, softmax_weights = get_conv_out(curr_x)\n",
    "\n",
    "    for i in range(len(x)):\n",
    "\n",
    "        curr_classes = y[i]\n",
    "        class_index=[]\n",
    "        [class_index.append(j) for j in range(len(curr_classes)) if curr_classes[j]==1]\n",
    "        heatmap=np.zeros((1,36)) # might need to fix this if GradCAM or primitive model changes\n",
    "\n",
    "        conv_out_ = conv_out[i] # (36, 128)\n",
    "\n",
    "\n",
    "        for label in class_index:  # multiclass일 경우 대비해서 for문\n",
    "            curr_weights = softmax_weights[:,label]\n",
    "            weighted_conv = conv_out_*curr_weights\n",
    "            \n",
    "            weighted_conv = weighted_conv.sum(axis=-1) # output = (1,36)\n",
    "            heatmap += weighted_conv\n",
    "            \n",
    "\n",
    "        heatmap %= len(class_index) # 단일 class일 경우 1로 나눠짐. 두개일 경우 더해진 heatmap들이 2로 나눠짐\n",
    "#         heatmap = np.resize(heatmap, (1,out_len))\n",
    "        heatmap = np.resize(heatmap, (out_len, 1))\n",
    "        heatmaps.append(heatmap)\n",
    "        \n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# heatmap = CAM_conv1D(p_model, conv_layer, softmax_layer,  x_mean_final, x_std_final, minimum_len, \n",
    "#                                   n_channels, batch_mels, batch_labels, out_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(heatmap[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(heatmap[0][0][0]) # 32, 1, 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_x, _ = randextract_mels(0,32, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "# curr_x = np.asarray(curr_x)\n",
    "# curr_x.shape\n",
    "# curr_x = curr_x.reshape(32,minimum_len,12)\n",
    "\n",
    "# # curr_x = curr_x[0]\n",
    "# # curr_x = curr_x.reshape(1,minimum_len,12)\n",
    "# CAMdir = '/home/taejoon/PhysioNetChallenge/results_20200601_1_CAM_primitive_model'\n",
    "# bestmodel = 'ECG_ABN_E87L0.33'\n",
    "# p_model = tf.keras.models.load_model(os.path.join(CAMdir, bestmodel), custom_objects={'score_f1' : score_f1}) # primitive model\n",
    "# conv_layer = 'batch_normalization_12'\n",
    "# softmax_layer = 'dense_final'\n",
    "# n_channels=12\n",
    "# out_len=128\n",
    "# softmax_weights = p_model.get_layer('dense_final').weights[0]\n",
    "\n",
    "# get_conv_out = K.function(p_model.input, p_model.get_layer(conv_layer).output)\n",
    "\n",
    "# # conv_out = get_conv_out(curr_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_out = get_conv_out(curr_x)\n",
    "# conv_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAMdir = '/home/taejoon/PhysioNetChallenge/results_20200601_1_CAM_primitive_model'\n",
    "# bestmodel = 'ECG_ABN_E87L0.33'\n",
    "# p_model = tf.keras.models.load_model(os.path.join(CAMdir, bestmodel), custom_objects={'score_f1' : score_f1}) # primitive model\n",
    "# conv_layer = 'batch_normalization_12'\n",
    "# softmax_layer = 'dense_final'\n",
    "# n_channels=12\n",
    "# out_len=128\n",
    "# def CAM_conv1D(model, conv_layer, softmax_layer, x_mean_final, x_std_final, minimum_len, \n",
    "#                     n_channels, x, y, out_len):\n",
    "    \n",
    "#     # x랑 y는 batch size만큼의 리스트 (32)\n",
    "       \n",
    "#     #레이어 이름에 해당되는 레이어 정보를 가져옴 \n",
    "# #     conv_out = model.get_layer(conv_layer).output\n",
    "# #     print(conv_out)\n",
    "# #     conv_output= conv_out.output\n",
    "# #     softmax_out = model.get_layer(softmax_layer)\n",
    "# #     softmax_weights = softmax_out.weights\n",
    "#     heatmaps=[]    \n",
    "#     for file in range(len(x)): # 굳이 이렇게 batch 따로돌리는 이유는 나중에 multilable일 때 heatmap 두개일 경우를 대비하기위해\n",
    "#         # 일단은 single label로 하기 \n",
    "#         class_index=[]\n",
    "#         curr_x  = x[file]\n",
    "#         curr_x = curr_x.reshape(1,minimum_len,n_channels)\n",
    "#         curr_classes = y[file]\n",
    "#         [class_index.append(i) for i in range(len(curr_classes)) if curr_classes[i]==1]\n",
    "#         heatmap=np.zeros((1,36)) # might need to fix this if GradCAM or primitive model changes\n",
    "# #         curr_x = tf.convert_to_tensor(curr_x)\n",
    "#         get_conv_out = K.function(model.input, [model.get_layer(conv_layer).output, model.get_layer(softmax_layer).weights[0]])\n",
    "#         conv_out, softmax_weights = get_conv_out(curr_x)\n",
    "#         print(file)\n",
    "#         for label in class_index:  # multiclass일 경우 대비해서 for문\n",
    "#             curr_weights = softmax_weights[:,label]\n",
    "#             weighted_conv = conv_out*curr_weights\n",
    "#             weighted_conv = weighted_conv.sum(axis=2) # output = (1,36)            \n",
    "#             heatmap += weighted_conv\n",
    "            \n",
    "#         heatmap %= len(class_index) # 단일 class일 경우 1로 나눠짐. 두개일 경우 더해진 heatmap들이 2로 나눠짐\n",
    "#         heatmap = np.resize(heatmap, (1,out_len))\n",
    "# #         heatmap = cv2.resize(heatmap, (1,out_len))\n",
    "#         heatmaps.append(heatmap)\n",
    "        \n",
    "#     return heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention editting by GradCAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradCAMdir = '/home/taejoon/PhysioNetChallenge/results_20200601_1_CAM_primitive_model'\n",
    "# bestmodel = 'ECG_ABN_E87L0.33'\n",
    "# p_model = tf.keras.models.load_model(os.path.join(GradCAMdir, bestmodel), custom_objects={'score_f1' : score_f1}) # primitive model\n",
    "# layer_nm = 'conv1d_12'\n",
    "# n_channels=12\n",
    "# out_len=128\n",
    "# def grad_cam_conv1D(model, layer_nm,x_mean_final, x_std_final, minimum_len, \n",
    "#                     n_channels, x, y, out_len,sample_weight=1,  keras_phase=0):\n",
    "    \n",
    "#     # x랑 y는 batch size만큼의 리스트 (32)\n",
    "       \n",
    "#     #레이어 이름에 해당되는 레이어 정보를 가져옴 \n",
    "#     layers_wt = model.get_layer(layer_nm).weights\n",
    "#     layers = model.get_layer(layer_nm)\n",
    "#     layers_weights = model.get_layer(layer_nm).get_weights()\n",
    "\n",
    "#     heatmaps=[]    \n",
    "#     for file in range(len(x)): # 굳이 이렇게 batch 따로돌리는 이유는 나중에 multilable일 때 heatmap 두개일 경우를 대비하기위해\n",
    "#         # 일단은 single label로 하기 \n",
    "#         print(file)\n",
    "#         class_index=[]\n",
    "#         curr_x  = x[file]\n",
    "# #         print(curr_x.shape)\n",
    "#         curr_x = curr_x.reshape(1,minimum_len,n_channels)\n",
    "#         curr_classes = y[file]\n",
    "# #         print(curr_classes)\n",
    "# #         print(curr_classes.shape)\n",
    "#         [class_index.append(i) for i in range(len(curr_classes)) if curr_classes[i]==1]\n",
    "#         heatmap=np.zeros(36) # might need to fix this if GradCAM or primitive model changes\n",
    "#         for label in class_index:  # multiclass일 경우 대비해서 for문\n",
    "\n",
    "#             #긍정 클래스를 설명할 수 있게 컨볼루션 필터 가중치의 gradient를 구함  \n",
    "#             grads = K.gradients(model.output[:,label], layers_wt)[0]\n",
    "\n",
    "#             #필터별로 가중치를 구함 \n",
    "#             pooled_grads = K.mean(grads, axis=(0,1))\n",
    "#             get_pooled_grads = K.function(model.input, \n",
    "#                                  [pooled_grads, layers.output[0]])\n",
    "\n",
    "#             pooled_grads_value, conv_layer_output_value = get_pooled_grads(curr_x)\n",
    "# #             print(conv_layer_output_value.shape) # (36,128)\n",
    "#             for i in range(conv_layer_output_value.shape[-1]):\n",
    "#                 conv_layer_output_value[:, i] *= pooled_grads_value[i]\n",
    "#                 for j in range(len(conv_layer_output_value[:,i])):\n",
    "#                     conv_layer_output_value[j,i] = max(0, conv_layer_output_value[j,i])\n",
    "#                 #YJS manually added RELU function on 2020-06-02\n",
    "# #                 conv_layer_output_value[:,i] = max(0, conv_layer_output_value[:,i])\n",
    "#             heatmap += np.mean(conv_layer_output_value, axis=-1)\n",
    "# #         print(class_index)\n",
    "#         heatmap %= len(class_index) # 단일 class일 경우 1로 나눠짐. 두개일 경우 더해진 heatmap들이 2로 나눠짐\n",
    "#         heatmap = cv2.resize(heatmap, (1,out_len))\n",
    "#         heatmaps.append(heatmap)\n",
    "# #     heatmaps = heatmaps.reshape(len(x),1,36)\n",
    "#     return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, 12)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, None, 64)     2368        input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, 64)     256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 64)     12352       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 64)     256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, None, 64)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 128)    24704       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, 128)    512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 128)    49280       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, 128)    512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, None, 128)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 256)    98560       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, 256)    1024        conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 256)    196864      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, 256)    1024        conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 256)    196864      batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, 256)    1024        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, None, 256)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 512)    393728      max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, 512)    2048        conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, None, 512)    786944      batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, 512)    2048        conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, None, 512)    786944      batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, 512)    2048        conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, None, 512)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, None, 512)    786944      max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, 512)    2048        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, None, 256)    393472      batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, 256)    1024        conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, None, 128)    98432       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, 128)    512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, None, 128)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, None, 64)     8192        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, 64)     256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, None, 64)     12288       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, 64)     256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 64)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, None, 256)    16384       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, None, 256)    32768       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, None, 256)    1024        conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, 256)    1024        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, None, 256)    0           batch_normalization_16[0][0]     \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 256)    0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_bn_1 (BatchNor (None, None, 256)    1024        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_conv_1 (Conv1D (None, None, 9)      2304        attention_branch_bn_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_conv_1 (Co (None, None, 1)      9           attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_bn_1 (Batc (None, None, 1)      4           attention_branch_att_conv_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_sigmoid_1  (None, None, 1)      0           attention_branch_att_bn_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "heatmap_image (InputLayer)      [(None, None, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, None, 1)      0           attention_branch_att_sigmoid_1[0]\n",
      "                                                                 heatmap_image[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 128)    0           max_pooling1d_4[0][0]            \n",
      "                                                                 lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, None, 64)     8192        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, None, 64)     256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, None, 64)     12288       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, None, 64)     256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, 64)     0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, None, 256)    16384       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, None, 256)    32768       lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, None, 256)    1024        conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, None, 256)    1024        conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, 256)    0           batch_normalization_20[0][0]     \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, 256)    0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_avgpool_1 (Gl (None, 256)          0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_pred_conv_1 (C (None, None, 9)      81          attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_1 (Dens (None, 256)          65792       perception_branch_avgpool_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_gap_1 (GlobalA (None, 9)            0           attention_branch_pred_conv_1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_2 (Dens (None, 9)            2313        perception_branch_dense_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_output (Softma (None, 9)            0           attention_branch_gap_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_output (Softm (None, 9)            0           perception_branch_dense_2[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 4,057,703\n",
      "Trainable params: 4,047,461\n",
      "Non-trainable params: 10,242\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from ABNmodules import *\n",
    "\n",
    "# def get_custom_model(input_shape, n_classes, minimum_len, target_classes, out_ch=256, n=18):\n",
    "# model = get_custom_model((None, 12), 9, minimum_len, 1, out_ch=256, n=1)\n",
    "# model = get_model((None, 12), 9, n=7)\n",
    "\n",
    "# model = cam_model((None, 12), 9, minimum_len, out_ch=256, n=18)\n",
    "\n",
    "\n",
    "model = edit_model((None,12), len(unique_classes), minimum_len)\n",
    "# model = get_custom_model((None, 12), 9, minimum_len, 1, out_ch=256, n=1)\n",
    "model.summary()\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=optimizers.Adam(lr=1e-5),           \n",
    "              metrics=[score_f1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 중단된 training 이어돌리기위해 임시로 사용\n",
    "# results_directory = results_directory.replace(\"0608\", \"0604\")\n",
    "# results_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 중단된 training 이어돌리기 위해 임시로 사용\n",
    "# latest = tf.train.latest_checkpoint(results_directory)\n",
    "# latest\n",
    "# model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results_20200611_editting_CAM_changed_splitratio_testlong\n",
      "\n",
      "Epoch 1 train_loss: 2.029 train_f1: 0.083 \t\n",
      "\n",
      "Validation 1 valid_acc: 0.482 best_acc: 0.482 \t\n",
      "\n",
      "Epoch 2 train_loss: 1.675 train_f1: 0.209 \t\n",
      "\n",
      "Validation 2 valid_acc: 0.627 best_acc: 0.627 \t\n",
      "\n",
      "Epoch 3 train_loss: 1.537 train_f1: 0.264 \t\n",
      "\n",
      "Validation 3 valid_acc: 0.678 best_acc: 0.678 \t\n",
      "\n",
      "Epoch 4 train_loss: 1.438 train_f1: 0.332 \t\n",
      "\n",
      "Validation 4 valid_acc: 0.702 best_acc: 0.702 \t\n",
      "\n",
      "Epoch 5 train_loss: 1.367 train_f1: 0.390 \t\n",
      "\n",
      "Validation 5 valid_acc: 0.714 best_acc: 0.714 \t\n",
      "\n",
      "Epoch 6 train_loss: 1.280 train_f1: 0.448 \t\n",
      "\n",
      "Validation 6 valid_acc: 0.733 best_acc: 0.733 \t\n",
      "\n",
      "Epoch 7 train_loss: 1.232 train_f1: 0.480 \t\n",
      "\n",
      "Validation 7 valid_acc: 0.757 best_acc: 0.757 \t\n",
      "\n",
      "Epoch 8 train_loss: 1.178 train_f1: 0.517 \t\n",
      "\n",
      "Validation 8 valid_acc: 0.753 best_acc: 0.757 \t\n",
      "\n",
      "Epoch 9 train_loss: 1.142 train_f1: 0.550 \t\n",
      "\n",
      "Validation 9 valid_acc: 0.765 best_acc: 0.765 \t\n",
      "\n",
      "Epoch 10 train_loss: 1.109 train_f1: 0.574 \t\n",
      "\n",
      "Validation 10 valid_acc: 0.784 best_acc: 0.784 \t\n",
      "\n",
      "Epoch 11 train_loss: 1.082 train_f1: 0.596 \t\n",
      "\n",
      "Validation 11 valid_acc: 0.780 best_acc: 0.784 \t\n",
      "\n",
      "Epoch 12 train_loss: 1.054 train_f1: 0.615 \t\n",
      "\n",
      "Validation 12 valid_acc: 0.784 best_acc: 0.784 \t\n",
      "\n",
      "Epoch 13 train_loss: 1.024 train_f1: 0.631 \t\n",
      "\n",
      "Validation 13 valid_acc: 0.776 best_acc: 0.784 \t\n",
      "\n",
      "Epoch 14 train_loss: 1.000 train_f1: 0.647 \t\n",
      "\n",
      "Validation 14 valid_acc: 0.780 best_acc: 0.784 \t\n",
      "\n",
      "Epoch 15 train_loss: 0.974 train_f1: 0.666 \t\n",
      "\n",
      "Validation 15 valid_acc: 0.784 best_acc: 0.784 \t\n",
      "\n",
      "Epoch 16 train_loss: 0.964 train_f1: 0.672 \t\n",
      "\n",
      "Validation 16 valid_acc: 0.784 best_acc: 0.784 \t\n",
      "\n",
      "Epoch 17 train_loss: 0.953 train_f1: 0.680 \t\n",
      "\n",
      "Validation 17 valid_acc: 0.757 best_acc: 0.784 \t\n",
      "\n",
      "Epoch 18 train_loss: 0.946 train_f1: 0.681 \t\n",
      "\n",
      "Validation 18 valid_acc: 0.780 best_acc: 0.784 \t\n",
      "\n",
      "Epoch 19 train_loss: 0.917 train_f1: 0.701 \t\n",
      "\n",
      "Validation 19 valid_acc: 0.765 best_acc: 0.784 \t\n",
      "\n",
      "Epoch 20 train_loss: 0.917 train_f1: 0.699 \t\n",
      "\n",
      "Validation 20 valid_acc: 0.780 best_acc: 0.784 \t\n",
      "\n",
      "Epoch 21 train_loss: 0.891 train_f1: 0.709 \t\n",
      "\n",
      "Validation 21 valid_acc: 0.769 best_acc: 0.784 \t\n",
      "\n",
      "Epoch 22 train_loss: 0.878 train_f1: 0.716 \t\n",
      "\n",
      "Validation 22 valid_acc: 0.765 best_acc: 0.784 \t\n",
      "\n",
      "Epoch 23 train_loss: 0.856 train_f1: 0.724 \t\n",
      "\n",
      "Validation 23 valid_acc: 0.761 best_acc: 0.784 \t\n",
      "\n",
      "Epoch 24 train_loss: 0.859 train_f1: 0.725 \t\n",
      "\n",
      "Validation 24 valid_acc: 0.765 best_acc: 0.784 \t\n",
      "\n",
      "Epoch 25 train_loss: 0.836 train_f1: 0.730 \t\n",
      "\n",
      "Validation 25 valid_acc: 0.769 best_acc: 0.784 \t\n",
      "\n",
      "Epoch 26 train_loss: 0.821 train_f1: 0.737 \t\n",
      "\n",
      "Validation 26 valid_acc: 0.757 best_acc: 0.784 \t\n",
      "\n",
      "Epoch 27 train_loss: 0.825 train_f1: 0.736 \t\n",
      "\n",
      "Validation 27 valid_acc: 0.776 best_acc: 0.784 \t\n",
      "\n",
      "Epoch 28 train_loss: 0.806 train_f1: 0.745 \t\n",
      "\n",
      "Validation 28 valid_acc: 0.769 best_acc: 0.784 \t\n",
      "\n",
      "Epoch 29 train_loss: 0.805 train_f1: 0.743 \t\n",
      "\n",
      "Validation 29 valid_acc: 0.788 best_acc: 0.788 \t\n",
      "\n",
      "Epoch 30 train_loss: 0.786 train_f1: 0.748 \t\n"
     ]
    }
   ],
   "source": [
    "# DISP DATETIME FOR CHECKING TIME\n",
    "start = time.time()\n",
    "val_acc_sum=[]\n",
    "train_loss_sum=[]\n",
    "train_acc_sum=[]\n",
    "val_loss_sum=[]\n",
    "val_acc_min = 0\n",
    "print(results_directory)\n",
    "\n",
    "for num_epoch in range(epochs):\n",
    "#     num_epoch += 32 # 중단된 코드 돌리기 위해 임의로 사용\n",
    "    random.shuffle(data_train)\n",
    "    train_loss, train_f1 = train_edit(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "    print('\\nEpoch',num_epoch+1,'train_loss:',f'{train_loss:.3f}','train_f1:',f'{train_f1:.3f}',\"\\t\")\n",
    "    model_output = \"ECG_ABN_E%02dL%.2f\" % (num_epoch, train_loss)\n",
    "    save_name = os.path.join(results_directory, model_output)\n",
    "    \n",
    "    val_acc = test_edit(data_val, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)\n",
    "\n",
    "    if val_acc > val_acc_min:\n",
    "        val_acc_min = val_acc\n",
    "        model.save_weights(save_name.format(epoch=0))\n",
    "#         model.save(save_name)\n",
    "#         cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_name,\n",
    "#                                                          save_weights_only=True,\n",
    "#                                                          verbose=1)\n",
    "    print('\\nValidation', num_epoch+1, 'valid_acc:',f'{val_acc:.3f}', 'best_acc:',f'{val_acc_min:.3f}', \"\\t\")\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_mels =np.asarray(batch_mels)\n",
    "# batch_mels = batch_mels.reshape(32,2880,12)\n",
    "# heatmap = np.asarray(heatmap)\n",
    "# heatmap = heatmap.reshape(batch_size, 1, out_len)\n",
    "# a = model2.predict([batch_mels, heatmap])\n",
    "# a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "617.813px",
    "left": "1544.27px",
    "right": "20px",
    "top": "79.75px",
    "width": "296.719px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
