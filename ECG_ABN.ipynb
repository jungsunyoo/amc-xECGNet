{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numba/errors.py:137: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import keras\n",
    "# import datetime as dt\n",
    "from datetime import datetime\n",
    "import time\n",
    "# import datetime.datetime\n",
    "\n",
    "\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "# from keras import optimizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from keras.applications.densenet import DenseNet121, DenseNet169\n",
    "#from keras.applications.nasnet import NASNetLarge\n",
    "# from keras_efficientnets import EfficientNetB7\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from keras import backend as K\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "random.seed(100)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.24290386e-03 -4.58280585e-05  4.31697309e-03 -3.00174693e-03\n",
      " -2.36609229e-04  1.28997408e-03  2.17347589e-04 -7.99152384e-04\n",
      " -3.42993744e-03 -1.69711686e-03  1.27138164e-03  1.94670545e-03]\n",
      "(3840,) (1280,) (1281,)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def score_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "def get_unique_classes(input_directory,files):\n",
    "\n",
    "    unique_classes=set()\n",
    "    for f in files:\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    for c in tmp:\n",
    "                        unique_classes.add(c.strip())\n",
    "    return sorted(unique_classes)\n",
    "\n",
    "def one_hot_encoding(one_hot_vector,y, class2index):\n",
    "    ind=class2index[y]\n",
    "    one_hot_vector[ind]=1\n",
    "    return one_hot_vector\n",
    "\n",
    "# Search for multi-label subjects\n",
    "def searching_overlap(input_directory,class2index, input_file_names):\n",
    "    multiclasses=[]\n",
    "    multisubjects=[]\n",
    "    number = []\n",
    "    for file in input_file_names:\n",
    "        f=file\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    if len(tmp)>1:\n",
    "                        one_hot_vector = [0]*(len(class2index))\n",
    "                        for c in tmp:\n",
    "                            one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                        multiclasses.append(one_hot_vector)\n",
    "                        multisubjects.append(g)\n",
    "                        number.append(len(tmp))\n",
    "    return multisubjects, multiclasses, number\n",
    "\n",
    "def block_feature(sequence_en, minimum_len): \n",
    "    new_en = []\n",
    "    if len(sequence_en) > minimum_len:  # 길이가 minimum보다 긴 경우\n",
    "        start = random.randint(0,len(sequence_en)-minimum_len)\n",
    "        #print(start)\n",
    "        new_en = sequence_en[start:start+minimum_len]\n",
    "    elif len(sequence_en) == minimum_len: # 길이가 minimum\n",
    "        new_en = sequence_en\n",
    "    else: \n",
    "        assert len(sequence_en) <= minimum_len\n",
    "    return new_en\n",
    "\n",
    "def exploratory_look(input_directory,file, class2index):\n",
    "    classes = []\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                print(tmp, len(tmp))\n",
    "    return tmp     \n",
    "\n",
    "# Get classes of sorted file names\n",
    "def get_labels(input_directory,file, class2index):\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                one_hot_vector = [0]*(len(class2index))\n",
    "                for c in tmp:\n",
    "                    one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                \n",
    "    return one_hot_vector\n",
    "\n",
    "def randextract_mels(curr_step, batch_size, data, mel_directory, class2index, minimum_len, x_mean_final, x_std_final):\n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    start = batch_size*curr_step\n",
    "    end = batch_size*(curr_step+1)\n",
    "    curr_file_indices = data[start:end]\n",
    "    for file in curr_file_indices:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        clip_file = block_feature(tmp_file, minimum_len)\n",
    "        #print(clip_file.shape)\n",
    "        #clip_file = tmp_file[:minimum_len]\n",
    "        clip_file -= x_mean_final\n",
    "        clip_file /= x_std_final\n",
    "        mel_files.append(clip_file)\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        classes.append(label)\n",
    "    \n",
    "    # YJS added for ABN -> should calculate 2 losses\n",
    "#     classes_abn = [classes,classes]\n",
    "    \n",
    "    concat = list(zip(mel_files, classes))\n",
    "#     concat = list(zip(mel_files, classes_abn))\n",
    "    random.shuffle(concat)\n",
    "    mel_files, classes = zip(*concat)\n",
    "    return mel_files, classes\n",
    "\n",
    "def train(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final): \n",
    "    loss=[]\n",
    "#     acc = []\n",
    "    f1 = []\n",
    "\n",
    "    total_steps = int(np.ceil(len(data_train)/batch_size))\n",
    "    for curr_step in range(total_steps):\n",
    "        batch_mels, batch_labels = randextract_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "        batch_mels = np.asarray(batch_mels)\n",
    "        batch_labels2 = np.asarray(np.squeeze(batch_labels))\n",
    "        batch_labels = [batch_labels2, batch_labels2]\n",
    "#         print(batch_labels.shape)\n",
    "#         print(batch_labels)\n",
    "\n",
    "# return of train\n",
    "# 0 = total loss (attention branch + perception branch)\n",
    "# 1 = loss of attention pred\n",
    "# 2 = loss of perception pred\n",
    "# 3 = f1 of attention pred\n",
    "# 4 = f1 of perception pred     \n",
    "\n",
    "        train_tmp = model.train_on_batch(batch_mels, batch_labels)\n",
    "#         print(train_tmp)\n",
    "        loss_ = train_tmp[0]/2\n",
    "        f1_ = np.mean((train_tmp[3], train_tmp[4]))\n",
    "        loss.append(loss_)\n",
    "#         acc.append(train_tmp[1])\n",
    "        f1.append(f1_)\n",
    "\n",
    "    loss = np.mean(np.array(loss))\n",
    "#     acc = np.mean(np.array(acc))\n",
    "    f1 = np.mean(np.array(f1))\n",
    "    return loss, f1\n",
    "\n",
    "def test(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    for i, file in enumerate(data):\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "        for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "            mel_files.append(clip_file)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "        logit = model.predict(mel_files)\n",
    "#         print(len(logit))\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "#         print(logit)\n",
    "        pred = np.argmax(logit)\n",
    "#         print('Pred={}'.format(pred))\n",
    "        \n",
    "        label = np.argmax(get_labels(input_directory, file, class2index))\n",
    "#         print('Label={}'.format(label))\n",
    "        #f1 = f1_score(label, logit)\n",
    "        #print(pred, label)\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        #total_f1 += f1\n",
    "    final_acc = total_acc / i\n",
    "    #final_f1 = total_f1 / i\n",
    "    return final_acc#, final_f1\n",
    "\n",
    "batch_size = 32#5#2#1#10#32\n",
    "minimum_len = 2880\n",
    "epochs = 100\n",
    "loss_function = 'categorical_crossentropy'\n",
    "activation_function = 'softmax'\n",
    "rootdir = '../'\n",
    "date = datetime.today().strftime(\"%Y%m%d\")\n",
    "input_directory = os.path.join(rootdir, 'Training_WFDB')\n",
    "mel_name = 'Raw_data_20200424' \n",
    "mel_directory = os.path.join(rootdir, mel_name)\n",
    "results_directory = os.path.join(rootdir, 'results_'+date+'_0')\n",
    "if not os.path.isdir(input_directory):\n",
    "    os.mkdir(input_directory)\n",
    "if not os.path.isdir(mel_directory):\n",
    "    os.mkdir(mel_directory)\n",
    "if not os.path.isdir(results_directory):\n",
    "    os.mkdir(results_directory)\n",
    "        \n",
    "input_files = []\n",
    "for f in os.listdir(input_directory):\n",
    "    if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('mat'):\n",
    "        input_files.append(f)\n",
    "input_file_names = sorted(input_files)\n",
    "\n",
    "unique_classes = get_unique_classes(input_directory, input_files)\n",
    "class2index = {}\n",
    "for a, b in enumerate(unique_classes):\n",
    "    class2index[b] = a\n",
    "    \n",
    "classes_orig= [x.replace('.mat', '.hea') for x in input_file_names] # total subjects\n",
    "classes_multi, _, _ = searching_overlap(input_directory,class2index, input_file_names)\n",
    "classes_single = [x for x in classes_orig if x not in classes_multi]\n",
    "classes_single = [x.replace('.hea', '.mat') for x in classes_single]\n",
    "\n",
    "# double-checking if classes_single have single-label\n",
    "a, b, c  = searching_overlap(input_directory,class2index,classes_single)\n",
    "\n",
    "# we can safely use classes_single as input_file_names\n",
    "input_file_names = classes_single\n",
    "random.shuffle(input_file_names)\n",
    "np.shape(input_file_names)\n",
    "\n",
    "x_mean_all = []\n",
    "x_std_all = []\n",
    "for file in input_file_names:\n",
    "    x = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "    x_mean = [np.mean(x[:,0]), np.mean(x[:,1]), np.mean(x[:,2]), np.mean(x[:,3]), np.mean(x[:,4]), np.mean(x[:,5]),\n",
    "             np.mean(x[:,6]), np.mean(x[:,7]), np.mean(x[:,8]), np.mean(x[:,9]), np.mean(x[:,10]), np.mean(x[:,11])]\n",
    "    \n",
    "    x_std = [np.std(x[:,0]), np.std(x[:,1]), np.std(x[:,2]), np.std(x[:,3]), np.std(x[:,4]), np.std(x[:,5]),\n",
    "             np.std(x[:,6]), np.std(x[:,7]), np.std(x[:,8]), np.std(x[:,9]), np.std(x[:,10]), np.std(x[:,11])]\n",
    "    #print(x_mean)\n",
    "    x_mean_all.append(x_mean)\n",
    "    x_std_all.append(x_mean)\n",
    "x_mean_final = np.mean(x_mean_all, axis=0)\n",
    "x_std_final = np.mean(x_std_all, axis=0)\n",
    "print(x_mean_final)\n",
    "\n",
    "data, data_test = train_test_split(input_file_names, test_size = 0.2, train_size = 0.8, shuffle=True)\n",
    "data_train, data_val = train_test_split(data, test_size = 0.25, train_size = 0.75, shuffle=True)\n",
    "print(np.shape(data_train), np.shape(data_val), np.shape(data_test))\n",
    "\n",
    "\n",
    "main_input = Input(shape=(minimum_len,12), dtype='float32', name='main_input')\n",
    "\n",
    "branch_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, 12)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, None, 256)    15360       input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, 256)    1024        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 256)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 64)     16384       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 64)     256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 64)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 64)     12288       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, 64)     256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 64)     0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 256)    16384       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, 256)    1024        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, None, 256)    0           batch_normalization_3[0][0]      \n",
      "                                                                 activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, 256)    0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 64)     16384       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, 64)     256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, 64)     0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 64)     12288       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, 64)     256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, 64)     0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 256)    16384       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, 256)    1024        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, 256)    0           batch_normalization_6[0][0]      \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, None, 256)    0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 64)     16384       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, 64)     256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, 64)     0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, None, 64)     12288       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, 64)     256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, 64)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, None, 256)    16384       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, 256)    1024        conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, None, 256)    0           batch_normalization_9[0][0]      \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, 256)    0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, None, 128)    32768       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, 128)    512         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, 128)    0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, None, 128)    49152       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, 128)    512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, None, 128)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, None, 512)    65536       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, None, 512)    131072      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, 512)    2048        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, 512)    2048        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, None, 512)    0           batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, None, 512)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, None, 128)    65536       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, 128)    512         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, None, 128)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, None, 128)    49152       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, 128)    512         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, None, 128)    0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, None, 512)    65536       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, None, 512)    2048        conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, None, 512)    0           batch_normalization_16[0][0]     \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, 512)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, None, 128)    65536       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, None, 128)    512         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, 128)    0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, None, 128)    49152       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, None, 128)    512         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, None, 128)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, None, 512)    65536       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, None, 512)    2048        conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, None, 512)    0           batch_normalization_19[0][0]     \n",
      "                                                                 activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, None, 512)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, None, 256)    131072      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, None, 256)    1024        conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, None, 256)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, None, 256)    196608      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, None, 256)    1024        conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, None, 256)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, None, 1024)   262144      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, None, 1024)   524288      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, None, 1024)   4096        conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, None, 1024)   4096        conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, None, 1024)   0           batch_normalization_23[0][0]     \n",
      "                                                                 batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, None, 1024)   0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, None, 256)    262144      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, None, 256)    1024        conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, None, 256)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, None, 256)    196608      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, None, 256)    1024        conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, None, 256)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, None, 1024)   262144      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, None, 1024)   4096        conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, None, 1024)   0           batch_normalization_26[0][0]     \n",
      "                                                                 activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, None, 1024)   0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, None, 256)    262144      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, None, 256)    1024        conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, None, 256)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, None, 256)    196608      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, None, 256)    1024        conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, None, 256)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, None, 1024)   262144      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, None, 1024)   4096        conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, None, 1024)   0           batch_normalization_29[0][0]     \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, None, 1024)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_bn_1 (BatchNor (None, None, 1024)   4096        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_conv_1 (Conv1D (None, None, 9)      9216        attention_branch_bn_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_conv_1 (Co (None, None, 1)      9           attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_bn_1 (Batc (None, None, 1)      4           attention_branch_att_conv_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_sigmoid_1  (None, None, 1)      0           attention_branch_att_bn_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, None, 512)    0           activation_18[0][0]              \n",
      "                                                                 attention_branch_att_sigmoid_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, None, 256)    131072      lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, None, 256)    1024        conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, None, 256)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, None, 256)    196608      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, None, 256)    1024        conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, None, 256)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, None, 1024)   262144      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, None, 1024)   524288      lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, None, 1024)   4096        conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, None, 1024)   4096        conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, None, 1024)   0           batch_normalization_33[0][0]     \n",
      "                                                                 batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, None, 1024)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, None, 256)    262144      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, None, 256)    1024        conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, None, 256)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, None, 256)    196608      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, None, 256)    1024        conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, None, 256)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, None, 1024)   262144      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, None, 1024)   4096        conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, None, 1024)   0           batch_normalization_36[0][0]     \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, None, 1024)   0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, None, 256)    262144      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, None, 256)    1024        conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, None, 256)    0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, None, 256)    196608      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, None, 256)    1024        conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, None, 256)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, None, 1024)   262144      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, None, 1024)   4096        conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, None, 1024)   0           batch_normalization_39[0][0]     \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, None, 1024)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_avgpool_1 (Gl (None, 1024)         0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_pred_conv_1 (C (None, None, 9)      81          attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_1 (Dens (None, 256)          262400      perception_branch_avgpool_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_gap_1 (GlobalA (None, 9)            0           attention_branch_pred_conv_1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_2 (Dens (None, 9)            2313        perception_branch_dense_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_output (Softma (None, 9)            0           attention_branch_gap_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_output (Softm (None, 9)            0           perception_branch_dense_2[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 6,241,383\n",
      "Trainable params: 6,208,357\n",
      "Non-trainable params: 33,026\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from ABNmodules import *\n",
    "\n",
    "\n",
    "model = get_model((None, 12), 9, n=3)\n",
    "model.summary()\n",
    "model.compile(loss=[loss_function, loss_function],\n",
    "              optimizer=optimizers.Adam(lr=1e-5),\n",
    "              \n",
    "              \n",
    "              \n",
    "              metrics=[score_f1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 train_loss: 1.958 train_f1: 0.067 \t\n",
      "\n",
      "Validation 1 valid_acc: 0.157 best_acc: 0.157 \t\n",
      "\n",
      "Epoch 2 train_loss: 1.719 train_f1: 0.195 \t\n",
      "\n",
      "Validation 2 valid_acc: 0.272 best_acc: 0.272 \t\n",
      "\n",
      "Epoch 3 train_loss: 1.603 train_f1: 0.281 \t\n",
      "\n",
      "Validation 3 valid_acc: 0.403 best_acc: 0.403 \t\n",
      "\n",
      "Epoch 4 train_loss: 1.513 train_f1: 0.348 \t\n",
      "\n",
      "Validation 4 valid_acc: 0.488 best_acc: 0.488 \t\n",
      "\n",
      "Epoch 5 train_loss: 1.469 train_f1: 0.376 \t\n",
      "\n",
      "Validation 5 valid_acc: 0.525 best_acc: 0.525 \t\n",
      "\n",
      "Epoch 6 train_loss: 1.440 train_f1: 0.400 \t\n",
      "\n",
      "Validation 6 valid_acc: 0.530 best_acc: 0.530 \t\n",
      "\n",
      "Epoch 7 train_loss: 1.399 train_f1: 0.415 \t\n",
      "\n",
      "Validation 7 valid_acc: 0.536 best_acc: 0.536 \t\n",
      "\n",
      "Epoch 8 train_loss: 1.378 train_f1: 0.436 \t\n",
      "\n",
      "Validation 8 valid_acc: 0.534 best_acc: 0.536 \t\n",
      "\n",
      "Epoch 9 train_loss: 1.348 train_f1: 0.443 \t\n",
      "\n",
      "Validation 9 valid_acc: 0.547 best_acc: 0.547 \t\n",
      "\n",
      "Epoch 10 train_loss: 1.336 train_f1: 0.460 \t\n",
      "\n",
      "Validation 10 valid_acc: 0.542 best_acc: 0.547 \t\n",
      "\n",
      "Epoch 11 train_loss: 1.320 train_f1: 0.461 \t\n",
      "\n",
      "Validation 11 valid_acc: 0.543 best_acc: 0.547 \t\n",
      "\n",
      "Epoch 12 train_loss: 1.302 train_f1: 0.472 \t\n",
      "\n",
      "Validation 12 valid_acc: 0.544 best_acc: 0.547 \t\n",
      "\n",
      "Epoch 13 train_loss: 1.283 train_f1: 0.484 \t\n",
      "\n",
      "Validation 13 valid_acc: 0.560 best_acc: 0.560 \t\n",
      "\n",
      "Epoch 14 train_loss: 1.266 train_f1: 0.494 \t\n",
      "\n",
      "Validation 14 valid_acc: 0.560 best_acc: 0.560 \t\n",
      "\n",
      "Epoch 15 train_loss: 1.263 train_f1: 0.503 \t\n",
      "\n",
      "Validation 15 valid_acc: 0.548 best_acc: 0.560 \t\n",
      "\n",
      "Epoch 16 train_loss: 1.263 train_f1: 0.500 \t\n",
      "\n",
      "Validation 16 valid_acc: 0.554 best_acc: 0.560 \t\n",
      "\n",
      "Epoch 17 train_loss: 1.223 train_f1: 0.514 \t\n",
      "\n",
      "Validation 17 valid_acc: 0.561 best_acc: 0.561 \t\n",
      "\n",
      "Epoch 18 train_loss: 1.208 train_f1: 0.520 \t\n",
      "\n",
      "Validation 18 valid_acc: 0.559 best_acc: 0.561 \t\n",
      "\n",
      "Epoch 19 train_loss: 1.193 train_f1: 0.528 \t\n",
      "\n",
      "Validation 19 valid_acc: 0.590 best_acc: 0.590 \t\n",
      "\n",
      "Epoch 20 train_loss: 1.193 train_f1: 0.529 \t\n",
      "\n",
      "Validation 20 valid_acc: 0.560 best_acc: 0.590 \t\n",
      "\n",
      "Epoch 21 train_loss: 1.180 train_f1: 0.542 \t\n",
      "\n",
      "Validation 21 valid_acc: 0.572 best_acc: 0.590 \t\n",
      "\n",
      "Epoch 22 train_loss: 1.147 train_f1: 0.552 \t\n",
      "\n",
      "Validation 22 valid_acc: 0.599 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 23 train_loss: 1.150 train_f1: 0.550 \t\n",
      "\n",
      "Validation 23 valid_acc: 0.595 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 24 train_loss: 1.132 train_f1: 0.561 \t\n",
      "\n",
      "Validation 24 valid_acc: 0.597 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 25 train_loss: 1.119 train_f1: 0.562 \t\n",
      "\n",
      "Validation 25 valid_acc: 0.559 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 26 train_loss: 1.101 train_f1: 0.570 \t\n",
      "\n",
      "Validation 26 valid_acc: 0.575 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 27 train_loss: 1.100 train_f1: 0.571 \t\n",
      "\n",
      "Validation 27 valid_acc: 0.578 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 28 train_loss: 1.084 train_f1: 0.576 \t\n",
      "\n",
      "Validation 28 valid_acc: 0.564 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 29 train_loss: 1.073 train_f1: 0.587 \t\n",
      "\n",
      "Validation 29 valid_acc: 0.573 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 30 train_loss: 1.061 train_f1: 0.596 \t\n",
      "\n",
      "Validation 30 valid_acc: 0.586 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 31 train_loss: 1.074 train_f1: 0.587 \t\n",
      "\n",
      "Validation 31 valid_acc: 0.571 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 32 train_loss: 1.054 train_f1: 0.593 \t\n",
      "\n",
      "Validation 32 valid_acc: 0.564 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 33 train_loss: 1.035 train_f1: 0.604 \t\n",
      "\n",
      "Validation 33 valid_acc: 0.564 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 34 train_loss: 1.027 train_f1: 0.609 \t\n",
      "\n",
      "Validation 34 valid_acc: 0.562 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 35 train_loss: 1.016 train_f1: 0.604 \t\n",
      "\n",
      "Validation 35 valid_acc: 0.580 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 36 train_loss: 1.008 train_f1: 0.614 \t\n",
      "\n",
      "Validation 36 valid_acc: 0.569 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 37 train_loss: 0.996 train_f1: 0.616 \t\n",
      "\n",
      "Validation 37 valid_acc: 0.587 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 38 train_loss: 1.003 train_f1: 0.622 \t\n",
      "\n",
      "Validation 38 valid_acc: 0.579 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 39 train_loss: 0.986 train_f1: 0.623 \t\n",
      "\n",
      "Validation 39 valid_acc: 0.577 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 40 train_loss: 0.974 train_f1: 0.629 \t\n",
      "\n",
      "Validation 40 valid_acc: 0.586 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 41 train_loss: 0.972 train_f1: 0.630 \t\n",
      "\n",
      "Validation 41 valid_acc: 0.573 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 42 train_loss: 0.962 train_f1: 0.636 \t\n",
      "\n",
      "Validation 42 valid_acc: 0.611 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 43 train_loss: 0.943 train_f1: 0.636 \t\n",
      "\n",
      "Validation 43 valid_acc: 0.570 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 44 train_loss: 0.931 train_f1: 0.646 \t\n",
      "\n",
      "Validation 44 valid_acc: 0.595 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 45 train_loss: 0.924 train_f1: 0.647 \t\n",
      "\n",
      "Validation 45 valid_acc: 0.590 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 46 train_loss: 0.932 train_f1: 0.642 \t\n",
      "\n",
      "Validation 46 valid_acc: 0.586 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 47 train_loss: 0.908 train_f1: 0.656 \t\n",
      "\n",
      "Validation 47 valid_acc: 0.599 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 48 train_loss: 0.894 train_f1: 0.659 \t\n",
      "\n",
      "Validation 48 valid_acc: 0.592 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 49 train_loss: 0.881 train_f1: 0.661 \t\n",
      "\n",
      "Validation 49 valid_acc: 0.604 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 50 train_loss: 0.890 train_f1: 0.666 \t\n",
      "\n",
      "Validation 50 valid_acc: 0.593 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 51 train_loss: 0.889 train_f1: 0.666 \t\n",
      "\n",
      "Validation 51 valid_acc: 0.597 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 52 train_loss: 0.877 train_f1: 0.670 \t\n",
      "\n",
      "Validation 52 valid_acc: 0.572 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 53 train_loss: 0.853 train_f1: 0.674 \t\n",
      "\n",
      "Validation 53 valid_acc: 0.563 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 54 train_loss: 0.845 train_f1: 0.680 \t\n",
      "\n",
      "Validation 54 valid_acc: 0.597 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 55 train_loss: 0.829 train_f1: 0.688 \t\n",
      "\n",
      "Validation 55 valid_acc: 0.587 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 56 train_loss: 0.849 train_f1: 0.682 \t\n",
      "\n",
      "Validation 56 valid_acc: 0.595 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 57 train_loss: 0.826 train_f1: 0.690 \t\n",
      "\n",
      "Validation 57 valid_acc: 0.601 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 58 train_loss: 0.821 train_f1: 0.692 \t\n",
      "\n",
      "Validation 58 valid_acc: 0.603 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 59 train_loss: 0.817 train_f1: 0.700 \t\n",
      "\n",
      "Validation 59 valid_acc: 0.590 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 60 train_loss: 0.802 train_f1: 0.699 \t\n",
      "\n",
      "Validation 60 valid_acc: 0.554 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 61 train_loss: 0.803 train_f1: 0.697 \t\n",
      "\n",
      "Validation 61 valid_acc: 0.573 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 62 train_loss: 0.793 train_f1: 0.705 \t\n",
      "\n",
      "Validation 62 valid_acc: 0.594 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 63 train_loss: 0.774 train_f1: 0.707 \t\n",
      "\n",
      "Validation 63 valid_acc: 0.581 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 64 train_loss: 0.752 train_f1: 0.721 \t\n",
      "\n",
      "Validation 64 valid_acc: 0.600 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 65 train_loss: 0.780 train_f1: 0.708 \t\n",
      "\n",
      "Validation 65 valid_acc: 0.563 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 66 train_loss: 0.777 train_f1: 0.711 \t\n",
      "\n",
      "Validation 66 valid_acc: 0.582 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 67 train_loss: 0.772 train_f1: 0.713 \t\n",
      "\n",
      "Validation 67 valid_acc: 0.568 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 68 train_loss: 0.756 train_f1: 0.720 \t\n",
      "\n",
      "Validation 68 valid_acc: 0.600 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 69 train_loss: 0.742 train_f1: 0.721 \t\n",
      "\n",
      "Validation 69 valid_acc: 0.613 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 70 train_loss: 0.747 train_f1: 0.720 \t\n",
      "\n",
      "Validation 70 valid_acc: 0.581 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 71 train_loss: 0.707 train_f1: 0.733 \t\n",
      "\n",
      "Validation 71 valid_acc: 0.594 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 72 train_loss: 0.725 train_f1: 0.732 \t\n",
      "\n",
      "Validation 72 valid_acc: 0.600 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 73 train_loss: 0.718 train_f1: 0.733 \t\n",
      "\n",
      "Validation 73 valid_acc: 0.575 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 74 train_loss: 0.695 train_f1: 0.741 \t\n",
      "\n",
      "Validation 74 valid_acc: 0.599 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 75 train_loss: 0.710 train_f1: 0.739 \t\n",
      "\n",
      "Validation 75 valid_acc: 0.572 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 76 train_loss: 0.710 train_f1: 0.734 \t\n",
      "\n",
      "Validation 76 valid_acc: 0.577 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 77 train_loss: 0.681 train_f1: 0.745 \t\n",
      "\n",
      "Validation 77 valid_acc: 0.570 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 78 train_loss: 0.681 train_f1: 0.745 \t\n",
      "\n",
      "Validation 78 valid_acc: 0.590 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 79 train_loss: 0.662 train_f1: 0.754 \t\n",
      "\n",
      "Validation 79 valid_acc: 0.590 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 80 train_loss: 0.683 train_f1: 0.747 \t\n",
      "\n",
      "Validation 80 valid_acc: 0.591 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 81 train_loss: 0.668 train_f1: 0.754 \t\n",
      "\n",
      "Validation 81 valid_acc: 0.588 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 82 train_loss: 0.673 train_f1: 0.750 \t\n",
      "\n",
      "Validation 82 valid_acc: 0.577 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 83 train_loss: 0.645 train_f1: 0.756 \t\n",
      "\n",
      "Validation 83 valid_acc: 0.580 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 84 train_loss: 0.641 train_f1: 0.765 \t\n",
      "\n",
      "Validation 84 valid_acc: 0.582 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 85 train_loss: 0.640 train_f1: 0.761 \t\n",
      "\n",
      "Validation 85 valid_acc: 0.573 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 86 train_loss: 0.627 train_f1: 0.767 \t\n",
      "\n",
      "Validation 86 valid_acc: 0.571 best_acc: 0.613 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 87 train_loss: 0.642 train_f1: 0.763 \t\n",
      "\n",
      "Validation 87 valid_acc: 0.592 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 88 train_loss: 0.653 train_f1: 0.763 \t\n",
      "\n",
      "Validation 88 valid_acc: 0.584 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 89 train_loss: 0.611 train_f1: 0.781 \t\n",
      "\n",
      "Validation 89 valid_acc: 0.574 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 90 train_loss: 0.622 train_f1: 0.771 \t\n",
      "\n",
      "Validation 90 valid_acc: 0.597 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 91 train_loss: 0.604 train_f1: 0.779 \t\n",
      "\n",
      "Validation 91 valid_acc: 0.590 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 92 train_loss: 0.605 train_f1: 0.777 \t\n",
      "\n",
      "Validation 92 valid_acc: 0.598 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 93 train_loss: 0.595 train_f1: 0.783 \t\n",
      "\n",
      "Validation 93 valid_acc: 0.601 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 94 train_loss: 0.595 train_f1: 0.783 \t\n",
      "\n",
      "Validation 94 valid_acc: 0.582 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 95 train_loss: 0.591 train_f1: 0.783 \t\n",
      "\n",
      "Validation 95 valid_acc: 0.556 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 96 train_loss: 0.567 train_f1: 0.791 \t\n",
      "\n",
      "Validation 96 valid_acc: 0.589 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 97 train_loss: 0.580 train_f1: 0.790 \t\n",
      "\n",
      "Validation 97 valid_acc: 0.592 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 98 train_loss: 0.551 train_f1: 0.798 \t\n",
      "\n",
      "Validation 98 valid_acc: 0.592 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 99 train_loss: 0.584 train_f1: 0.785 \t\n",
      "\n",
      "Validation 99 valid_acc: 0.596 best_acc: 0.613 \t\n",
      "\n",
      "Epoch 100 train_loss: 0.546 train_f1: 0.802 \t\n",
      "\n",
      "Validation 100 valid_acc: 0.590 best_acc: 0.613 \t\n",
      "10496.929039239883\n"
     ]
    }
   ],
   "source": [
    "# DISP DATETIME FOR CHECKING TIME\n",
    "start = time.time()\n",
    "val_acc_sum=[]\n",
    "train_loss_sum=[]\n",
    "train_acc_sum=[]\n",
    "val_loss_sum=[]\n",
    "val_acc_min = 0\n",
    "disp(save_name)\n",
    "for num_epoch in range(epochs):\n",
    "    random.shuffle(data_train)\n",
    "    train_loss, train_f1 = train(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "    print('\\nEpoch',num_epoch+1,'train_loss:',f'{train_loss:.3f}','train_f1:',f'{train_f1:.3f}',\"\\t\")\n",
    "    model_output = \"ECG_ABN_E%02dL%.2f\" % (num_epoch, train_loss)\n",
    "    save_name = os.path.join(results_directory, model_output)\n",
    "\n",
    "    val_acc = test(data_val, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)\n",
    "\n",
    "    if val_acc > val_acc_min:\n",
    "        val_acc_min = val_acc\n",
    "        model.save(save_name)\n",
    "    print('\\nValidation', num_epoch+1, 'valid_acc:',f'{val_acc:.3f}', 'best_acc:',f'{val_acc_min:.3f}', \"\\t\")\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data imbalance problem -> slideshare 1D CNN data augmentation 참고\n",
    "# n=18일때: E18L1.53 (총 20 epoch) ; 어쨌든 loss로만 판단했을 때 n 18 -> 3으로 줄이니까 성능 더 좋아짐"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "395.881px",
    "left": "1487.43px",
    "right": "20px",
    "top": "128.94px",
    "width": "200.142px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
