{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numba/errors.py:137: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import keras\n",
    "# import datetime as dt\n",
    "from datetime import datetime\n",
    "import time\n",
    "# import datetime.datetime\n",
    "\n",
    "\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "# from keras import optimizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from keras.applications.densenet import DenseNet121, DenseNet169\n",
    "#from keras.applications.nasnet import NASNetLarge\n",
    "# from keras_efficientnets import EfficientNetB7\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from keras import backend as K\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "random.seed(100)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.24290386e-03 -4.58280585e-05  4.31697309e-03 -3.00174693e-03\n",
      " -2.36609229e-04  1.28997408e-03  2.17347589e-04 -7.99152384e-04\n",
      " -3.42993744e-03 -1.69711686e-03  1.27138164e-03  1.94670545e-03]\n",
      "(3840,) (1280,) (1281,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def score_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "def get_unique_classes(input_directory,files):\n",
    "\n",
    "    unique_classes=set()\n",
    "    for f in files:\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    for c in tmp:\n",
    "                        unique_classes.add(c.strip())\n",
    "    return sorted(unique_classes)\n",
    "\n",
    "def one_hot_encoding(one_hot_vector,y, class2index):\n",
    "    ind=class2index[y]\n",
    "    one_hot_vector[ind]=1\n",
    "    return one_hot_vector\n",
    "\n",
    "# Search for multi-label subjects\n",
    "def searching_overlap(input_directory,class2index, input_file_names):\n",
    "    multiclasses=[]\n",
    "    multisubjects=[]\n",
    "    number = []\n",
    "    for file in input_file_names:\n",
    "        f=file\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    if len(tmp)>1:\n",
    "                        one_hot_vector = [0]*(len(class2index))\n",
    "                        for c in tmp:\n",
    "                            one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                        multiclasses.append(one_hot_vector)\n",
    "                        multisubjects.append(g)\n",
    "                        number.append(len(tmp))\n",
    "    return multisubjects, multiclasses, number\n",
    "\n",
    "def block_feature(sequence_en, minimum_len): \n",
    "    new_en = []\n",
    "    if len(sequence_en) > minimum_len:  # 길이가 minimum보다 긴 경우\n",
    "        start = random.randint(0,len(sequence_en)-minimum_len)\n",
    "        #print(start)\n",
    "        new_en = sequence_en[start:start+minimum_len]\n",
    "    elif len(sequence_en) == minimum_len: # 길이가 minimum\n",
    "        new_en = sequence_en\n",
    "    else: \n",
    "        assert len(sequence_en) <= minimum_len\n",
    "    return new_en\n",
    "\n",
    "def exploratory_look(input_directory,file, class2index):\n",
    "    classes = []\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                print(tmp, len(tmp))\n",
    "    return tmp     \n",
    "\n",
    "# Get classes of sorted file names\n",
    "def get_labels(input_directory,file, class2index):\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                one_hot_vector = [0]*(len(class2index))\n",
    "                for c in tmp:\n",
    "                    one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                \n",
    "    return one_hot_vector\n",
    "\n",
    "def randextract_mels(curr_step, batch_size, data, mel_directory, class2index, minimum_len, x_mean_final, x_std_final):\n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    start = batch_size*curr_step\n",
    "    end = batch_size*(curr_step+1)\n",
    "    curr_file_indices = data[start:end]\n",
    "    for file in curr_file_indices:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        clip_file = block_feature(tmp_file, minimum_len)\n",
    "        #print(clip_file.shape)\n",
    "        #clip_file = tmp_file[:minimum_len]\n",
    "        clip_file -= x_mean_final\n",
    "        clip_file /= x_std_final\n",
    "        mel_files.append(clip_file)\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        classes.append(label)\n",
    "    \n",
    "    # YJS added for ABN -> should calculate 2 losses\n",
    "#     classes_abn = [classes,classes]\n",
    "    \n",
    "    concat = list(zip(mel_files, classes))\n",
    "#     concat = list(zip(mel_files, classes_abn))\n",
    "    random.shuffle(concat)\n",
    "    mel_files, classes = zip(*concat)\n",
    "    return mel_files, classes\n",
    "\n",
    "def train(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final): \n",
    "    loss=[]\n",
    "#     acc = []\n",
    "    f1 = []\n",
    "\n",
    "    total_steps = int(np.ceil(len(data_train)/batch_size))\n",
    "    for curr_step in range(total_steps):\n",
    "        batch_mels, batch_labels = randextract_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "        batch_mels = np.asarray(batch_mels)\n",
    "        batch_labels2 = np.asarray(np.squeeze(batch_labels))\n",
    "        batch_labels = [batch_labels2, batch_labels2]\n",
    "#         print(batch_labels.shape)\n",
    "#         print(batch_labels)\n",
    "\n",
    "# return of train\n",
    "# 0 = total loss (attention branch + perception branch)\n",
    "# 1 = loss of attention pred\n",
    "# 2 = loss of perception pred\n",
    "# 3 = f1 of attention pred\n",
    "# 4 = f1 of perception pred     \n",
    "\n",
    "        train_tmp = model.train_on_batch(batch_mels, batch_labels)\n",
    "#         print(train_tmp)\n",
    "        loss_ = train_tmp[0]/2\n",
    "        f1_ = np.mean((train_tmp[3], train_tmp[4]))\n",
    "        loss.append(loss_)\n",
    "#         acc.append(train_tmp[1])\n",
    "        f1.append(f1_)\n",
    "\n",
    "    loss = np.mean(np.array(loss))\n",
    "#     acc = np.mean(np.array(acc))\n",
    "    f1 = np.mean(np.array(f1))\n",
    "    return loss, f1\n",
    "\n",
    "def test(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    for i, file in enumerate(data):\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "        for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "            mel_files.append(clip_file)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "        logit = model.predict(mel_files)\n",
    "#         print(len(logit))\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "#         print(logit)\n",
    "        pred = np.argmax(logit)\n",
    "#         print('Pred={}'.format(pred))\n",
    "        \n",
    "        label = np.argmax(get_labels(input_directory, file, class2index))\n",
    "#         print('Label={}'.format(label))\n",
    "        #f1 = f1_score(label, logit)\n",
    "        #print(pred, label)\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        #total_f1 += f1\n",
    "    final_acc = total_acc / i\n",
    "    #final_f1 = total_f1 / i\n",
    "    return final_acc#, final_f1\n",
    "\n",
    "batch_size = 16#20#32#5#2#1#10#32\n",
    "\n",
    "minimum_len = 2880\n",
    "epochs = 1000\n",
    "loss_function = 'categorical_crossentropy'\n",
    "activation_function = 'softmax'\n",
    "rootdir = '../'\n",
    "date = datetime.today().strftime(\"%Y%m%d\")\n",
    "input_directory = os.path.join(rootdir, 'Training_WFDB')\n",
    "mel_name = 'Raw_data_20200424' \n",
    "mel_directory = os.path.join(rootdir, mel_name)\n",
    "results_directory = os.path.join(rootdir, 'results_'+date+'_0_after_std_correction')\n",
    "if not os.path.isdir(input_directory):\n",
    "    os.mkdir(input_directory)\n",
    "if not os.path.isdir(mel_directory):\n",
    "    os.mkdir(mel_directory)\n",
    "if not os.path.isdir(results_directory):\n",
    "    os.mkdir(results_directory)\n",
    "        \n",
    "input_files = []\n",
    "for f in os.listdir(input_directory):\n",
    "    if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('mat'):\n",
    "        input_files.append(f)\n",
    "input_file_names = sorted(input_files)\n",
    "\n",
    "unique_classes = get_unique_classes(input_directory, input_files)\n",
    "class2index = {}\n",
    "for a, b in enumerate(unique_classes):\n",
    "    class2index[b] = a\n",
    "    \n",
    "classes_orig= [x.replace('.mat', '.hea') for x in input_file_names] # total subjects\n",
    "classes_multi, _, _ = searching_overlap(input_directory,class2index, input_file_names)\n",
    "classes_single = [x for x in classes_orig if x not in classes_multi]\n",
    "classes_single = [x.replace('.hea', '.mat') for x in classes_single]\n",
    "\n",
    "# double-checking if classes_single have single-label\n",
    "a, b, c  = searching_overlap(input_directory,class2index,classes_single)\n",
    "\n",
    "# we can safely use classes_single as input_file_names\n",
    "input_file_names = classes_single\n",
    "random.shuffle(input_file_names)\n",
    "np.shape(input_file_names)\n",
    "\n",
    "x_mean_all = []\n",
    "x_std_all = []\n",
    "for file in input_file_names:\n",
    "    x = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "    x_mean = [np.mean(x[:,0]), np.mean(x[:,1]), np.mean(x[:,2]), np.mean(x[:,3]), np.mean(x[:,4]), np.mean(x[:,5]),\n",
    "             np.mean(x[:,6]), np.mean(x[:,7]), np.mean(x[:,8]), np.mean(x[:,9]), np.mean(x[:,10]), np.mean(x[:,11])]\n",
    "    \n",
    "    x_std = [np.std(x[:,0]), np.std(x[:,1]), np.std(x[:,2]), np.std(x[:,3]), np.std(x[:,4]), np.std(x[:,5]),\n",
    "             np.std(x[:,6]), np.std(x[:,7]), np.std(x[:,8]), np.std(x[:,9]), np.std(x[:,10]), np.std(x[:,11])]\n",
    "    #print(x_mean)\n",
    "    x_mean_all.append(x_mean)\n",
    "    x_std_all.append(x_std) # yjs corrected on 2020-05-25\n",
    "x_mean_final = np.mean(x_mean_all, axis=0)\n",
    "x_std_final = np.mean(x_std_all, axis=0)\n",
    "print(x_mean_final)\n",
    "\n",
    "data, data_test = train_test_split(input_file_names, test_size = 0.2, train_size = 0.8, shuffle=True)\n",
    "data_train, data_val = train_test_split(data, test_size = 0.25, train_size = 0.75, shuffle=True)\n",
    "print(np.shape(data_train), np.shape(data_val), np.shape(data_test))\n",
    "\n",
    "\n",
    "main_input = Input(shape=(minimum_len,12), dtype='float32', name='main_input')\n",
    "\n",
    "branch_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, 12)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_88 (Conv1D)              (None, None, 256)    9216        input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, None, 256)    1024        conv1d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, None, 256)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_89 (Conv1D)              (None, None, 64)     16384       activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, None, 64)     256         conv1d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, None, 64)     0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_90 (Conv1D)              (None, None, 64)     12288       activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, None, 64)     256         conv1d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, None, 64)     0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_91 (Conv1D)              (None, None, 256)    16384       activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, None, 256)    1024        conv1d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, None, 256)    0           batch_normalization_91[0][0]     \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, None, 256)    0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_92 (Conv1D)              (None, None, 64)     16384       activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, None, 64)     256         conv1d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, None, 64)     0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_93 (Conv1D)              (None, None, 64)     12288       activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, None, 64)     256         conv1d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, None, 64)     0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_94 (Conv1D)              (None, None, 256)    16384       activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, None, 256)    1024        conv1d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, None, 256)    0           batch_normalization_94[0][0]     \n",
      "                                                                 activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, None, 256)    0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_95 (Conv1D)              (None, None, 64)     16384       activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, None, 64)     256         conv1d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, None, 64)     0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_96 (Conv1D)              (None, None, 64)     12288       activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, None, 64)     256         conv1d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, None, 64)     0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_97 (Conv1D)              (None, None, 256)    16384       activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, None, 256)    1024        conv1d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, None, 256)    0           batch_normalization_97[0][0]     \n",
      "                                                                 activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, None, 256)    0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_98 (Conv1D)              (None, None, 64)     16384       activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, None, 64)     256         conv1d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, None, 64)     0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_99 (Conv1D)              (None, None, 64)     12288       activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, None, 64)     256         conv1d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, None, 64)     0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_100 (Conv1D)             (None, None, 256)    16384       activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, None, 256)    1024        conv1d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, None, 256)    0           batch_normalization_100[0][0]    \n",
      "                                                                 activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, None, 256)    0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_101 (Conv1D)             (None, None, 64)     16384       activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, None, 64)     256         conv1d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, None, 64)     0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_102 (Conv1D)             (None, None, 64)     12288       activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, None, 64)     256         conv1d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, None, 64)     0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_103 (Conv1D)             (None, None, 256)    16384       activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, None, 256)    1024        conv1d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, None, 256)    0           batch_normalization_103[0][0]    \n",
      "                                                                 activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, None, 256)    0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_104 (Conv1D)             (None, None, 64)     16384       activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, None, 64)     256         conv1d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, None, 64)     0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_105 (Conv1D)             (None, None, 64)     12288       activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, None, 64)     256         conv1d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, None, 64)     0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_106 (Conv1D)             (None, None, 256)    16384       activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, None, 256)    1024        conv1d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, None, 256)    0           batch_normalization_106[0][0]    \n",
      "                                                                 activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, None, 256)    0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_107 (Conv1D)             (None, None, 64)     16384       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, None, 64)     256         conv1d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, None, 64)     0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_108 (Conv1D)             (None, None, 64)     12288       activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, None, 64)     256         conv1d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, None, 64)     0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_109 (Conv1D)             (None, None, 256)    16384       activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, None, 256)    1024        conv1d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, None, 256)    0           batch_normalization_109[0][0]    \n",
      "                                                                 activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, None, 256)    0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_111 (Conv1D)             (None, None, 128)    32768       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, None, 128)    512         conv1d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, None, 128)    0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_112 (Conv1D)             (None, None, 128)    49152       activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, None, 128)    512         conv1d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, None, 128)    0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_113 (Conv1D)             (None, None, 512)    65536       activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_110 (Conv1D)             (None, None, 512)    131072      activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, None, 512)    2048        conv1d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, None, 512)    2048        conv1d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, None, 512)    0           batch_normalization_113[0][0]    \n",
      "                                                                 batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, None, 512)    0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_114 (Conv1D)             (None, None, 128)    65536       activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, None, 128)    512         conv1d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, None, 128)    0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_115 (Conv1D)             (None, None, 128)    49152       activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, None, 128)    512         conv1d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, None, 128)    0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_116 (Conv1D)             (None, None, 512)    65536       activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, None, 512)    2048        conv1d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, None, 512)    0           batch_normalization_116[0][0]    \n",
      "                                                                 activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, None, 512)    0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_117 (Conv1D)             (None, None, 128)    65536       activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, None, 128)    512         conv1d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, None, 128)    0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_118 (Conv1D)             (None, None, 128)    49152       activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, None, 128)    512         conv1d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, None, 128)    0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_119 (Conv1D)             (None, None, 512)    65536       activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, None, 512)    2048        conv1d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, None, 512)    0           batch_normalization_119[0][0]    \n",
      "                                                                 activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, None, 512)    0           add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_120 (Conv1D)             (None, None, 128)    65536       activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, None, 128)    512         conv1d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, None, 128)    0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_121 (Conv1D)             (None, None, 128)    49152       activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, None, 128)    512         conv1d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, None, 128)    0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_122 (Conv1D)             (None, None, 512)    65536       activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, None, 512)    2048        conv1d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, None, 512)    0           batch_normalization_122[0][0]    \n",
      "                                                                 activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, None, 512)    0           add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_123 (Conv1D)             (None, None, 128)    65536       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, None, 128)    512         conv1d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, None, 128)    0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_124 (Conv1D)             (None, None, 128)    49152       activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, None, 128)    512         conv1d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, None, 128)    0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_125 (Conv1D)             (None, None, 512)    65536       activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, None, 512)    2048        conv1d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, None, 512)    0           batch_normalization_125[0][0]    \n",
      "                                                                 activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, None, 512)    0           add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_126 (Conv1D)             (None, None, 128)    65536       activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, None, 128)    512         conv1d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, None, 128)    0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_127 (Conv1D)             (None, None, 128)    49152       activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, None, 128)    512         conv1d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, None, 128)    0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_128 (Conv1D)             (None, None, 512)    65536       activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, None, 512)    2048        conv1d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, None, 512)    0           batch_normalization_128[0][0]    \n",
      "                                                                 activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, None, 512)    0           add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_129 (Conv1D)             (None, None, 128)    65536       activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, None, 128)    512         conv1d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, None, 128)    0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_130 (Conv1D)             (None, None, 128)    49152       activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, None, 128)    512         conv1d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, None, 128)    0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_131 (Conv1D)             (None, None, 512)    65536       activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, None, 512)    2048        conv1d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, None, 512)    0           batch_normalization_131[0][0]    \n",
      "                                                                 activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, None, 512)    0           add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_133 (Conv1D)             (None, None, 256)    131072      activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, None, 256)    1024        conv1d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, None, 256)    0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_134 (Conv1D)             (None, None, 256)    196608      activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, None, 256)    1024        conv1d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, None, 256)    0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_135 (Conv1D)             (None, None, 1024)   262144      activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_132 (Conv1D)             (None, None, 1024)   524288      activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, None, 1024)   4096        conv1d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, None, 1024)   4096        conv1d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, None, 1024)   0           batch_normalization_135[0][0]    \n",
      "                                                                 batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, None, 1024)   0           add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_136 (Conv1D)             (None, None, 256)    262144      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, None, 256)    1024        conv1d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, None, 256)    0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_137 (Conv1D)             (None, None, 256)    196608      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, None, 256)    1024        conv1d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, None, 256)    0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_138 (Conv1D)             (None, None, 1024)   262144      activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, None, 1024)   4096        conv1d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, None, 1024)   0           batch_normalization_138[0][0]    \n",
      "                                                                 activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, None, 1024)   0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_139 (Conv1D)             (None, None, 256)    262144      activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, None, 256)    1024        conv1d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, None, 256)    0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_140 (Conv1D)             (None, None, 256)    196608      activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, None, 256)    1024        conv1d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, None, 256)    0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_141 (Conv1D)             (None, None, 1024)   262144      activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, None, 1024)   4096        conv1d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, None, 1024)   0           batch_normalization_141[0][0]    \n",
      "                                                                 activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, None, 1024)   0           add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_142 (Conv1D)             (None, None, 256)    262144      activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, None, 256)    1024        conv1d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, None, 256)    0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_143 (Conv1D)             (None, None, 256)    196608      activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, None, 256)    1024        conv1d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, None, 256)    0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_144 (Conv1D)             (None, None, 1024)   262144      activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, None, 1024)   4096        conv1d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, None, 1024)   0           batch_normalization_144[0][0]    \n",
      "                                                                 activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, None, 1024)   0           add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_145 (Conv1D)             (None, None, 256)    262144      activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, None, 256)    1024        conv1d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, None, 256)    0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_146 (Conv1D)             (None, None, 256)    196608      activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, None, 256)    1024        conv1d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, None, 256)    0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_147 (Conv1D)             (None, None, 1024)   262144      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, None, 1024)   4096        conv1d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, None, 1024)   0           batch_normalization_147[0][0]    \n",
      "                                                                 activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, None, 1024)   0           add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_148 (Conv1D)             (None, None, 256)    262144      activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, None, 256)    1024        conv1d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, None, 256)    0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_149 (Conv1D)             (None, None, 256)    196608      activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, None, 256)    1024        conv1d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, None, 256)    0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_150 (Conv1D)             (None, None, 1024)   262144      activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, None, 1024)   4096        conv1d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, None, 1024)   0           batch_normalization_150[0][0]    \n",
      "                                                                 activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, None, 1024)   0           add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_151 (Conv1D)             (None, None, 256)    262144      activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, None, 256)    1024        conv1d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, None, 256)    0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_152 (Conv1D)             (None, None, 256)    196608      activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, None, 256)    1024        conv1d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, None, 256)    0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_153 (Conv1D)             (None, None, 1024)   262144      activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, None, 1024)   4096        conv1d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, None, 1024)   0           batch_normalization_153[0][0]    \n",
      "                                                                 activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, None, 1024)   0           add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_bn_1 (BatchNor (None, None, 1024)   4096        activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_conv_1 (Conv1D (None, None, 9)      9216        attention_branch_bn_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_conv_1 (Co (None, None, 1)      9           attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_bn_1 (Batc (None, None, 1)      4           attention_branch_att_conv_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_sigmoid_1  (None, None, 1)      0           attention_branch_att_bn_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 512)    0           activation_127[0][0]             \n",
      "                                                                 attention_branch_att_sigmoid_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_155 (Conv1D)             (None, None, 256)    131072      lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, None, 256)    1024        conv1d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, None, 256)    0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_156 (Conv1D)             (None, None, 256)    196608      activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, None, 256)    1024        conv1d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, None, 256)    0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_157 (Conv1D)             (None, None, 1024)   262144      activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_154 (Conv1D)             (None, None, 1024)   524288      lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, None, 1024)   4096        conv1d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, None, 1024)   4096        conv1d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_49 (Add)                    (None, None, 1024)   0           batch_normalization_157[0][0]    \n",
      "                                                                 batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, None, 1024)   0           add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_158 (Conv1D)             (None, None, 256)    262144      activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, None, 256)    1024        conv1d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, None, 256)    0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_159 (Conv1D)             (None, None, 256)    196608      activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, None, 256)    1024        conv1d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, None, 256)    0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_160 (Conv1D)             (None, None, 1024)   262144      activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, None, 1024)   4096        conv1d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_50 (Add)                    (None, None, 1024)   0           batch_normalization_160[0][0]    \n",
      "                                                                 activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, None, 1024)   0           add_50[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_161 (Conv1D)             (None, None, 256)    262144      activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, None, 256)    1024        conv1d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, None, 256)    0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_162 (Conv1D)             (None, None, 256)    196608      activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, None, 256)    1024        conv1d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, None, 256)    0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_163 (Conv1D)             (None, None, 1024)   262144      activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, None, 1024)   4096        conv1d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_51 (Add)                    (None, None, 1024)   0           batch_normalization_163[0][0]    \n",
      "                                                                 activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, None, 1024)   0           add_51[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_164 (Conv1D)             (None, None, 256)    262144      activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, None, 256)    1024        conv1d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, None, 256)    0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_165 (Conv1D)             (None, None, 256)    196608      activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, None, 256)    1024        conv1d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, None, 256)    0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_166 (Conv1D)             (None, None, 1024)   262144      activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, None, 1024)   4096        conv1d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_52 (Add)                    (None, None, 1024)   0           batch_normalization_166[0][0]    \n",
      "                                                                 activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, None, 1024)   0           add_52[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_167 (Conv1D)             (None, None, 256)    262144      activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, None, 256)    1024        conv1d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, None, 256)    0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_168 (Conv1D)             (None, None, 256)    196608      activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, None, 256)    1024        conv1d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, None, 256)    0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_169 (Conv1D)             (None, None, 1024)   262144      activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, None, 1024)   4096        conv1d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_53 (Add)                    (None, None, 1024)   0           batch_normalization_169[0][0]    \n",
      "                                                                 activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, None, 1024)   0           add_53[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_170 (Conv1D)             (None, None, 256)    262144      activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, None, 256)    1024        conv1d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, None, 256)    0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_171 (Conv1D)             (None, None, 256)    196608      activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, None, 256)    1024        conv1d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, None, 256)    0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_172 (Conv1D)             (None, None, 1024)   262144      activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, None, 1024)   4096        conv1d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_54 (Add)                    (None, None, 1024)   0           batch_normalization_172[0][0]    \n",
      "                                                                 activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, None, 1024)   0           add_54[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_173 (Conv1D)             (None, None, 256)    262144      activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, None, 256)    1024        conv1d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, None, 256)    0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_174 (Conv1D)             (None, None, 256)    196608      activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, None, 256)    1024        conv1d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, None, 256)    0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_175 (Conv1D)             (None, None, 1024)   262144      activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, None, 1024)   4096        conv1d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_55 (Add)                    (None, None, 1024)   0           batch_normalization_175[0][0]    \n",
      "                                                                 activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, None, 1024)   0           add_55[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_avgpool_1 (Gl (None, 1024)         0           activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_pred_conv_1 (C (None, None, 9)      81          attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_1 (Dens (None, 256)          262400      perception_branch_avgpool_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_gap_1 (GlobalA (None, 9)            0           attention_branch_pred_conv_1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_2 (Dens (None, 9)            2313        perception_branch_dense_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_output (Softma (None, 9)            0           attention_branch_gap_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_output (Softm (None, 9)            0           perception_branch_dense_2[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 12,971,111\n",
      "Trainable params: 12,904,293\n",
      "Non-trainable params: 66,818\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from ABNmodules import *\n",
    "\n",
    "\n",
    "model = get_model((None, 12), 9, n=7)\n",
    "model.summary()\n",
    "model.compile(loss=[loss_function, loss_function],\n",
    "              optimizer=optimizers.Adam(lr=1e-5),\n",
    "              \n",
    "              \n",
    "              \n",
    "              metrics=[score_f1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results_20200525_0_after_std_correction\n",
      "\n",
      "Epoch 1 train_loss: 1.998 train_f1: 0.054 \t\n",
      "\n",
      "Validation 1 valid_acc: 0.144 best_acc: 0.144 \t\n",
      "\n",
      "Epoch 2 train_loss: 1.794 train_f1: 0.158 \t\n",
      "\n",
      "Validation 2 valid_acc: 0.407 best_acc: 0.407 \t\n",
      "\n",
      "Epoch 3 train_loss: 1.686 train_f1: 0.259 \t\n",
      "\n",
      "Validation 3 valid_acc: 0.453 best_acc: 0.453 \t\n",
      "\n",
      "Epoch 4 train_loss: 1.598 train_f1: 0.308 \t\n",
      "\n",
      "Validation 4 valid_acc: 0.501 best_acc: 0.501 \t\n",
      "\n",
      "Epoch 5 train_loss: 1.558 train_f1: 0.332 \t\n",
      "\n",
      "Validation 5 valid_acc: 0.522 best_acc: 0.522 \t\n",
      "\n",
      "Epoch 6 train_loss: 1.524 train_f1: 0.349 \t\n",
      "\n",
      "Validation 6 valid_acc: 0.525 best_acc: 0.525 \t\n",
      "\n",
      "Epoch 7 train_loss: 1.493 train_f1: 0.367 \t\n",
      "\n",
      "Validation 7 valid_acc: 0.517 best_acc: 0.525 \t\n",
      "\n",
      "Epoch 8 train_loss: 1.460 train_f1: 0.378 \t\n",
      "\n",
      "Validation 8 valid_acc: 0.546 best_acc: 0.546 \t\n",
      "\n",
      "Epoch 9 train_loss: 1.446 train_f1: 0.390 \t\n",
      "\n",
      "Validation 9 valid_acc: 0.525 best_acc: 0.546 \t\n",
      "\n",
      "Epoch 10 train_loss: 1.413 train_f1: 0.408 \t\n",
      "\n",
      "Validation 10 valid_acc: 0.527 best_acc: 0.546 \t\n",
      "\n",
      "Epoch 11 train_loss: 1.376 train_f1: 0.423 \t\n",
      "\n",
      "Validation 11 valid_acc: 0.554 best_acc: 0.554 \t\n",
      "\n",
      "Epoch 12 train_loss: 1.365 train_f1: 0.424 \t\n",
      "\n",
      "Validation 12 valid_acc: 0.552 best_acc: 0.554 \t\n",
      "\n",
      "Epoch 13 train_loss: 1.358 train_f1: 0.430 \t\n",
      "\n",
      "Validation 13 valid_acc: 0.542 best_acc: 0.554 \t\n",
      "\n",
      "Epoch 14 train_loss: 1.321 train_f1: 0.452 \t\n",
      "\n",
      "Validation 14 valid_acc: 0.549 best_acc: 0.554 \t\n",
      "\n",
      "Epoch 15 train_loss: 1.296 train_f1: 0.460 \t\n",
      "\n",
      "Validation 15 valid_acc: 0.566 best_acc: 0.566 \t\n",
      "\n",
      "Epoch 16 train_loss: 1.267 train_f1: 0.477 \t\n",
      "\n",
      "Validation 16 valid_acc: 0.566 best_acc: 0.566 \t\n",
      "\n",
      "Epoch 17 train_loss: 1.273 train_f1: 0.474 \t\n",
      "\n",
      "Validation 17 valid_acc: 0.572 best_acc: 0.572 \t\n",
      "\n",
      "Epoch 18 train_loss: 1.251 train_f1: 0.493 \t\n",
      "\n",
      "Validation 18 valid_acc: 0.575 best_acc: 0.575 \t\n",
      "\n",
      "Epoch 19 train_loss: 1.232 train_f1: 0.500 \t\n",
      "\n",
      "Validation 19 valid_acc: 0.572 best_acc: 0.575 \t\n",
      "\n",
      "Epoch 20 train_loss: 1.227 train_f1: 0.507 \t\n",
      "\n",
      "Validation 20 valid_acc: 0.574 best_acc: 0.575 \t\n",
      "\n",
      "Epoch 21 train_loss: 1.219 train_f1: 0.506 \t\n",
      "\n",
      "Validation 21 valid_acc: 0.590 best_acc: 0.590 \t\n",
      "\n",
      "Epoch 22 train_loss: 1.174 train_f1: 0.528 \t\n",
      "\n",
      "Validation 22 valid_acc: 0.592 best_acc: 0.592 \t\n",
      "\n",
      "Epoch 23 train_loss: 1.189 train_f1: 0.525 \t\n",
      "\n",
      "Validation 23 valid_acc: 0.586 best_acc: 0.592 \t\n",
      "\n",
      "Epoch 24 train_loss: 1.161 train_f1: 0.540 \t\n",
      "\n",
      "Validation 24 valid_acc: 0.573 best_acc: 0.592 \t\n",
      "\n",
      "Epoch 25 train_loss: 1.140 train_f1: 0.550 \t\n",
      "\n",
      "Validation 25 valid_acc: 0.598 best_acc: 0.598 \t\n",
      "\n",
      "Epoch 26 train_loss: 1.148 train_f1: 0.549 \t\n",
      "\n",
      "Validation 26 valid_acc: 0.599 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 27 train_loss: 1.118 train_f1: 0.562 \t\n",
      "\n",
      "Validation 27 valid_acc: 0.596 best_acc: 0.599 \t\n",
      "\n",
      "Epoch 28 train_loss: 1.099 train_f1: 0.570 \t\n",
      "\n",
      "Validation 28 valid_acc: 0.611 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 29 train_loss: 1.094 train_f1: 0.569 \t\n",
      "\n",
      "Validation 29 valid_acc: 0.611 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 30 train_loss: 1.107 train_f1: 0.572 \t\n",
      "\n",
      "Validation 30 valid_acc: 0.572 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 31 train_loss: 1.068 train_f1: 0.574 \t\n",
      "\n",
      "Validation 31 valid_acc: 0.571 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 32 train_loss: 1.069 train_f1: 0.580 \t\n",
      "\n",
      "Validation 32 valid_acc: 0.596 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 33 train_loss: 1.052 train_f1: 0.589 \t\n",
      "\n",
      "Validation 33 valid_acc: 0.608 best_acc: 0.611 \t\n",
      "\n",
      "Epoch 34 train_loss: 1.052 train_f1: 0.595 \t\n",
      "\n",
      "Validation 34 valid_acc: 0.622 best_acc: 0.622 \t\n",
      "\n",
      "Epoch 35 train_loss: 1.049 train_f1: 0.594 \t\n",
      "\n",
      "Validation 35 valid_acc: 0.597 best_acc: 0.622 \t\n",
      "\n",
      "Epoch 36 train_loss: 1.018 train_f1: 0.601 \t\n",
      "\n",
      "Validation 36 valid_acc: 0.629 best_acc: 0.629 \t\n",
      "\n",
      "Epoch 37 train_loss: 1.007 train_f1: 0.611 \t\n",
      "\n",
      "Validation 37 valid_acc: 0.635 best_acc: 0.635 \t\n",
      "\n",
      "Epoch 38 train_loss: 0.987 train_f1: 0.619 \t\n",
      "\n",
      "Validation 38 valid_acc: 0.615 best_acc: 0.635 \t\n",
      "\n",
      "Epoch 39 train_loss: 0.989 train_f1: 0.613 \t\n",
      "\n",
      "Validation 39 valid_acc: 0.623 best_acc: 0.635 \t\n",
      "\n",
      "Epoch 40 train_loss: 0.974 train_f1: 0.618 \t\n",
      "\n",
      "Validation 40 valid_acc: 0.598 best_acc: 0.635 \t\n",
      "\n",
      "Epoch 41 train_loss: 0.975 train_f1: 0.624 \t\n",
      "\n",
      "Validation 41 valid_acc: 0.605 best_acc: 0.635 \t\n",
      "\n",
      "Epoch 42 train_loss: 0.960 train_f1: 0.627 \t\n",
      "\n",
      "Validation 42 valid_acc: 0.598 best_acc: 0.635 \t\n",
      "\n",
      "Epoch 43 train_loss: 0.957 train_f1: 0.635 \t\n",
      "\n",
      "Validation 43 valid_acc: 0.573 best_acc: 0.635 \t\n",
      "\n",
      "Epoch 44 train_loss: 0.955 train_f1: 0.637 \t\n",
      "\n",
      "Validation 44 valid_acc: 0.606 best_acc: 0.635 \t\n",
      "\n",
      "Epoch 45 train_loss: 0.934 train_f1: 0.644 \t\n",
      "\n",
      "Validation 45 valid_acc: 0.611 best_acc: 0.635 \t\n",
      "\n",
      "Epoch 46 train_loss: 0.940 train_f1: 0.641 \t\n",
      "\n",
      "Validation 46 valid_acc: 0.608 best_acc: 0.635 \t\n",
      "\n",
      "Epoch 47 train_loss: 0.903 train_f1: 0.657 \t\n",
      "\n",
      "Validation 47 valid_acc: 0.588 best_acc: 0.635 \t\n",
      "\n",
      "Epoch 48 train_loss: 0.932 train_f1: 0.650 \t\n",
      "\n",
      "Validation 48 valid_acc: 0.617 best_acc: 0.635 \t\n",
      "\n",
      "Epoch 49 train_loss: 0.910 train_f1: 0.658 \t\n",
      "\n",
      "Validation 49 valid_acc: 0.610 best_acc: 0.635 \t\n",
      "\n",
      "Epoch 50 train_loss: 0.885 train_f1: 0.658 \t\n",
      "\n",
      "Validation 50 valid_acc: 0.632 best_acc: 0.635 \t\n",
      "\n",
      "Epoch 51 train_loss: 0.885 train_f1: 0.662 \t\n",
      "\n",
      "Validation 51 valid_acc: 0.595 best_acc: 0.635 \t\n",
      "\n",
      "Epoch 52 train_loss: 0.893 train_f1: 0.658 \t\n",
      "\n",
      "Validation 52 valid_acc: 0.591 best_acc: 0.635 \t\n",
      "\n",
      "Epoch 53 train_loss: 0.859 train_f1: 0.672 \t\n",
      "\n",
      "Validation 53 valid_acc: 0.591 best_acc: 0.635 \t\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[16,1440,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node Adam_1/gradients/batch_normalization_175/moments/variance_grad/Tile}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-329a09d14b4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum_epoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass2index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_mean_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_std_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEpoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train_loss:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf'{train_loss:.3f}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train_f1:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf'{train_f1:.3f}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ECG_ABN_E%02dL%.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8c5c71e7cbd9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;31m# 4 = f1 of perception pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mtrain_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;31m#         print(train_tmp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_tmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[16,1440,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node Adam_1/gradients/batch_normalization_175/moments/variance_grad/Tile}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "# DISP DATETIME FOR CHECKING TIME\n",
    "start = time.time()\n",
    "val_acc_sum=[]\n",
    "train_loss_sum=[]\n",
    "train_acc_sum=[]\n",
    "val_loss_sum=[]\n",
    "val_acc_min = 0\n",
    "print(results_directory)\n",
    "\n",
    "for num_epoch in range(epochs):\n",
    "    random.shuffle(data_train)\n",
    "    train_loss, train_f1 = train(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "    print('\\nEpoch',num_epoch+1,'train_loss:',f'{train_loss:.3f}','train_f1:',f'{train_f1:.3f}',\"\\t\")\n",
    "    model_output = \"ECG_ABN_E%02dL%.2f\" % (num_epoch, train_loss)\n",
    "    save_name = os.path.join(results_directory, model_output)\n",
    "    \n",
    "    val_acc = test(data_val, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)\n",
    "\n",
    "    if val_acc > val_acc_min:\n",
    "        val_acc_min = val_acc\n",
    "        model.save(save_name)\n",
    "    print('\\nValidation', num_epoch+1, 'valid_acc:',f'{val_acc:.3f}', 'best_acc:',f'{val_acc_min:.3f}', \"\\t\")\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data imbalance problem -> slideshare 1D CNN data augmentation 참고\n",
    "# n=18일때: E18L1.53 (총 20 epoch) ; 어쨌든 loss로만 판단했을 때 n 18 -> 3으로 줄이니까 성능 더 좋아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results_20200525_0_after_std_correction\n",
      "\n",
      "Epoch 87 train_loss: 0.632 train_f1: 0.771 \t\n",
      "\n",
      "Validation 87 valid_acc: 0.613 best_acc: 0.660 \t\n",
      "\n",
      "Epoch 88 train_loss: 0.629 train_f1: 0.772 \t\n",
      "\n",
      "Validation 88 valid_acc: 0.594 best_acc: 0.660 \t\n",
      "\n",
      "Epoch 89 train_loss: 0.624 train_f1: 0.778 \t\n",
      "\n",
      "Validation 89 valid_acc: 0.625 best_acc: 0.660 \t\n",
      "\n",
      "Epoch 90 train_loss: 0.610 train_f1: 0.778 \t\n",
      "\n",
      "Validation 90 valid_acc: 0.621 best_acc: 0.660 \t\n",
      "\n",
      "Epoch 91 train_loss: 0.617 train_f1: 0.776 \t\n",
      "\n",
      "Validation 91 valid_acc: 0.643 best_acc: 0.660 \t\n",
      "\n",
      "Epoch 92 train_loss: 0.597 train_f1: 0.784 \t\n",
      "\n",
      "Validation 92 valid_acc: 0.615 best_acc: 0.660 \t\n",
      "\n",
      "Epoch 93 train_loss: 0.589 train_f1: 0.792 \t\n",
      "\n",
      "Validation 93 valid_acc: 0.609 best_acc: 0.660 \t\n",
      "\n",
      "Epoch 94 train_loss: 0.593 train_f1: 0.790 \t\n",
      "\n",
      "Validation 94 valid_acc: 0.633 best_acc: 0.660 \t\n",
      "\n",
      "Epoch 95 train_loss: 0.594 train_f1: 0.786 \t\n",
      "\n",
      "Validation 95 valid_acc: 0.642 best_acc: 0.660 \t\n",
      "\n",
      "Epoch 96 train_loss: 0.586 train_f1: 0.790 \t\n",
      "\n",
      "Validation 96 valid_acc: 0.637 best_acc: 0.660 \t\n",
      "\n",
      "Epoch 97 train_loss: 0.585 train_f1: 0.790 \t\n",
      "\n",
      "Validation 97 valid_acc: 0.642 best_acc: 0.660 \t\n",
      "\n",
      "Epoch 98 train_loss: 0.590 train_f1: 0.786 \t\n",
      "\n",
      "Validation 98 valid_acc: 0.620 best_acc: 0.660 \t\n",
      "\n",
      "Epoch 99 train_loss: 0.574 train_f1: 0.794 \t\n",
      "\n",
      "Validation 99 valid_acc: 0.632 best_acc: 0.660 \t\n",
      "\n",
      "Epoch 100 train_loss: 0.570 train_f1: 0.799 \t\n",
      "\n",
      "Validation 100 valid_acc: 0.608 best_acc: 0.660 \t\n",
      "\n",
      "Epoch 101 train_loss: 0.573 train_f1: 0.794 \t\n",
      "\n",
      "Validation 101 valid_acc: 0.634 best_acc: 0.660 \t\n",
      "\n",
      "Epoch 102 train_loss: 0.557 train_f1: 0.799 \t\n",
      "\n",
      "Validation 102 valid_acc: 0.627 best_acc: 0.660 \t\n",
      "\n",
      "Epoch 103 train_loss: 0.559 train_f1: 0.804 \t\n",
      "\n",
      "Validation 103 valid_acc: 0.632 best_acc: 0.660 \t\n",
      "\n",
      "Epoch 104 train_loss: 0.545 train_f1: 0.807 \t\n",
      "\n",
      "Validation 104 valid_acc: 0.623 best_acc: 0.660 \t\n",
      "\n",
      "Epoch 105 train_loss: 0.551 train_f1: 0.805 \t\n",
      "\n",
      "Validation 105 valid_acc: 0.661 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 106 train_loss: 0.534 train_f1: 0.811 \t\n",
      "\n",
      "Validation 106 valid_acc: 0.623 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 107 train_loss: 0.532 train_f1: 0.813 \t\n",
      "\n",
      "Validation 107 valid_acc: 0.636 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 108 train_loss: 0.528 train_f1: 0.814 \t\n",
      "\n",
      "Validation 108 valid_acc: 0.624 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 109 train_loss: 0.543 train_f1: 0.808 \t\n",
      "\n",
      "Validation 109 valid_acc: 0.629 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 110 train_loss: 0.527 train_f1: 0.813 \t\n",
      "\n",
      "Validation 110 valid_acc: 0.618 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 111 train_loss: 0.512 train_f1: 0.824 \t\n",
      "\n",
      "Validation 111 valid_acc: 0.600 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 112 train_loss: 0.510 train_f1: 0.820 \t\n",
      "\n",
      "Validation 112 valid_acc: 0.641 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 113 train_loss: 0.506 train_f1: 0.825 \t\n",
      "\n",
      "Validation 113 valid_acc: 0.599 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 114 train_loss: 0.526 train_f1: 0.815 \t\n",
      "\n",
      "Validation 114 valid_acc: 0.643 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 115 train_loss: 0.521 train_f1: 0.818 \t\n",
      "\n",
      "Validation 115 valid_acc: 0.628 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 116 train_loss: 0.498 train_f1: 0.826 \t\n",
      "\n",
      "Validation 116 valid_acc: 0.640 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 117 train_loss: 0.497 train_f1: 0.830 \t\n",
      "\n",
      "Validation 117 valid_acc: 0.647 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 118 train_loss: 0.476 train_f1: 0.835 \t\n",
      "\n",
      "Validation 118 valid_acc: 0.640 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 119 train_loss: 0.478 train_f1: 0.831 \t\n",
      "\n",
      "Validation 119 valid_acc: 0.640 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 120 train_loss: 0.485 train_f1: 0.831 \t\n",
      "\n",
      "Validation 120 valid_acc: 0.636 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 121 train_loss: 0.482 train_f1: 0.829 \t\n",
      "\n",
      "Validation 121 valid_acc: 0.628 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 122 train_loss: 0.485 train_f1: 0.831 \t\n",
      "\n",
      "Validation 122 valid_acc: 0.625 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 123 train_loss: 0.467 train_f1: 0.838 \t\n",
      "\n",
      "Validation 123 valid_acc: 0.615 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 124 train_loss: 0.458 train_f1: 0.840 \t\n",
      "\n",
      "Validation 124 valid_acc: 0.640 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 125 train_loss: 0.478 train_f1: 0.835 \t\n",
      "\n",
      "Validation 125 valid_acc: 0.643 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 126 train_loss: 0.448 train_f1: 0.844 \t\n",
      "\n",
      "Validation 126 valid_acc: 0.640 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 127 train_loss: 0.461 train_f1: 0.837 \t\n",
      "\n",
      "Validation 127 valid_acc: 0.601 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 128 train_loss: 0.467 train_f1: 0.840 \t\n",
      "\n",
      "Validation 128 valid_acc: 0.615 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 129 train_loss: 0.451 train_f1: 0.841 \t\n",
      "\n",
      "Validation 129 valid_acc: 0.640 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 130 train_loss: 0.458 train_f1: 0.838 \t\n",
      "\n",
      "Validation 130 valid_acc: 0.626 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 131 train_loss: 0.435 train_f1: 0.852 \t\n",
      "\n",
      "Validation 131 valid_acc: 0.627 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 132 train_loss: 0.459 train_f1: 0.842 \t\n",
      "\n",
      "Validation 132 valid_acc: 0.613 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 133 train_loss: 0.421 train_f1: 0.855 \t\n",
      "\n",
      "Validation 133 valid_acc: 0.640 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 134 train_loss: 0.444 train_f1: 0.846 \t\n",
      "\n",
      "Validation 134 valid_acc: 0.611 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 135 train_loss: 0.431 train_f1: 0.855 \t\n",
      "\n",
      "Validation 135 valid_acc: 0.606 best_acc: 0.661 \t\n",
      "\n",
      "Epoch 136 train_loss: 0.428 train_f1: 0.855 \t\n"
     ]
    }
   ],
   "source": [
    "# DISP DATETIME FOR CHECKING TIME\n",
    "start = time.time()\n",
    "# val_acc_sum=[]\n",
    "# train_loss_sum=[]\n",
    "# train_acc_sum=[]\n",
    "# val_loss_sum=[]\n",
    "# val_acc_min = 0\n",
    "print(results_directory)\n",
    "\n",
    "for num_epoch in range(epochs):\n",
    "    num_epoch+=86\n",
    "    random.shuffle(data_train)\n",
    "    train_loss, train_f1 = train(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "    print('\\nEpoch',num_epoch+1,'train_loss:',f'{train_loss:.3f}','train_f1:',f'{train_f1:.3f}',\"\\t\")\n",
    "    model_output = \"ECG_ABN_E%02dL%.2f\" % (num_epoch, train_loss)\n",
    "    save_name = os.path.join(results_directory, model_output)\n",
    "    \n",
    "    val_acc = test(data_val, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)\n",
    "\n",
    "    if val_acc > val_acc_min:\n",
    "        val_acc_min = val_acc\n",
    "        model.save(save_name)\n",
    "    print('\\nValidation', num_epoch+1, 'valid_acc:',f'{val_acc:.3f}', 'best_acc:',f'{val_acc_min:.3f}', \"\\t\")\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "395.881px",
    "left": "1487.43px",
    "right": "20px",
    "top": "128.94px",
    "width": "200.142px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
