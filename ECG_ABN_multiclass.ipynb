{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numba/errors.py:137: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import keras\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from ABNmodules_multiclass import *\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "tf.set_random_seed(1004)\n",
    "random.seed(1004)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5225,) (276,) (1376,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def score_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "def get_unique_classes(input_directory,files):\n",
    "\n",
    "    unique_classes=set()\n",
    "    for f in files:\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    for c in tmp:\n",
    "                        unique_classes.add(c.strip())\n",
    "    return sorted(unique_classes)\n",
    "\n",
    "def one_hot_encoding(one_hot_vector,y, class2index):\n",
    "    ind=class2index[y]\n",
    "    one_hot_vector[ind]=1\n",
    "    return one_hot_vector\n",
    "\n",
    "# Search for multi-label subjects\n",
    "def searching_overlap(input_directory,class2index, input_file_names):\n",
    "    multiclasses=[]\n",
    "    multisubjects=[]\n",
    "    number = []\n",
    "    for file in input_file_names:\n",
    "        f=file\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    if len(tmp)>1:\n",
    "                        one_hot_vector = [0]*(len(class2index))\n",
    "                        for c in tmp:\n",
    "                            one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                        multiclasses.append(one_hot_vector)\n",
    "                        multisubjects.append(g)\n",
    "                        number.append(len(tmp))\n",
    "    return multisubjects, multiclasses, number\n",
    "\n",
    "def block_feature(sequence_en, minimum_len): \n",
    "    new_en = []\n",
    "    if len(sequence_en) > minimum_len:  # 길이가 minimum보다 긴 경우\n",
    "        start = random.randint(0,len(sequence_en)-minimum_len)\n",
    "        #print(start)\n",
    "        new_en = sequence_en[start:start+minimum_len]\n",
    "    elif len(sequence_en) == minimum_len: # 길이가 minimum\n",
    "        new_en = sequence_en\n",
    "    else: \n",
    "        assert len(sequence_en) <= minimum_len\n",
    "    return new_en\n",
    "\n",
    "def exploratory_look(input_directory,file, class2index):\n",
    "    classes = []\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                print(tmp, len(tmp))\n",
    "    return tmp     \n",
    "\n",
    "# Get classes of sorted file names\n",
    "def get_labels(input_directory,file, class2index):\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                one_hot_vector = [0]*(len(class2index))\n",
    "                for c in tmp:\n",
    "                    one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                \n",
    "    return one_hot_vector\n",
    "\n",
    "def randextract_mels(curr_step, batch_size, data, mel_directory, class2index, minimum_len, x_mean_final, x_std_final):\n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    start = batch_size*curr_step\n",
    "    end = batch_size*(curr_step+1)\n",
    "    curr_file_indices = data[start:end]\n",
    "    for file in curr_file_indices:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        clip_file = block_feature(tmp_file, minimum_len)\n",
    "        clip_file -= x_mean_final\n",
    "        clip_file /= x_std_final\n",
    "        mel_files.append(clip_file)\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        classes.append(label)\n",
    "    \n",
    "    concat = list(zip(mel_files, classes))\n",
    "    random.shuffle(concat)\n",
    "    mel_files, classes = zip(*concat)\n",
    "    return mel_files, classes\n",
    "\n",
    "\n",
    "\n",
    "def train(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final, model): \n",
    "    loss=[]\n",
    "#     acc = []\n",
    "    f1 = []\n",
    "\n",
    "    total_steps = int(np.ceil(len(data_train)/batch_size))\n",
    "    for curr_step in range(total_steps): # loops over batches\n",
    "        batch_mels, batch_labels = randextract_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "\n",
    "        batch_mels = np.asarray(batch_mels)\n",
    "        batch_labels = np.asarray(np.squeeze(batch_labels))\n",
    "        \n",
    "        batch_labels = [batch_labels, batch_labels]\n",
    "\n",
    "\n",
    "# return of train\n",
    "# 0 = total loss (attention branch + perception branch)\n",
    "# 1 = loss of attention pred\n",
    "# 2 = loss of perception pred\n",
    "# 3 = f1 of attention pred\n",
    "# 4 = f1 of perception pred     \n",
    "\n",
    "\n",
    "        train_tmp = model.train_on_batch(batch_mels, batch_labels)\n",
    "        loss_ = train_tmp[0]/2\n",
    "        f1_ = np.mean((train_tmp[3], train_tmp[4]))\n",
    "        loss.append(loss_)\n",
    "        f1.append(f1_)\n",
    "\n",
    "    loss = np.mean(np.array(loss))\n",
    "    f1 = np.mean(np.array(f1))\n",
    "    return loss, f1\n",
    "\n",
    "def train_edit(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final, model): \n",
    "    loss=[]\n",
    "#     acc = []\n",
    "    f1 = []\n",
    "\n",
    "    total_steps = int(np.ceil(len(data_train)/batch_size))\n",
    "    for curr_step in range(total_steps): # loops over batches\n",
    "        batch_mels, batch_labels = randextract_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "\n",
    "        batch_mels = np.asarray(batch_mels)\n",
    "        batch_labels = np.asarray(np.squeeze(batch_labels))\n",
    "\n",
    "\n",
    "        heatmap = CAM_conv1D(minimum_len, n_channels, batch_mels, batch_labels, out_len, get_conv_out)        \n",
    "        heatmap = np.asarray(heatmap)\n",
    "        \n",
    "        batch_labels = [batch_labels, batch_labels]\n",
    "\n",
    "\n",
    "# return of train\n",
    "# 0 = total loss (attention branch + perception branch)\n",
    "# 1 = loss of attention pred\n",
    "# 2 = loss of perception pred\n",
    "# 3 = f1 of attention pred\n",
    "# 4 = f1 of perception pred     \n",
    "\n",
    "        train_tmp = model.train_on_batch([batch_mels, heatmap], batch_labels)\n",
    "        loss_ = train_tmp[0]/2\n",
    "        f1_ = np.mean((train_tmp[3], train_tmp[4]))\n",
    "        loss.append(loss_)\n",
    "        f1.append(f1_)\n",
    "\n",
    "    loss = np.mean(np.array(loss))\n",
    "    f1 = np.mean(np.array(f1))\n",
    "    return loss, f1\n",
    "\n",
    "def test(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred=[]\n",
    "    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    \n",
    "    multi_files, _, _ = searching_overlap(input_directory,class2index, data)\n",
    "    multi_acc = 0\n",
    "    multi_y_true=[]\n",
    "    multi_y_pred=[]    \n",
    "    \n",
    "    \n",
    "    for file in data:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "        heatmap_files=[]\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        label = [label]\n",
    "        for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "            clip_file = clip_file.reshape(1,minimum_len,n_channels)\n",
    "            mel_files.append(clip_file)    \n",
    "        mel_files = np.asarray(mel_files)\n",
    "        mel_files = mel_files.reshape(steps,minimum_len,n_channels)\n",
    "        logit = model.predict(mel_files)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        pred = np.zeros(len(logit))\n",
    "        for ii, label in enumerate(logit):\n",
    "            if label >= 0.5:\n",
    "                pred[ii] = 1\n",
    "            else:\n",
    "                pred[ii] = 0\n",
    "        pred = pred.tolist()\n",
    "        y_pred.append(pred)\n",
    "        label = get_labels(input_directory,file,class2index)\n",
    "        y_true.append(label)\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        if file.replace('.mat', '.hea') in multi_files:\n",
    "            multi_y_true.append(label)            \n",
    "            multi_y_pred.append(pred)\n",
    "            multi_acc += acc       \n",
    "        \n",
    "    final_acc = total_acc / len(data)\n",
    "    f1_classes = f1_score(y_true, y_pred, average=None)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    \n",
    "    multi_final_acc = multi_acc / len(multi_files)\n",
    "    multi_f1_classes = f1_score(multi_y_true, multi_y_pred, average=None)\n",
    "    multi_f1_micro = f1_score(multi_y_true, multi_y_pred, average='micro')\n",
    "    \n",
    "    return final_acc, f1_classes, f1_micro, multi_final_acc, multi_f1_classes, multi_f1_micro\n",
    "\n",
    "\n",
    "\n",
    "def test_edit(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    \n",
    "    multi_files, _, _ = searching_overlap(input_directory,class2index, data)\n",
    "    multi_acc = 0\n",
    "    multi_y_true=[]\n",
    "    multi_y_pred=[]    \n",
    "    \n",
    "    for file in data:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "        heatmap_files=[]\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        label = [label]\n",
    "        for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "            clip_file = clip_file.reshape(1,minimum_len,n_channels)\n",
    "            heatmap = CAM_conv1D(minimum_len, n_channels, clip_file, label, out_len, get_conv_out)\n",
    "            mel_files.append(clip_file)    \n",
    "            heatmap_files.append(heatmap)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "        heatmap_files = np.asarray(heatmap_files)\n",
    "        heatmap_files = heatmap_files.reshape(steps, out_len,1)\n",
    "        mel_files = mel_files.reshape(steps,minimum_len,n_channels)\n",
    "        logit = model.predict([mel_files, heatmap_files])\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        pred = np.zeros(len(logit))\n",
    "        for ii, label in enumerate(logit):\n",
    "            if label >= 0.5:\n",
    "                pred[ii] = 1\n",
    "            else:\n",
    "                pred[ii] = 0\n",
    "        pred = pred.tolist()\n",
    "        y_pred.append(pred)\n",
    "        label = get_labels(input_directory,file,class2index)\n",
    "        y_true.append(label)\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        if file.replace('.mat', '.hea') in multi_files:\n",
    "            multi_y_true.append(label)\n",
    "            multi_y_pred.append(pred)\n",
    "            multi_acc += acc               \n",
    "    final_acc = total_acc / len(data)\n",
    "    f1_classes = f1_score(y_true, y_pred, average=None)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    \n",
    "    multi_final_acc = multi_acc / len(multi_files)\n",
    "    multi_f1_classes = f1_score(multi_y_true, multi_y_pred, average=None)\n",
    "    multi_f1_micro = f1_score(multi_y_true, multi_y_pred, average='micro')\n",
    "    \n",
    "    return final_acc, f1_classes, f1_micro, multi_final_acc, multi_f1_classes, multi_f1_micro\n",
    "\n",
    "def test_edit_final(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final, p_model):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred=[]\n",
    "    \n",
    "    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    \n",
    "    multi_files, _, _ = searching_overlap(input_directory,class2index, data)\n",
    "    multi_acc = 0\n",
    "    multi_y_true=[]\n",
    "    multi_y_pred=[]\n",
    "    \n",
    "    for file in data:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "        mel_files_logit = []\n",
    "        heatmap_files=[]\n",
    "\n",
    "        for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "            clip_file = clip_file.reshape(1,minimum_len,n_channels)\n",
    "            \n",
    "            # 여기서 logit으로 y구함#####################\n",
    "            logit_ = p_model.predict(clip_file)\n",
    "            logit_ = np.mean(logit_, axis=0)\n",
    "            logit_ = np.mean(logit_, axis=0)\n",
    "            pred_ = np.zeros(len(logit_))\n",
    "            for ii, label in enumerate(logit_):\n",
    "                if label >= 0.5: \n",
    "                    pred_[ii] = 1\n",
    "                else:\n",
    "                    pred_[ii] = 0\n",
    "            pred_ = pred_.tolist()\n",
    "            ##########################################\n",
    "            \n",
    "            heatmap = CAM_conv1D(minimum_len, n_channels, clip_file, [pred_], out_len, get_conv_out)\n",
    "            mel_files.append(clip_file)    \n",
    "            heatmap_files.append(heatmap)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "        heatmap_files = np.asarray(heatmap_files)\n",
    "        heatmap_files = heatmap_files.reshape(steps, out_len,1)\n",
    "        mel_files = mel_files.reshape(steps,minimum_len,n_channels)\n",
    "        logit = model.predict([mel_files, heatmap_files])\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        pred = np.zeros(len(logit))\n",
    "        for ii, label in enumerate(logit):\n",
    "            if label >= 0.5:\n",
    "                pred[ii] = 1\n",
    "            else:\n",
    "                pred[ii] = 0\n",
    "        pred = pred.tolist()\n",
    "        y_pred.append(pred)\n",
    "        label = get_labels(input_directory,file,class2index)\n",
    "        y_true.append(label)\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        if file.replace('.mat', '.hea') in multi_files:\n",
    "            multi_y_true.append(label)          \n",
    "            multi_y_pred.append(pred)\n",
    "            multi_acc += acc       \n",
    "\n",
    "    final_acc = total_acc / len(data)\n",
    "    f1_classes = f1_score(y_true, y_pred, average=None)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    \n",
    "    multi_final_acc = multi_acc / len(multi_files)\n",
    "    multi_f1_classes = f1_score(multi_y_true, multi_y_pred, average=None)\n",
    "    multi_f1_micro = f1_score(multi_y_true, multi_y_pred, average='micro')\n",
    "    \n",
    "    return final_acc, f1_classes, f1_micro, multi_final_acc, multi_f1_classes, multi_f1_micro\n",
    "\n",
    "def test_edit_final2(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final, p_model):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred=[]    \n",
    "\n",
    "    multi_files, _, _ = searching_overlap(input_directory,class2index, data)\n",
    "    multi_acc = 0\n",
    "    multi_y_true=[]\n",
    "    multi_y_pred=[]    \n",
    "    \n",
    "    for file in data:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "        mel_files_logit = []\n",
    "        heatmap_files=[]\n",
    "\n",
    "        for block in range(steps):\n",
    "            # 여기서 logit으로 y구함\n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "            clip_file = clip_file.reshape(1,minimum_len,n_channels)\n",
    "            mel_files_logit.append(clip_file)\n",
    "            \n",
    "        mel_files_logit = np.asarray(mel_files_logit)\n",
    "        mel_files_logit = mel_files_logit.reshape(steps,minimum_len,n_channels)\n",
    "        \n",
    "        \n",
    "        \n",
    "        logit_ = p_model.predict(mel_files_logit)\n",
    "        \n",
    "        logit_ = np.mean(logit_, axis=0)\n",
    "        logit_ = np.mean(logit_, axis=0)\n",
    "#         print(logit_)\n",
    "        pred_ = np.zeros(len(logit_))\n",
    "        for ii, label in enumerate(logit_):\n",
    "            if label >= 0.5: \n",
    "                pred_[ii] = 1\n",
    "            else:\n",
    "                pred_[ii] = 0\n",
    "        \n",
    "        pred_ = pred_.tolist()\n",
    "\n",
    "#         print(pred_)\n",
    "            #######\n",
    "        \n",
    "        \n",
    "        for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "            clip_file = clip_file.reshape(1,minimum_len,n_channels)\n",
    "\n",
    "            heatmap = CAM_conv1D(minimum_len, n_channels, clip_file, [pred_], out_len, get_conv_out)\n",
    "#             print(heatmap)\n",
    "            mel_files.append(clip_file)    \n",
    "            heatmap_files.append(heatmap)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "        heatmap_files = np.asarray(heatmap_files)\n",
    "        heatmap_files = heatmap_files.reshape(steps, out_len,1)\n",
    "        mel_files = mel_files.reshape(steps,minimum_len,n_channels)\n",
    "        logit = model.predict([mel_files, heatmap_files])\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        pred = np.zeros(len(logit))\n",
    "       \n",
    "\n",
    "        for ii, label in enumerate(logit):\n",
    "            if label >= 0.5:\n",
    "                pred[ii] = 1\n",
    "            else:\n",
    "                pred[ii] = 0\n",
    "        pred = pred.tolist()\n",
    "        y_pred.append(pred)\n",
    "#         print(pred)\n",
    "        label = get_labels(input_directory,file,class2index)\n",
    "        y_true.append(label)\n",
    "#         print(label)\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        if file.replace('.mat', '.hea') in multi_files:\n",
    "            multi_y_true.append(label)\n",
    "            multi_y_pred.append(pred)\n",
    "            multi_acc += acc       \n",
    "\n",
    "    final_acc = total_acc / len(data)\n",
    "    f1_classes = f1_score(y_true, y_pred, average=None)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    \n",
    "    multi_final_acc = multi_acc / len(multi_files)\n",
    "    multi_f1_classes = f1_score(multi_y_true, multi_y_pred, average=None)\n",
    "    multi_f1_micro = f1_score(multi_y_true, multi_y_pred, average='micro')\n",
    "    \n",
    "    return final_acc, f1_classes, f1_micro, multi_final_acc, multi_f1_classes, multi_f1_micro\n",
    "\n",
    "batch_size = 32#16#20#32#5#2#1#10#32\n",
    "minimum_len = 2880\n",
    "epochs = 1000#10000#1000#300#370#100#1000\n",
    "n_channels=12\n",
    "loss_function = 'binary_crossentropy' \n",
    "rootdir = '../'\n",
    "date = dt.datetime.today().strftime(\"%Y%m%d\")\n",
    "input_directory = os.path.join(rootdir, 'Training_WFDB')\n",
    "mel_name = 'Raw_data_20200424' \n",
    "mel_directory = os.path.join(rootdir, mel_name)\n",
    "# results_directory = os.path.join(rootdir, 'results_'+date+'_0_IEEE_n=1')\n",
    "# results_directory = os.path.join(rootdir, 'results_'+date+'_editting_CAM_multiclass')\n",
    "\n",
    "if not os.path.isdir(input_directory):\n",
    "    os.mkdir(input_directory)\n",
    "if not os.path.isdir(mel_directory):\n",
    "    os.mkdir(mel_directory)\n",
    "\n",
    "        \n",
    "input_files = []\n",
    "for f in os.listdir(input_directory):\n",
    "    if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('mat'):\n",
    "        input_files.append(f)\n",
    "input_file_names = sorted(input_files)\n",
    "\n",
    "unique_classes = get_unique_classes(input_directory, input_files)\n",
    "class2index = {}\n",
    "for a, b in enumerate(unique_classes):\n",
    "    class2index[b] = a\n",
    "\n",
    "np.shape(input_file_names)\n",
    "\n",
    "x_mean_all = []\n",
    "x_std_all = []\n",
    "for file in input_file_names:\n",
    "    x = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "    x_mean = [np.mean(x[:,0]), np.mean(x[:,1]), np.mean(x[:,2]), np.mean(x[:,3]), np.mean(x[:,4]), np.mean(x[:,5]),\n",
    "             np.mean(x[:,6]), np.mean(x[:,7]), np.mean(x[:,8]), np.mean(x[:,9]), np.mean(x[:,10]), np.mean(x[:,11])]\n",
    "    \n",
    "    x_std = [np.std(x[:,0]), np.std(x[:,1]), np.std(x[:,2]), np.std(x[:,3]), np.std(x[:,4]), np.std(x[:,5]),\n",
    "             np.std(x[:,6]), np.std(x[:,7]), np.std(x[:,8]), np.std(x[:,9]), np.std(x[:,10]), np.std(x[:,11])]\n",
    "    x_mean_all.append(x_mean)\n",
    "    x_std_all.append(x_std) # yjs corrected on 2020-05-25\n",
    "x_mean_final = np.mean(x_mean_all, axis=0)\n",
    "x_std_final = np.mean(x_std_all, axis=0)\n",
    "\n",
    "\n",
    "data, data_test = train_test_split(input_file_names, test_size = 0.2, train_size = 0.8, shuffle=True, random_state=1004)\n",
    "data_train, data_val = train_test_split(data, test_size = 0.05, train_size = 0.95, shuffle=True, random_state=1004)\n",
    "print(np.shape(data_train), np.shape(data_val), np.shape(data_test))\n",
    "\n",
    "main_input = Input(shape=(minimum_len,12), dtype='float32', name='main_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A2355.mat',\n",
       " 'A3446.mat',\n",
       " 'A1814.mat',\n",
       " 'A3575.mat',\n",
       " 'A6197.mat',\n",
       " 'A6727.mat',\n",
       " 'A3974.mat',\n",
       " 'A3380.mat',\n",
       " 'A1978.mat',\n",
       " 'A2265.mat',\n",
       " 'A3514.mat',\n",
       " 'A4147.mat',\n",
       " 'A3852.mat',\n",
       " 'A0245.mat',\n",
       " 'A1846.mat',\n",
       " 'A5979.mat',\n",
       " 'A1696.mat',\n",
       " 'A3454.mat',\n",
       " 'A5164.mat',\n",
       " 'A6827.mat',\n",
       " 'A6778.mat',\n",
       " 'A5403.mat',\n",
       " 'A5567.mat',\n",
       " 'A5923.mat',\n",
       " 'A0254.mat',\n",
       " 'A4950.mat',\n",
       " 'A3926.mat',\n",
       " 'A4083.mat',\n",
       " 'A0223.mat',\n",
       " 'A6569.mat',\n",
       " 'A5393.mat',\n",
       " 'A6343.mat',\n",
       " 'A0300.mat',\n",
       " 'A3324.mat',\n",
       " 'A5812.mat',\n",
       " 'A6865.mat',\n",
       " 'A6600.mat',\n",
       " 'A6153.mat',\n",
       " 'A5947.mat',\n",
       " 'A0999.mat',\n",
       " 'A0613.mat',\n",
       " 'A5423.mat',\n",
       " 'A5316.mat',\n",
       " 'A2158.mat',\n",
       " 'A3799.mat',\n",
       " 'A0598.mat',\n",
       " 'A2677.mat',\n",
       " 'A5973.mat',\n",
       " 'A2911.mat',\n",
       " 'A5053.mat',\n",
       " 'A1400.mat',\n",
       " 'A1283.mat',\n",
       " 'A3298.mat',\n",
       " 'A2962.mat',\n",
       " 'A3459.mat',\n",
       " 'A5462.mat',\n",
       " 'A5589.mat',\n",
       " 'A1946.mat',\n",
       " 'A3583.mat',\n",
       " 'A3842.mat',\n",
       " 'A5419.mat',\n",
       " 'A1267.mat',\n",
       " 'A2669.mat',\n",
       " 'A6353.mat',\n",
       " 'A4129.mat',\n",
       " 'A1017.mat',\n",
       " 'A6503.mat',\n",
       " 'A2249.mat',\n",
       " 'A0345.mat',\n",
       " 'A0445.mat',\n",
       " 'A6595.mat',\n",
       " 'A3295.mat',\n",
       " 'A2297.mat',\n",
       " 'A2339.mat',\n",
       " 'A4796.mat',\n",
       " 'A2592.mat',\n",
       " 'A5975.mat',\n",
       " 'A6603.mat',\n",
       " 'A4948.mat',\n",
       " 'A2484.mat',\n",
       " 'A0970.mat',\n",
       " 'A1486.mat',\n",
       " 'A6150.mat',\n",
       " 'A4953.mat',\n",
       " 'A0401.mat',\n",
       " 'A1884.mat',\n",
       " 'A0142.mat',\n",
       " 'A4253.mat',\n",
       " 'A0091.mat',\n",
       " 'A5929.mat',\n",
       " 'A2437.mat',\n",
       " 'A2848.mat',\n",
       " 'A6081.mat',\n",
       " 'A3128.mat',\n",
       " 'A3368.mat',\n",
       " 'A5696.mat',\n",
       " 'A3561.mat',\n",
       " 'A2113.mat',\n",
       " 'A6672.mat',\n",
       " 'A0642.mat',\n",
       " 'A0601.mat',\n",
       " 'A2875.mat',\n",
       " 'A5877.mat',\n",
       " 'A0161.mat',\n",
       " 'A2241.mat',\n",
       " 'A2224.mat',\n",
       " 'A0278.mat',\n",
       " 'A5825.mat',\n",
       " 'A5221.mat',\n",
       " 'A5564.mat',\n",
       " 'A3909.mat',\n",
       " 'A0436.mat',\n",
       " 'A1418.mat',\n",
       " 'A2874.mat',\n",
       " 'A2630.mat',\n",
       " 'A3948.mat',\n",
       " 'A2637.mat',\n",
       " 'A0603.mat',\n",
       " 'A6694.mat',\n",
       " 'A6105.mat',\n",
       " 'A0654.mat',\n",
       " 'A3328.mat',\n",
       " 'A6777.mat',\n",
       " 'A4679.mat',\n",
       " 'A2877.mat',\n",
       " 'A1102.mat',\n",
       " 'A0850.mat',\n",
       " 'A1131.mat',\n",
       " 'A4874.mat',\n",
       " 'A1434.mat',\n",
       " 'A1305.mat',\n",
       " 'A5644.mat',\n",
       " 'A1749.mat',\n",
       " 'A0283.mat',\n",
       " 'A4110.mat',\n",
       " 'A6104.mat',\n",
       " 'A5254.mat',\n",
       " 'A0207.mat',\n",
       " 'A4018.mat',\n",
       " 'A3463.mat',\n",
       " 'A6661.mat',\n",
       " 'A1912.mat',\n",
       " 'A3615.mat',\n",
       " 'A2600.mat',\n",
       " 'A4786.mat',\n",
       " 'A4181.mat',\n",
       " 'A0890.mat',\n",
       " 'A4371.mat',\n",
       " 'A2697.mat',\n",
       " 'A1950.mat',\n",
       " 'A6241.mat',\n",
       " 'A2606.mat',\n",
       " 'A2882.mat',\n",
       " 'A4592.mat',\n",
       " 'A4781.mat',\n",
       " 'A6766.mat',\n",
       " 'A4746.mat',\n",
       " 'A3157.mat',\n",
       " 'A2432.mat',\n",
       " 'A5832.mat',\n",
       " 'A0937.mat',\n",
       " 'A6829.mat',\n",
       " 'A5914.mat',\n",
       " 'A2639.mat',\n",
       " 'A0509.mat',\n",
       " 'A0697.mat',\n",
       " 'A0251.mat',\n",
       " 'A1586.mat',\n",
       " 'A2259.mat',\n",
       " 'A4245.mat',\n",
       " 'A4234.mat',\n",
       " 'A1251.mat',\n",
       " 'A5470.mat',\n",
       " 'A3447.mat',\n",
       " 'A4630.mat',\n",
       " 'A4743.mat',\n",
       " 'A1954.mat',\n",
       " 'A4112.mat',\n",
       " 'A4837.mat',\n",
       " 'A2998.mat',\n",
       " 'A6774.mat',\n",
       " 'A3471.mat',\n",
       " 'A0902.mat',\n",
       " 'A5927.mat',\n",
       " 'A5933.mat',\n",
       " 'A5450.mat',\n",
       " 'A1123.mat',\n",
       " 'A4251.mat',\n",
       " 'A6068.mat',\n",
       " 'A3008.mat',\n",
       " 'A6152.mat',\n",
       " 'A5355.mat',\n",
       " 'A2787.mat',\n",
       " 'A6362.mat',\n",
       " 'A2352.mat',\n",
       " 'A0956.mat',\n",
       " 'A3420.mat',\n",
       " 'A1147.mat',\n",
       " 'A5597.mat',\n",
       " 'A3294.mat',\n",
       " 'A0318.mat',\n",
       " 'A6359.mat',\n",
       " 'A5444.mat',\n",
       " 'A0012.mat',\n",
       " 'A1032.mat',\n",
       " 'A2603.mat',\n",
       " 'A0391.mat',\n",
       " 'A4011.mat',\n",
       " 'A0581.mat',\n",
       " 'A0316.mat',\n",
       " 'A6372.mat',\n",
       " 'A3935.mat',\n",
       " 'A1545.mat',\n",
       " 'A2508.mat',\n",
       " 'A1204.mat',\n",
       " 'A2138.mat',\n",
       " 'A4313.mat',\n",
       " 'A1895.mat',\n",
       " 'A3818.mat',\n",
       " 'A6537.mat',\n",
       " 'A2231.mat',\n",
       " 'A2080.mat',\n",
       " 'A3719.mat',\n",
       " 'A3150.mat',\n",
       " 'A0468.mat',\n",
       " 'A2497.mat',\n",
       " 'A1137.mat',\n",
       " 'A6828.mat',\n",
       " 'A5422.mat',\n",
       " 'A0998.mat',\n",
       " 'A4667.mat',\n",
       " 'A6527.mat',\n",
       " 'A5514.mat',\n",
       " 'A3770.mat',\n",
       " 'A3418.mat',\n",
       " 'A3423.mat',\n",
       " 'A6709.mat',\n",
       " 'A3080.mat',\n",
       " 'A2515.mat',\n",
       " 'A2192.mat',\n",
       " 'A0048.mat',\n",
       " 'A3144.mat',\n",
       " 'A1709.mat',\n",
       " 'A6763.mat',\n",
       " 'A6858.mat',\n",
       " 'A1787.mat',\n",
       " 'A4214.mat',\n",
       " 'A0446.mat',\n",
       " 'A2178.mat',\n",
       " 'A5659.mat',\n",
       " 'A2262.mat',\n",
       " 'A0096.mat',\n",
       " 'A0809.mat',\n",
       " 'A2146.mat',\n",
       " 'A2499.mat',\n",
       " 'A6062.mat',\n",
       " 'A1661.mat',\n",
       " 'A1651.mat',\n",
       " 'A5370.mat',\n",
       " 'A2755.mat',\n",
       " 'A3924.mat',\n",
       " 'A6458.mat',\n",
       " 'A3325.mat',\n",
       " 'A5239.mat',\n",
       " 'A0420.mat',\n",
       " 'A0504.mat',\n",
       " 'A6374.mat',\n",
       " 'A2201.mat',\n",
       " 'A2520.mat',\n",
       " 'A2452.mat',\n",
       " 'A5103.mat',\n",
       " 'A4291.mat',\n",
       " 'A2460.mat',\n",
       " 'A5888.mat',\n",
       " 'A2021.mat',\n",
       " 'A1629.mat',\n",
       " 'A3831.mat',\n",
       " 'A1410.mat',\n",
       " 'A4493.mat',\n",
       " 'A6701.mat',\n",
       " 'A6842.mat',\n",
       " 'A1300.mat',\n",
       " 'A5683.mat',\n",
       " 'A1908.mat',\n",
       " 'A5785.mat',\n",
       " 'A6298.mat',\n",
       " 'A0060.mat',\n",
       " 'A6313.mat',\n",
       " 'A0101.mat',\n",
       " 'A6312.mat',\n",
       " 'A3748.mat',\n",
       " 'A2028.mat',\n",
       " 'A6728.mat',\n",
       " 'A2434.mat',\n",
       " 'A6818.mat',\n",
       " 'A2416.mat',\n",
       " 'A0747.mat',\n",
       " 'A3355.mat',\n",
       " 'A0441.mat',\n",
       " 'A5375.mat',\n",
       " 'A1795.mat',\n",
       " 'A2154.mat',\n",
       " 'A1216.mat',\n",
       " 'A5051.mat',\n",
       " 'A3200.mat',\n",
       " 'A3792.mat',\n",
       " 'A0024.mat',\n",
       " 'A0986.mat',\n",
       " 'A6645.mat',\n",
       " 'A3010.mat',\n",
       " 'A3114.mat',\n",
       " 'A2312.mat',\n",
       " 'A6182.mat',\n",
       " 'A0175.mat',\n",
       " 'A3392.mat',\n",
       " 'A3929.mat',\n",
       " 'A6349.mat',\n",
       " 'A2597.mat',\n",
       " 'A0985.mat',\n",
       " 'A2015.mat',\n",
       " 'A6756.mat',\n",
       " 'A3410.mat',\n",
       " 'A3549.mat',\n",
       " 'A4647.mat',\n",
       " 'A6252.mat',\n",
       " 'A2844.mat',\n",
       " 'A4977.mat',\n",
       " 'A4488.mat',\n",
       " 'A2128.mat',\n",
       " 'A4640.mat',\n",
       " 'A3302.mat',\n",
       " 'A4613.mat',\n",
       " 'A6649.mat',\n",
       " 'A1163.mat',\n",
       " 'A0506.mat',\n",
       " 'A1700.mat',\n",
       " 'A1311.mat',\n",
       " 'A0553.mat',\n",
       " 'A4834.mat',\n",
       " 'A5783.mat',\n",
       " 'A6083.mat',\n",
       " 'A2120.mat',\n",
       " 'A1277.mat',\n",
       " 'A2984.mat',\n",
       " 'A0880.mat',\n",
       " 'A6439.mat',\n",
       " 'A6032.mat',\n",
       " 'A3254.mat',\n",
       " 'A2301.mat',\n",
       " 'A2834.mat',\n",
       " 'A4737.mat',\n",
       " 'A1303.mat',\n",
       " 'A0105.mat',\n",
       " 'A6311.mat',\n",
       " 'A3580.mat',\n",
       " 'A0390.mat',\n",
       " 'A6748.mat',\n",
       " 'A0064.mat',\n",
       " 'A4275.mat',\n",
       " 'A1887.mat',\n",
       " 'A0957.mat',\n",
       " 'A0851.mat',\n",
       " 'A4793.mat',\n",
       " 'A6307.mat',\n",
       " 'A1213.mat',\n",
       " 'A4367.mat',\n",
       " 'A1502.mat',\n",
       " 'A4775.mat',\n",
       " 'A0148.mat',\n",
       " 'A6294.mat',\n",
       " 'A0021.mat',\n",
       " 'A3151.mat',\n",
       " 'A0683.mat',\n",
       " 'A2833.mat',\n",
       " 'A5744.mat',\n",
       " 'A5096.mat',\n",
       " 'A5739.mat',\n",
       " 'A4506.mat',\n",
       " 'A5141.mat',\n",
       " 'A6544.mat',\n",
       " 'A4383.mat',\n",
       " 'A1298.mat',\n",
       " 'A4272.mat',\n",
       " 'A5209.mat',\n",
       " 'A2044.mat',\n",
       " 'A3914.mat',\n",
       " 'A5106.mat',\n",
       " 'A6604.mat',\n",
       " 'A6725.mat',\n",
       " 'A3365.mat',\n",
       " 'A3658.mat',\n",
       " 'A2808.mat',\n",
       " 'A3116.mat',\n",
       " 'A1681.mat',\n",
       " 'A0723.mat',\n",
       " 'A0017.mat',\n",
       " 'A5229.mat',\n",
       " 'A4125.mat',\n",
       " 'A4091.mat',\n",
       " 'A0293.mat',\n",
       " 'A0800.mat',\n",
       " 'A4114.mat',\n",
       " 'A2653.mat',\n",
       " 'A3118.mat',\n",
       " 'A0384.mat',\n",
       " 'A5320.mat',\n",
       " 'A3768.mat',\n",
       " 'A4759.mat',\n",
       " 'A4441.mat',\n",
       " 'A3548.mat',\n",
       " 'A2721.mat',\n",
       " 'A6581.mat',\n",
       " 'A3394.mat',\n",
       " 'A4719.mat',\n",
       " 'A4675.mat',\n",
       " 'A2948.mat',\n",
       " 'A6033.mat',\n",
       " 'A2832.mat',\n",
       " 'A4560.mat',\n",
       " 'A2235.mat',\n",
       " 'A4887.mat',\n",
       " 'A2104.mat',\n",
       " 'A0486.mat',\n",
       " 'A3557.mat',\n",
       " 'A0310.mat',\n",
       " 'A0820.mat',\n",
       " 'A3236.mat',\n",
       " 'A4609.mat',\n",
       " 'A0812.mat',\n",
       " 'A5261.mat',\n",
       " 'A3000.mat',\n",
       " 'A1944.mat',\n",
       " 'A1731.mat',\n",
       " 'A0525.mat',\n",
       " 'A6580.mat',\n",
       " 'A6355.mat',\n",
       " 'A4701.mat',\n",
       " 'A0825.mat',\n",
       " 'A0428.mat',\n",
       " 'A6125.mat',\n",
       " 'A2293.mat',\n",
       " 'A5035.mat',\n",
       " 'A0832.mat',\n",
       " 'A1444.mat',\n",
       " 'A3969.mat',\n",
       " 'A3905.mat',\n",
       " 'A6744.mat',\n",
       " 'A4283.mat',\n",
       " 'A1479.mat',\n",
       " 'A6049.mat',\n",
       " 'A3367.mat',\n",
       " 'A6634.mat',\n",
       " 'A3017.mat',\n",
       " 'A0386.mat',\n",
       " 'A5770.mat',\n",
       " 'A0901.mat',\n",
       " 'A2285.mat',\n",
       " 'A3938.mat',\n",
       " 'A4599.mat',\n",
       " 'A1777.mat',\n",
       " 'A3229.mat',\n",
       " 'A2624.mat',\n",
       " 'A6830.mat',\n",
       " 'A0966.mat',\n",
       " 'A3143.mat',\n",
       " 'A2381.mat',\n",
       " 'A6493.mat',\n",
       " 'A5265.mat',\n",
       " 'A1960.mat',\n",
       " 'A1228.mat',\n",
       " 'A0897.mat',\n",
       " 'A6486.mat',\n",
       " 'A4908.mat',\n",
       " 'A4767.mat',\n",
       " 'A2916.mat',\n",
       " 'A2668.mat',\n",
       " 'A4690.mat',\n",
       " 'A4209.mat',\n",
       " 'A6700.mat',\n",
       " 'A1544.mat',\n",
       " 'A4979.mat',\n",
       " 'A2109.mat',\n",
       " 'A4897.mat',\n",
       " 'A1065.mat',\n",
       " 'A5344.mat',\n",
       " 'A0106.mat',\n",
       " 'A2732.mat',\n",
       " 'A6473.mat',\n",
       " 'A0426.mat',\n",
       " 'A4387.mat',\n",
       " 'A0340.mat',\n",
       " 'A3300.mat',\n",
       " 'A5038.mat',\n",
       " 'A2295.mat',\n",
       " 'A5796.mat',\n",
       " 'A4449.mat',\n",
       " 'A3625.mat',\n",
       " 'A3411.mat',\n",
       " 'A1326.mat',\n",
       " 'A5948.mat',\n",
       " 'A0146.mat',\n",
       " 'A2790.mat',\n",
       " 'A0782.mat',\n",
       " 'A3731.mat',\n",
       " 'A3221.mat',\n",
       " 'A0978.mat',\n",
       " 'A2946.mat',\n",
       " 'A5574.mat',\n",
       " 'A4554.mat',\n",
       " 'A4577.mat',\n",
       " 'A2446.mat',\n",
       " 'A0505.mat',\n",
       " 'A2282.mat',\n",
       " 'A2306.mat',\n",
       " 'A1023.mat',\n",
       " 'A4672.mat',\n",
       " 'A4430.mat',\n",
       " 'A4695.mat',\n",
       " 'A0763.mat',\n",
       " 'A4867.mat',\n",
       " 'A4602.mat',\n",
       " 'A3916.mat',\n",
       " 'A3796.mat',\n",
       " 'A1081.mat',\n",
       " 'A5191.mat',\n",
       " 'A4255.mat',\n",
       " 'A1618.mat',\n",
       " 'A5572.mat',\n",
       " 'A6199.mat',\n",
       " 'A2272.mat',\n",
       " 'A3981.mat',\n",
       " 'A0298.mat',\n",
       " 'A3963.mat',\n",
       " 'A5539.mat',\n",
       " 'A1699.mat',\n",
       " 'A4157.mat',\n",
       " 'A4991.mat',\n",
       " 'A4239.mat',\n",
       " 'A1224.mat',\n",
       " 'A4390.mat',\n",
       " 'A5446.mat',\n",
       " 'A2118.mat',\n",
       " 'A5487.mat',\n",
       " 'A2788.mat',\n",
       " 'A3452.mat',\n",
       " 'A2711.mat',\n",
       " 'A2335.mat',\n",
       " 'A3678.mat',\n",
       " 'A3642.mat',\n",
       " 'A1708.mat',\n",
       " 'A0975.mat',\n",
       " 'A2461.mat',\n",
       " 'A1642.mat',\n",
       " 'A3607.mat',\n",
       " 'A0121.mat',\n",
       " 'A3968.mat',\n",
       " 'A4104.mat',\n",
       " 'A4101.mat',\n",
       " 'A5833.mat',\n",
       " 'A3203.mat',\n",
       " 'A4631.mat',\n",
       " 'A4048.mat',\n",
       " 'A0704.mat',\n",
       " 'A3371.mat',\n",
       " 'A6041.mat',\n",
       " 'A6656.mat',\n",
       " 'A0241.mat',\n",
       " 'A2601.mat',\n",
       " 'A2699.mat',\n",
       " 'A3855.mat',\n",
       " 'A4366.mat',\n",
       " 'A5390.mat',\n",
       " 'A1001.mat',\n",
       " 'A1461.mat',\n",
       " 'A0789.mat',\n",
       " 'A1207.mat',\n",
       " 'A1832.mat',\n",
       " 'A1034.mat',\n",
       " 'A6176.mat',\n",
       " 'A1798.mat',\n",
       " 'A6419.mat',\n",
       " 'A6471.mat',\n",
       " 'A0080.mat',\n",
       " 'A2198.mat',\n",
       " 'A1543.mat',\n",
       " 'A6711.mat',\n",
       " 'A1262.mat',\n",
       " 'A3811.mat',\n",
       " 'A5612.mat',\n",
       " 'A3758.mat',\n",
       " 'A3528.mat',\n",
       " 'A2133.mat',\n",
       " 'A1132.mat',\n",
       " 'A1209.mat',\n",
       " 'A2982.mat',\n",
       " 'A5206.mat',\n",
       " 'A5988.mat',\n",
       " 'A0965.mat',\n",
       " 'A5357.mat',\n",
       " 'A0859.mat',\n",
       " 'A2401.mat',\n",
       " 'A4642.mat',\n",
       " 'A5426.mat',\n",
       " 'A6315.mat',\n",
       " 'A6453.mat',\n",
       " 'A1564.mat',\n",
       " 'A3687.mat',\n",
       " 'A5116.mat',\n",
       " 'A1897.mat',\n",
       " 'A5240.mat',\n",
       " 'A2267.mat',\n",
       " 'A5705.mat',\n",
       " 'A2664.mat',\n",
       " 'A3571.mat',\n",
       " 'A3611.mat',\n",
       " 'A0862.mat',\n",
       " 'A4135.mat',\n",
       " 'A6535.mat',\n",
       " 'A5241.mat',\n",
       " 'A1096.mat',\n",
       " 'A3895.mat',\n",
       " 'A1372.mat',\n",
       " 'A5474.mat',\n",
       " 'A6042.mat',\n",
       " 'A0365.mat',\n",
       " 'A5858.mat',\n",
       " 'A1280.mat',\n",
       " 'A4415.mat',\n",
       " 'A2687.mat',\n",
       " 'A1595.mat',\n",
       " 'A0423.mat',\n",
       " 'A5956.mat',\n",
       " 'A4940.mat',\n",
       " 'A6031.mat',\n",
       " 'A4736.mat',\n",
       " 'A6279.mat',\n",
       " 'A4784.mat',\n",
       " 'A1141.mat',\n",
       " 'A4650.mat',\n",
       " 'A2445.mat',\n",
       " 'A2715.mat',\n",
       " 'A6029.mat',\n",
       " 'A5966.mat',\n",
       " 'A6554.mat',\n",
       " 'A1785.mat',\n",
       " 'A3467.mat',\n",
       " 'A5764.mat',\n",
       " 'A4334.mat',\n",
       " 'A3790.mat',\n",
       " 'A2411.mat',\n",
       " 'A6631.mat',\n",
       " 'A1334.mat',\n",
       " 'A4247.mat',\n",
       " 'A6435.mat',\n",
       " 'A1625.mat',\n",
       " 'A5920.mat',\n",
       " 'A5025.mat',\n",
       " 'A0584.mat',\n",
       " 'A1951.mat',\n",
       " 'A1597.mat',\n",
       " 'A6536.mat',\n",
       " 'A0465.mat',\n",
       " 'A3822.mat',\n",
       " 'A6630.mat',\n",
       " 'A5088.mat',\n",
       " 'A1214.mat',\n",
       " 'A5384.mat',\n",
       " 'A5618.mat',\n",
       " 'A0039.mat',\n",
       " 'A2358.mat',\n",
       " 'A5545.mat',\n",
       " 'A3891.mat',\n",
       " 'A1052.mat',\n",
       " 'A4713.mat',\n",
       " 'A3554.mat',\n",
       " 'A3751.mat',\n",
       " 'A3646.mat',\n",
       " 'A0707.mat',\n",
       " 'A2869.mat',\n",
       " 'A2179.mat',\n",
       " 'A3305.mat',\n",
       " 'A3140.mat',\n",
       " 'A3269.mat',\n",
       " 'A0193.mat',\n",
       " 'A1521.mat',\n",
       " 'A0035.mat',\n",
       " 'A2400.mat',\n",
       " 'A5295.mat',\n",
       " 'A3640.mat',\n",
       " 'A3595.mat',\n",
       " 'A3263.mat',\n",
       " 'A3540.mat',\n",
       " 'A5836.mat',\n",
       " 'A1982.mat',\n",
       " 'A4220.mat',\n",
       " 'A3172.mat',\n",
       " 'A6871.mat',\n",
       " 'A1149.mat',\n",
       " 'A5131.mat',\n",
       " 'A6225.mat',\n",
       " 'A0089.mat',\n",
       " 'A1767.mat',\n",
       " 'A3717.mat',\n",
       " 'A2364.mat',\n",
       " 'A3918.mat',\n",
       " 'A6717.mat',\n",
       " 'A1304.mat',\n",
       " 'A5600.mat',\n",
       " 'A0565.mat',\n",
       " 'A3992.mat',\n",
       " 'A3987.mat',\n",
       " 'A6280.mat',\n",
       " 'A3705.mat',\n",
       " 'A3477.mat',\n",
       " 'A6149.mat',\n",
       " 'A3210.mat',\n",
       " 'A3741.mat',\n",
       " 'A4294.mat',\n",
       " 'A0608.mat',\n",
       " 'A0215.mat',\n",
       " 'A6578.mat',\n",
       " 'A5995.mat',\n",
       " 'A0713.mat',\n",
       " 'A1934.mat',\n",
       " 'A2300.mat',\n",
       " 'A4725.mat',\n",
       " 'A6057.mat',\n",
       " 'A5073.mat',\n",
       " 'A4142.mat',\n",
       " 'A1161.mat',\n",
       " 'A0355.mat',\n",
       " 'A2213.mat',\n",
       " 'A6811.mat',\n",
       " 'A5168.mat',\n",
       " 'A2689.mat',\n",
       " 'A0094.mat',\n",
       " 'A5654.mat',\n",
       " 'A1952.mat',\n",
       " 'A0619.mat',\n",
       " 'A6075.mat',\n",
       " 'A0433.mat',\n",
       " 'A2719.mat',\n",
       " 'A1217.mat',\n",
       " 'A0576.mat',\n",
       " 'A1855.mat',\n",
       " 'A2857.mat',\n",
       " 'A5454.mat',\n",
       " 'A1074.mat',\n",
       " 'A2292.mat',\n",
       " 'A3293.mat',\n",
       " 'A0261.mat',\n",
       " 'A3767.mat',\n",
       " 'A3427.mat',\n",
       " 'A4144.mat',\n",
       " 'A1331.mat',\n",
       " 'A6760.mat',\n",
       " 'A5752.mat',\n",
       " 'A0294.mat',\n",
       " 'A1747.mat',\n",
       " 'A6017.mat',\n",
       " 'A6507.mat',\n",
       " 'A3127.mat',\n",
       " 'A1589.mat',\n",
       " 'A4943.mat',\n",
       " 'A2894.mat',\n",
       " 'A4100.mat',\n",
       " 'A0167.mat',\n",
       " 'A1316.mat',\n",
       " 'A4819.mat',\n",
       " 'A1002.mat',\n",
       " 'A1019.mat',\n",
       " 'A5666.mat',\n",
       " 'A2219.mat',\n",
       " 'A1669.mat',\n",
       " 'A0836.mat',\n",
       " 'A2761.mat',\n",
       " 'A2593.mat',\n",
       " 'A2043.mat',\n",
       " 'A6335.mat',\n",
       " 'A3435.mat',\n",
       " 'A3650.mat',\n",
       " 'A1739.mat',\n",
       " 'A4833.mat',\n",
       " 'A2408.mat',\n",
       " 'A3598.mat',\n",
       " 'A6652.mat',\n",
       " 'A4857.mat',\n",
       " 'A5352.mat',\n",
       " 'A3026.mat',\n",
       " 'A4282.mat',\n",
       " 'A3893.mat',\n",
       " 'A5456.mat',\n",
       " 'A0481.mat',\n",
       " 'A4911.mat',\n",
       " 'A2558.mat',\n",
       " 'A1626.mat',\n",
       " 'A3412.mat',\n",
       " 'A4705.mat',\n",
       " 'A5354.mat',\n",
       " 'A1670.mat',\n",
       " 'A4025.mat',\n",
       " 'A6201.mat',\n",
       " 'A3359.mat',\n",
       " 'A5378.mat',\n",
       " 'A1018.mat',\n",
       " 'A2359.mat',\n",
       " 'A4782.mat',\n",
       " 'A1901.mat',\n",
       " 'A2332.mat',\n",
       " 'A4434.mat',\n",
       " 'A1202.mat',\n",
       " 'A4433.mat',\n",
       " 'A0860.mat',\n",
       " 'A0415.mat',\n",
       " 'A0918.mat',\n",
       " 'A6178.mat',\n",
       " 'A3482.mat',\n",
       " 'A4890.mat',\n",
       " 'A0996.mat',\n",
       " 'A4102.mat',\n",
       " 'A5158.mat',\n",
       " 'A2093.mat',\n",
       " 'A4703.mat',\n",
       " 'A2867.mat',\n",
       " 'A1591.mat',\n",
       " 'A5582.mat',\n",
       " 'A3314.mat',\n",
       " 'A4182.mat',\n",
       " 'A1133.mat',\n",
       " 'A5533.mat',\n",
       " 'A2794.mat',\n",
       " 'A4028.mat',\n",
       " 'A5273.mat',\n",
       " 'A4794.mat',\n",
       " 'A2588.mat',\n",
       " 'A3958.mat',\n",
       " 'A5516.mat',\n",
       " 'A2048.mat',\n",
       " 'A2681.mat',\n",
       " 'A2925.mat',\n",
       " 'A6035.mat',\n",
       " 'A1484.mat',\n",
       " 'A1652.mat',\n",
       " 'A2553.mat',\n",
       " 'A2634.mat',\n",
       " 'A0771.mat',\n",
       " 'A1433.mat',\n",
       " 'A1694.mat',\n",
       " 'A1839.mat',\n",
       " 'A5182.mat',\n",
       " 'A2011.mat',\n",
       " 'A0469.mat',\n",
       " 'A1116.mat',\n",
       " 'A0700.mat',\n",
       " 'A3783.mat',\n",
       " 'A5055.mat',\n",
       " 'A2745.mat',\n",
       " 'A6214.mat',\n",
       " 'A5112.mat',\n",
       " 'A4902.mat',\n",
       " 'A6707.mat',\n",
       " 'A3402.mat',\n",
       " 'A5363.mat',\n",
       " 'A1229.mat',\n",
       " 'A0238.mat',\n",
       " 'A3695.mat',\n",
       " 'A2145.mat',\n",
       " 'A4154.mat',\n",
       " 'A6785.mat',\n",
       " 'A1294.mat',\n",
       " 'A0677.mat',\n",
       " 'A3689.mat',\n",
       " 'A2150.mat',\n",
       " 'A3564.mat',\n",
       " 'A0537.mat',\n",
       " 'A0405.mat',\n",
       " 'A4364.mat',\n",
       " 'A4511.mat',\n",
       " 'A6338.mat',\n",
       " 'A4192.mat',\n",
       " 'A1883.mat',\n",
       " 'A6278.mat',\n",
       " 'A0561.mat',\n",
       " 'A4861.mat',\n",
       " 'A0811.mat',\n",
       " 'A2091.mat',\n",
       " 'A2388.mat',\n",
       " 'A1126.mat',\n",
       " 'A5757.mat',\n",
       " 'A5248.mat',\n",
       " 'A3509.mat',\n",
       " 'A0451.mat',\n",
       " 'A1919.mat',\n",
       " 'A0815.mat',\n",
       " 'A4836.mat',\n",
       " 'A1054.mat',\n",
       " 'A1559.mat',\n",
       " 'A3130.mat',\n",
       " 'A4208.mat',\n",
       " 'A2251.mat',\n",
       " 'A0233.mat',\n",
       " 'A5089.mat',\n",
       " 'A4698.mat',\n",
       " 'A1611.mat',\n",
       " 'A6637.mat',\n",
       " 'A1220.mat',\n",
       " 'A4831.mat',\n",
       " 'A0794.mat',\n",
       " 'A5268.mat',\n",
       " 'A5367.mat',\n",
       " 'A3071.mat',\n",
       " 'A0003.mat',\n",
       " 'A3348.mat',\n",
       " 'A2622.mat',\n",
       " 'A0801.mat',\n",
       " 'A4976.mat',\n",
       " 'A1605.mat',\n",
       " 'A5417.mat',\n",
       " 'A1440.mat',\n",
       " 'A3618.mat',\n",
       " 'A5536.mat',\n",
       " 'A4183.mat',\n",
       " 'A1975.mat',\n",
       " 'A2491.mat',\n",
       " 'A6084.mat',\n",
       " 'A6001.mat',\n",
       " 'A5800.mat',\n",
       " 'A5198.mat',\n",
       " 'A2972.mat',\n",
       " 'A2544.mat',\n",
       " 'A1971.mat',\n",
       " 'A5445.mat',\n",
       " 'A4809.mat',\n",
       " 'A2727.mat',\n",
       " 'A1906.mat',\n",
       " 'A1249.mat',\n",
       " 'A6191.mat',\n",
       " 'A0295.mat',\n",
       " 'A5161.mat',\n",
       " 'A4305.mat',\n",
       " 'A4885.mat',\n",
       " 'A1234.mat',\n",
       " 'A4376.mat',\n",
       " 'A2280.mat',\n",
       " 'A3769.mat',\n",
       " 'A1653.mat',\n",
       " 'A1875.mat',\n",
       " 'A0979.mat',\n",
       " 'A3998.mat',\n",
       " 'A1222.mat',\n",
       " 'A5353.mat',\n",
       " 'A5515.mat',\n",
       " 'A2029.mat',\n",
       " 'A3231.mat',\n",
       " 'A2023.mat',\n",
       " 'A6614.mat',\n",
       " 'A6761.mat',\n",
       " 'A6500.mat',\n",
       " 'A3261.mat',\n",
       " 'A2052.mat',\n",
       " 'A0411.mat',\n",
       " 'A3864.mat',\n",
       " 'A0935.mat',\n",
       " 'A5980.mat',\n",
       " 'A2578.mat',\n",
       " 'A4278.mat',\n",
       " 'A1471.mat',\n",
       " 'A6239.mat',\n",
       " 'A1170.mat',\n",
       " 'A0196.mat',\n",
       " 'A5755.mat',\n",
       " 'A0130.mat',\n",
       " 'A5639.mat',\n",
       " 'A2529.mat',\n",
       " 'A5603.mat',\n",
       " 'A0435.mat',\n",
       " 'A0950.mat',\n",
       " 'A4012.mat',\n",
       " 'A6408.mat',\n",
       " 'A1880.mat',\n",
       " 'A0013.mat',\n",
       " 'A0513.mat',\n",
       " 'A5728.mat',\n",
       " 'A1257.mat',\n",
       " 'A2505.mat',\n",
       " 'A5427.mat',\n",
       " 'A6047.mat',\n",
       " 'A3462.mat',\n",
       " 'A6658.mat',\n",
       " 'A6749.mat',\n",
       " 'A0040.mat',\n",
       " 'A1957.mat',\n",
       " 'A4655.mat',\n",
       " 'A5024.mat',\n",
       " 'A3844.mat',\n",
       " 'A3174.mat',\n",
       " 'A1053.mat',\n",
       " 'A5551.mat',\n",
       " 'A5892.mat',\n",
       " 'A2783.mat',\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train # starts with A2355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A5949.mat',\n",
       " 'A0268.mat',\n",
       " 'A0938.mat',\n",
       " 'A0379.mat',\n",
       " 'A6235.mat',\n",
       " 'A5085.mat',\n",
       " 'A0368.mat',\n",
       " 'A4547.mat',\n",
       " 'A6477.mat',\n",
       " 'A3075.mat',\n",
       " 'A1011.mat',\n",
       " 'A0821.mat',\n",
       " 'A6566.mat',\n",
       " 'A0688.mat',\n",
       " 'A3682.mat',\n",
       " 'A5270.mat',\n",
       " 'A6875.mat',\n",
       " 'A0698.mat',\n",
       " 'A1369.mat',\n",
       " 'A1633.mat',\n",
       " 'A3438.mat',\n",
       " 'A6648.mat',\n",
       " 'A0988.mat',\n",
       " 'A2271.mat',\n",
       " 'A4479.mat',\n",
       " 'A0499.mat',\n",
       " 'A5788.mat',\n",
       " 'A5377.mat',\n",
       " 'A4074.mat',\n",
       " 'A3279.mat',\n",
       " 'A0944.mat',\n",
       " 'A1244.mat',\n",
       " 'A4372.mat',\n",
       " 'A3159.mat',\n",
       " 'A5350.mat',\n",
       " 'A1703.mat',\n",
       " 'A5599.mat',\n",
       " 'A5319.mat',\n",
       " 'A3609.mat',\n",
       " 'A0018.mat',\n",
       " 'A4472.mat',\n",
       " 'A0262.mat',\n",
       " 'A3497.mat',\n",
       " 'A4132.mat',\n",
       " 'A2589.mat',\n",
       " 'A0033.mat',\n",
       " 'A6427.mat',\n",
       " 'A3516.mat',\n",
       " 'A3639.mat',\n",
       " 'A5699.mat',\n",
       " 'A6117.mat',\n",
       " 'A2881.mat',\n",
       " 'A1386.mat',\n",
       " 'A0592.mat',\n",
       " 'A5711.mat',\n",
       " 'A4696.mat',\n",
       " 'A3192.mat',\n",
       " 'A2427.mat',\n",
       " 'A5336.mat',\n",
       " 'A1233.mat',\n",
       " 'A3805.mat',\n",
       " 'A1748.mat',\n",
       " 'A0407.mat',\n",
       " 'A4002.mat',\n",
       " 'A1318.mat',\n",
       " 'A3105.mat',\n",
       " 'A3764.mat',\n",
       " 'A4894.mat',\n",
       " 'A5889.mat',\n",
       " 'A0225.mat',\n",
       " 'A3407.mat',\n",
       " 'A2347.mat',\n",
       " 'A5465.mat',\n",
       " 'A3875.mat',\n",
       " 'A5167.mat',\n",
       " 'A1366.mat',\n",
       " 'A0049.mat',\n",
       " 'A0113.mat',\n",
       " 'A6123.mat',\n",
       " 'A3566.mat',\n",
       " 'A0701.mat',\n",
       " 'A6303.mat',\n",
       " 'A3115.mat',\n",
       " 'A5031.mat',\n",
       " 'A2490.mat',\n",
       " 'A4606.mat',\n",
       " 'A0081.mat',\n",
       " 'A2835.mat',\n",
       " 'A2607.mat',\n",
       " 'A6738.mat',\n",
       " 'A0974.mat',\n",
       " 'A6260.mat',\n",
       " 'A0302.mat',\n",
       " 'A2296.mat',\n",
       " 'A5725.mat',\n",
       " 'A5072.mat',\n",
       " 'A1073.mat',\n",
       " 'A1976.mat',\n",
       " 'A0107.mat',\n",
       " 'A0464.mat',\n",
       " 'A4285.mat',\n",
       " 'A6208.mat',\n",
       " 'A2968.mat',\n",
       " 'A2649.mat',\n",
       " 'A5461.mat',\n",
       " 'A6360.mat',\n",
       " 'A6387.mat',\n",
       " 'A6320.mat',\n",
       " 'A4558.mat',\n",
       " 'A6367.mat',\n",
       " 'A2159.mat',\n",
       " 'A2873.mat',\n",
       " 'A4109.mat',\n",
       " 'A2846.mat',\n",
       " 'A5883.mat',\n",
       " 'A2360.mat',\n",
       " 'A4541.mat',\n",
       " 'A5606.mat',\n",
       " 'A6437.mat',\n",
       " 'A1273.mat',\n",
       " 'A5234.mat',\n",
       " 'A1254.mat',\n",
       " 'A6109.mat',\n",
       " 'A5438.mat',\n",
       " 'A4540.mat',\n",
       " 'A0848.mat',\n",
       " 'A1024.mat',\n",
       " 'A0346.mat',\n",
       " 'A1637.mat',\n",
       " 'A0398.mat',\n",
       " 'A0640.mat',\n",
       " 'A5425.mat',\n",
       " 'A0503.mat',\n",
       " 'A5601.mat',\n",
       " 'A1891.mat',\n",
       " 'A2433.mat',\n",
       " 'A6454.mat',\n",
       " 'A5348.mat',\n",
       " 'A5713.mat',\n",
       " 'A5550.mat',\n",
       " 'A4206.mat',\n",
       " 'A1134.mat',\n",
       " 'A4301.mat',\n",
       " 'A3984.mat',\n",
       " 'A3536.mat',\n",
       " 'A0780.mat',\n",
       " 'A6851.mat',\n",
       " 'A3042.mat',\n",
       " 'A4769.mat',\n",
       " 'A1084.mat',\n",
       " 'A4092.mat',\n",
       " 'A2399.mat',\n",
       " 'A5156.mat',\n",
       " 'A6015.mat',\n",
       " 'A6291.mat',\n",
       " 'A6618.mat',\n",
       " 'A2064.mat',\n",
       " 'A0847.mat',\n",
       " 'A5022.mat',\n",
       " 'A1497.mat',\n",
       " 'A2559.mat',\n",
       " 'A5186.mat',\n",
       " 'A5478.mat',\n",
       " 'A5884.mat',\n",
       " 'A4942.mat',\n",
       " 'A0129.mat',\n",
       " 'A0647.mat',\n",
       " 'A1928.mat',\n",
       " 'A2442.mat',\n",
       " 'A5615.mat',\n",
       " 'A0114.mat',\n",
       " 'A4934.mat',\n",
       " 'A3713.mat',\n",
       " 'A6009.mat',\n",
       " 'A3532.mat',\n",
       " 'A4067.mat',\n",
       " 'A3246.mat',\n",
       " 'A1501.mat',\n",
       " 'A3943.mat',\n",
       " 'A1973.mat',\n",
       " 'A4965.mat',\n",
       " 'A6193.mat',\n",
       " 'A5685.mat',\n",
       " 'A3207.mat',\n",
       " 'A1357.mat',\n",
       " 'A2577.mat',\n",
       " 'A2234.mat',\n",
       " 'A2365.mat',\n",
       " 'A5017.mat',\n",
       " 'A4250.mat',\n",
       " 'A1757.mat',\n",
       " 'A5271.mat',\n",
       " 'A1660.mat',\n",
       " 'A5392.mat',\n",
       " 'A3651.mat',\n",
       " 'A4330.mat',\n",
       " 'A4663.mat',\n",
       " 'A4674.mat',\n",
       " 'A4556.mat',\n",
       " 'A3480.mat',\n",
       " 'A1258.mat',\n",
       " 'A2800.mat',\n",
       " 'A1719.mat',\n",
       " 'A5899.mat',\n",
       " 'A1111.mat',\n",
       " 'A1033.mat',\n",
       " 'A1451.mat',\n",
       " 'A5867.mat',\n",
       " 'A3659.mat',\n",
       " 'A1862.mat',\n",
       " 'A1803.mat',\n",
       " 'A1707.mat',\n",
       " 'A5279.mat',\n",
       " 'A2707.mat',\n",
       " 'A2927.mat',\n",
       " 'A6386.mat',\n",
       " 'A0272.mat',\n",
       " 'A2923.mat',\n",
       " 'A3744.mat',\n",
       " 'A3199.mat',\n",
       " 'A3660.mat',\n",
       " 'A5464.mat',\n",
       " 'A3515.mat',\n",
       " 'A3738.mat',\n",
       " 'A3031.mat',\n",
       " 'A4218.mat',\n",
       " 'A4174.mat',\n",
       " 'A0255.mat',\n",
       " 'A6491.mat',\n",
       " 'A4361.mat',\n",
       " 'A5522.mat',\n",
       " 'A6565.mat',\n",
       " 'A4010.mat',\n",
       " 'A0984.mat',\n",
       " 'A3377.mat',\n",
       " 'A0633.mat',\n",
       " 'A5758.mat',\n",
       " 'A1587.mat',\n",
       " 'A5990.mat',\n",
       " 'A0531.mat',\n",
       " 'A0155.mat',\n",
       " 'A1154.mat',\n",
       " 'A2368.mat',\n",
       " 'A0973.mat',\n",
       " 'A4353.mat',\n",
       " 'A5771.mat',\n",
       " 'A2214.mat',\n",
       " 'A2831.mat',\n",
       " 'A6053.mat',\n",
       " 'A5631.mat',\n",
       " 'A2468.mat',\n",
       " 'A2014.mat',\n",
       " 'A1317.mat',\n",
       " 'A6139.mat',\n",
       " 'A5569.mat',\n",
       " 'A5517.mat',\n",
       " 'A1942.mat',\n",
       " 'A1512.mat',\n",
       " 'A0471.mat',\n",
       " 'A4394.mat',\n",
       " 'A3578.mat',\n",
       " 'A6157.mat',\n",
       " 'A6090.mat',\n",
       " 'A6571.mat',\n",
       " 'A6282.mat',\n",
       " 'A3152.mat',\n",
       " 'A3400.mat',\n",
       " 'A5964.mat',\n",
       " 'A2130.mat',\n",
       " 'A5300.mat',\n",
       " 'A5505.mat',\n",
       " 'A2315.mat',\n",
       " 'A6679.mat',\n",
       " 'A6852.mat',\n",
       " 'A5701.mat',\n",
       " 'A2985.mat']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val # starts with A5949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A0349.mat',\n",
       " 'A3947.mat',\n",
       " 'A3900.mat',\n",
       " 'A0817.mat',\n",
       " 'A2140.mat',\n",
       " 'A6793.mat',\n",
       " 'A1879.mat',\n",
       " 'A0900.mat',\n",
       " 'A0454.mat',\n",
       " 'A6855.mat',\n",
       " 'A1142.mat',\n",
       " 'A2316.mat',\n",
       " 'A1701.mat',\n",
       " 'A0892.mat',\n",
       " 'A4156.mat',\n",
       " 'A6654.mat',\n",
       " 'A1902.mat',\n",
       " 'A3887.mat',\n",
       " 'A4328.mat',\n",
       " 'A6705.mat',\n",
       " 'A2393.mat',\n",
       " 'A1654.mat',\n",
       " 'A4534.mat',\n",
       " 'A4223.mat',\n",
       " 'A6232.mat',\n",
       " 'A5215.mat',\n",
       " 'A6361.mat',\n",
       " 'A2060.mat',\n",
       " 'A5460.mat',\n",
       " 'A4804.mat',\n",
       " 'A5066.mat',\n",
       " 'A6283.mat',\n",
       " 'A3889.mat',\n",
       " 'A0621.mat',\n",
       " 'A5856.mat',\n",
       " 'A0943.mat',\n",
       " 'A1936.mat',\n",
       " 'A3808.mat',\n",
       " 'A5473.mat',\n",
       " 'A6224.mat',\n",
       " 'A6848.mat',\n",
       " 'A1420.mat',\n",
       " 'A3335.mat',\n",
       " 'A3258.mat',\n",
       " 'A1623.mat',\n",
       " 'A4062.mat',\n",
       " 'A5721.mat',\n",
       " 'A5733.mat',\n",
       " 'A1238.mat',\n",
       " 'A4654.mat',\n",
       " 'A4504.mat',\n",
       " 'A0987.mat',\n",
       " 'A2839.mat',\n",
       " 'A4243.mat',\n",
       " 'A5778.mat',\n",
       " 'A6173.mat',\n",
       " 'A1782.mat',\n",
       " 'A5404.mat',\n",
       " 'A1443.mat',\n",
       " 'A3470.mat',\n",
       " 'A6876.mat',\n",
       " 'A1230.mat',\n",
       " 'A1870.mat',\n",
       " 'A1186.mat',\n",
       " 'A0736.mat',\n",
       " 'A4685.mat',\n",
       " 'A4562.mat',\n",
       " 'A4444.mat',\n",
       " 'A1568.mat',\n",
       " 'A3403.mat',\n",
       " 'A6465.mat',\n",
       " 'A0394.mat',\n",
       " 'A5871.mat',\n",
       " 'A4337.mat',\n",
       " 'A1373.mat',\n",
       " 'A0070.mat',\n",
       " 'A6119.mat',\n",
       " 'A0357.mat',\n",
       " 'A5125.mat',\n",
       " 'A4207.mat',\n",
       " 'A0589.mat',\n",
       " 'A6206.mat',\n",
       " 'A2661.mat',\n",
       " 'A0067.mat',\n",
       " 'A1720.mat',\n",
       " 'A0099.mat',\n",
       " 'A6102.mat',\n",
       " 'A4491.mat',\n",
       " 'A6297.mat',\n",
       " 'A1995.mat',\n",
       " 'A6764.mat',\n",
       " 'A5468.mat',\n",
       " 'A1848.mat',\n",
       " 'A5475.mat',\n",
       " 'A2865.mat',\n",
       " 'A1152.mat',\n",
       " 'A4520.mat',\n",
       " 'A5146.mat',\n",
       " 'A6845.mat',\n",
       " 'A6650.mat',\n",
       " 'A1263.mat',\n",
       " 'A5900.mat',\n",
       " 'A2369.mat',\n",
       " 'A4121.mat',\n",
       " 'A6720.mat',\n",
       " 'A5841.mat',\n",
       " 'A3953.mat',\n",
       " 'A0661.mat',\n",
       " 'A1674.mat',\n",
       " 'A4106.mat',\n",
       " 'A0351.mat',\n",
       " 'A2701.mat',\n",
       " 'A2383.mat',\n",
       " 'A6135.mat',\n",
       " 'A5118.mat',\n",
       " 'A0772.mat',\n",
       " 'A4303.mat',\n",
       " 'A3241.mat',\n",
       " 'A0297.mat',\n",
       " 'A4777.mat',\n",
       " 'A2963.mat',\n",
       " 'A1395.mat',\n",
       " 'A2960.mat',\n",
       " 'A1620.mat',\n",
       " 'A5459.mat',\n",
       " 'A0041.mat',\n",
       " 'A0292.mat',\n",
       " 'A5895.mat',\n",
       " 'A6290.mat',\n",
       " 'A4226.mat',\n",
       " 'A5278.mat',\n",
       " 'A1513.mat',\n",
       " 'A4723.mat',\n",
       " 'A4308.mat',\n",
       " 'A1051.mat',\n",
       " 'A3344.mat',\n",
       " 'A6174.mat',\n",
       " 'A5595.mat',\n",
       " 'A5959.mat',\n",
       " 'A1038.mat',\n",
       " 'A0628.mat',\n",
       " 'A3222.mat',\n",
       " 'A1506.mat',\n",
       " 'A5700.mat',\n",
       " 'A2585.mat',\n",
       " 'A1662.mat',\n",
       " 'A1449.mat',\n",
       " 'A5808.mat',\n",
       " 'A4501.mat',\n",
       " 'A6671.mat',\n",
       " 'A5095.mat',\n",
       " 'A3620.mat',\n",
       " 'A6805.mat',\n",
       " 'A6194.mat',\n",
       " 'A1397.mat',\n",
       " 'A6808.mat',\n",
       " 'A1474.mat',\n",
       " 'A6840.mat',\n",
       " 'A2194.mat',\n",
       " 'A2612.mat',\n",
       " 'A4429.mat',\n",
       " 'A1120.mat',\n",
       " 'A0947.mat',\n",
       " 'A1245.mat',\n",
       " 'A1000.mat',\n",
       " 'A3849.mat',\n",
       " 'A4846.mat',\n",
       " 'A0679.mat',\n",
       " 'A6340.mat',\n",
       " 'A6467.mat',\n",
       " 'A0805.mat',\n",
       " 'A5857.mat',\n",
       " 'A4733.mat',\n",
       " 'A2379.mat',\n",
       " 'A4918.mat',\n",
       " 'A2663.mat',\n",
       " 'A5734.mat',\n",
       " 'A2330.mat',\n",
       " 'A4634.mat',\n",
       " 'A0197.mat',\n",
       " 'A2415.mat',\n",
       " 'A0699.mat',\n",
       " 'A3259.mat',\n",
       " 'A3737.mat',\n",
       " 'A3772.mat',\n",
       " 'A5301.mat',\n",
       " 'A5208.mat',\n",
       " 'A1526.mat',\n",
       " 'A6160.mat',\n",
       " 'A0248.mat',\n",
       " 'A6011.mat',\n",
       " 'A4397.mat',\n",
       " 'A3599.mat',\n",
       " 'A3399.mat',\n",
       " 'A0891.mat',\n",
       " 'A3385.mat',\n",
       " 'A0438.mat',\n",
       " 'A2658.mat',\n",
       " 'A2749.mat',\n",
       " 'A5149.mat',\n",
       " 'A2425.mat',\n",
       " 'A2004.mat',\n",
       " 'A3248.mat',\n",
       " 'A2704.mat',\n",
       " 'A6551.mat',\n",
       " 'A4019.mat',\n",
       " 'A1920.mat',\n",
       " 'A3327.mat',\n",
       " 'A5730.mat',\n",
       " 'A5549.mat',\n",
       " 'A0595.mat',\n",
       " 'A4898.mat',\n",
       " 'A5019.mat',\n",
       " 'A1472.mat',\n",
       " 'A2900.mat',\n",
       " 'A1738.mat',\n",
       " 'A0488.mat',\n",
       " 'A3519.mat',\n",
       " 'A3038.mat',\n",
       " 'A6567.mat',\n",
       " 'A4173.mat',\n",
       " 'A5294.mat',\n",
       " 'A0906.mat',\n",
       " 'A0941.mat',\n",
       " 'A3356.mat',\n",
       " 'A5496.mat',\n",
       " 'A0133.mat',\n",
       " 'A6697.mat',\n",
       " 'A5586.mat',\n",
       " 'A2519.mat',\n",
       " 'A4785.mat',\n",
       " 'A5996.mat',\n",
       " 'A1106.mat',\n",
       " 'A0844.mat',\n",
       " 'A4964.mat',\n",
       " 'A4000.mat',\n",
       " 'A5048.mat',\n",
       " 'A1826.mat',\n",
       " 'A5593.mat',\n",
       " 'A3573.mat',\n",
       " 'A4123.mat',\n",
       " 'A5166.mat',\n",
       " 'A2318.mat',\n",
       " 'A3752.mat',\n",
       " 'A3311.mat',\n",
       " 'A5926.mat',\n",
       " 'A3113.mat',\n",
       " 'A3313.mat',\n",
       " 'A1272.mat',\n",
       " 'A5660.mat',\n",
       " 'A0396.mat',\n",
       " 'A3951.mat',\n",
       " 'A5637.mat',\n",
       " 'A0634.mat',\n",
       " 'A3533.mat',\n",
       " 'A4168.mat',\n",
       " 'A5129.mat',\n",
       " 'A2815.mat',\n",
       " 'A1915.mat',\n",
       " 'A2374.mat',\n",
       " 'A6184.mat',\n",
       " 'A6598.mat',\n",
       " 'A2511.mat',\n",
       " 'A1752.mat',\n",
       " 'A0123.mat',\n",
       " 'A4298.mat',\n",
       " 'A1755.mat',\n",
       " 'A1963.mat',\n",
       " 'A6432.mat',\n",
       " 'A5668.mat',\n",
       " 'A3101.mat',\n",
       " 'A1310.mat',\n",
       " 'A6706.mat',\n",
       " 'A0779.mat',\n",
       " 'A5057.mat',\n",
       " 'A3569.mat',\n",
       " 'A2933.mat',\n",
       " 'A0770.mat',\n",
       " 'A4704.mat',\n",
       " 'A2501.mat',\n",
       " 'A5636.mat',\n",
       " 'A1781.mat',\n",
       " 'A3434.mat',\n",
       " 'A5640.mat',\n",
       " 'A0452.mat',\n",
       " 'A5801.mat',\n",
       " 'A2184.mat',\n",
       " 'A4164.mat',\n",
       " 'A4633.mat',\n",
       " 'A5184.mat',\n",
       " 'A3338.mat',\n",
       " 'A2868.mat',\n",
       " 'A6719.mat',\n",
       " 'A2402.mat',\n",
       " 'A0946.mat',\n",
       " 'A3816.mat',\n",
       " 'A2667.mat',\n",
       " 'A2290.mat',\n",
       " 'A5011.mat',\n",
       " 'A2568.mat',\n",
       " 'A3894.mat',\n",
       " 'A5063.mat',\n",
       " 'A4751.mat',\n",
       " 'A0284.mat',\n",
       " 'A4055.mat',\n",
       " 'A6156.mat',\n",
       " 'A1551.mat',\n",
       " 'A6478.mat',\n",
       " 'A5414.mat',\n",
       " 'A2860.mat',\n",
       " 'A0425.mat',\n",
       " 'A2682.mat',\n",
       " 'A0716.mat',\n",
       " 'A3262.mat',\n",
       " 'A1507.mat',\n",
       " 'A2069.mat',\n",
       " 'A4380.mat',\n",
       " 'A4636.mat',\n",
       " 'A0287.mat',\n",
       " 'A0266.mat',\n",
       " 'A0338.mat',\n",
       " 'A2090.mat',\n",
       " 'A3027.mat',\n",
       " 'A4280.mat',\n",
       " 'A0735.mat',\n",
       " 'A1500.mat',\n",
       " 'A0308.mat',\n",
       " 'A0373.mat',\n",
       " 'A5525.mat',\n",
       " 'A4322.mat',\n",
       " 'A0807.mat',\n",
       " 'A5084.mat',\n",
       " 'A4315.mat',\n",
       " 'A2772.mat',\n",
       " 'A3456.mat',\n",
       " 'A2991.mat',\n",
       " 'A1276.mat',\n",
       " 'A2542.mat',\n",
       " 'A0676.mat',\n",
       " 'A3415.mat',\n",
       " 'A4779.mat',\n",
       " 'A2885.mat',\n",
       " 'A5821.mat',\n",
       " 'A6069.mat',\n",
       " 'A5818.mat',\n",
       " 'A1144.mat',\n",
       " 'A0362.mat',\n",
       " 'A1560.mat',\n",
       " 'A5682.mat',\n",
       " 'A5643.mat',\n",
       " 'A1156.mat',\n",
       " 'A5226.mat',\n",
       " 'A3908.mat',\n",
       " 'A5484.mat',\n",
       " 'A5781.mat',\n",
       " 'A6129.mat',\n",
       " 'A3641.mat',\n",
       " 'A5050.mat',\n",
       " 'A4900.mat',\n",
       " 'A0174.mat',\n",
       " 'A0910.mat',\n",
       " 'A4715.mat',\n",
       " 'A4113.mat',\n",
       " 'A5976.mat',\n",
       " 'A0228.mat',\n",
       " 'A1864.mat',\n",
       " 'A6051.mat',\n",
       " 'A2361.mat',\n",
       " 'A1010.mat',\n",
       " 'A0863.mat',\n",
       " 'A5457.mat',\n",
       " 'A1109.mat',\n",
       " 'A3173.mat',\n",
       " 'A0258.mat',\n",
       " 'A3624.mat',\n",
       " 'A1894.mat',\n",
       " 'A0869.mat',\n",
       " 'A5006.mat',\n",
       " 'A3050.mat',\n",
       " 'A0458.mat',\n",
       " 'A2089.mat',\n",
       " 'A1578.mat',\n",
       " 'A2975.mat',\n",
       " 'A5671.mat',\n",
       " 'A1340.mat',\n",
       " 'A5571.mat',\n",
       " 'A0210.mat',\n",
       " 'A5299.mat',\n",
       " 'A1007.mat',\n",
       " 'A0563.mat',\n",
       " 'A4462.mat',\n",
       " 'A1095.mat',\n",
       " 'A1241.mat',\n",
       " 'A3315.mat',\n",
       " 'A5558.mat',\n",
       " 'A3334.mat',\n",
       " 'A4702.mat',\n",
       " 'A3727.mat',\n",
       " 'A1047.mat',\n",
       " 'A3994.mat',\n",
       " 'A3906.mat',\n",
       " 'A1692.mat',\n",
       " 'A1379.mat',\n",
       " 'A0662.mat',\n",
       " 'A4821.mat',\n",
       " 'A0702.mat',\n",
       " 'A3019.mat',\n",
       " 'A3352.mat',\n",
       " 'A1364.mat',\n",
       " 'A4571.mat',\n",
       " 'A4797.mat',\n",
       " 'A0911.mat',\n",
       " 'A3676.mat',\n",
       " 'A2540.mat',\n",
       " 'A1143.mat',\n",
       " 'A5180.mat',\n",
       " 'A1572.mat',\n",
       " 'A3374.mat',\n",
       " 'A5611.mat',\n",
       " 'A3100.mat',\n",
       " 'A0731.mat',\n",
       " 'A3859.mat',\n",
       " 'A6008.mat',\n",
       " 'A1050.mat',\n",
       " 'A2122.mat',\n",
       " 'A4414.mat',\n",
       " 'A3066.mat',\n",
       " 'A2961.mat',\n",
       " 'A0404.mat',\n",
       " 'A5814.mat',\n",
       " 'A4791.mat',\n",
       " 'A3520.mat',\n",
       " 'A4665.mat',\n",
       " 'A2049.mat',\n",
       " 'A3791.mat',\n",
       " 'A2819.mat',\n",
       " 'A0397.mat',\n",
       " 'A6060.mat',\n",
       " 'A4670.mat',\n",
       " 'A1196.mat',\n",
       " 'A1510.mat',\n",
       " 'A6244.mat',\n",
       " 'A3917.mat',\n",
       " 'A6620.mat',\n",
       " 'A2027.mat',\n",
       " 'A5009.mat',\n",
       " 'A1505.mat',\n",
       " 'A5406.mat',\n",
       " 'A6093.mat',\n",
       " 'A5054.mat',\n",
       " 'A3301.mat',\n",
       " 'A3724.mat',\n",
       " 'A6266.mat',\n",
       " 'A6134.mat',\n",
       " 'A3185.mat',\n",
       " 'A4176.mat',\n",
       " 'A4649.mat',\n",
       " 'A4269.mat',\n",
       " 'A0750.mat',\n",
       " 'A6181.mat',\n",
       " 'A3819.mat',\n",
       " 'A3287.mat',\n",
       " 'A2182.mat',\n",
       " 'A1408.mat',\n",
       " 'A3213.mat',\n",
       " 'A3490.mat',\n",
       " 'A2920.mat',\n",
       " 'A4627.mat',\n",
       " 'A2343.mat',\n",
       " 'A3647.mat',\n",
       " 'A4917.mat',\n",
       " 'A1066.mat',\n",
       " 'A0494.mat',\n",
       " 'A4266.mat',\n",
       " 'A5283.mat',\n",
       " 'A3912.mat',\n",
       " 'A4763.mat',\n",
       " 'A1523.mat',\n",
       " 'A4768.mat',\n",
       " 'A4302.mat',\n",
       " 'A0882.mat',\n",
       " 'A2764.mat',\n",
       " 'A4475.mat',\n",
       " 'A3696.mat',\n",
       " 'A4059.mat',\n",
       " 'A4063.mat',\n",
       " 'A0795.mat',\n",
       " 'A3638.mat',\n",
       " 'A1118.mat',\n",
       " 'A0188.mat',\n",
       " 'A5622.mat',\n",
       " 'A3862.mat',\n",
       " 'A6103.mat',\n",
       " 'A3260.mat',\n",
       " 'A0818.mat',\n",
       " 'A5588.mat',\n",
       " 'A5791.mat',\n",
       " 'A0968.mat',\n",
       " 'A6568.mat',\n",
       " 'A3396.mat',\n",
       " 'A5287.mat',\n",
       " 'A5835.mat',\n",
       " 'A2189.mat',\n",
       " 'A5389.mat',\n",
       " 'A4455.mat',\n",
       " 'A4673.mat',\n",
       " 'A5532.mat',\n",
       " 'A5542.mat',\n",
       " 'A3296.mat',\n",
       " 'A3129.mat',\n",
       " 'A2136.mat',\n",
       " 'A4684.mat',\n",
       " 'A4146.mat',\n",
       " 'A5961.mat',\n",
       " 'A0669.mat',\n",
       " 'A1093.mat',\n",
       " 'A1431.mat',\n",
       " 'A6680.mat',\n",
       " 'A1164.mat',\n",
       " 'A3388.mat',\n",
       " 'A5720.mat',\n",
       " 'A2470.mat',\n",
       " 'A4866.mat',\n",
       " 'A3059.mat',\n",
       " 'A5953.mat',\n",
       " 'A6416.mat',\n",
       " 'A4148.mat',\n",
       " 'A5753.mat',\n",
       " 'A3634.mat',\n",
       " 'A1704.mat',\n",
       " 'A1834.mat',\n",
       " 'A5411.mat',\n",
       " 'A1496.mat',\n",
       " 'A6330.mat',\n",
       " 'A6553.mat',\n",
       " 'A0095.mat',\n",
       " 'A1260.mat',\n",
       " 'A2774.mat',\n",
       " 'A3637.mat',\n",
       " 'A3586.mat',\n",
       " 'A0036.mat',\n",
       " 'A0037.mat',\n",
       " 'A1436.mat',\n",
       " 'A4815.mat',\n",
       " 'A0709.mat',\n",
       " 'A3068.mat',\n",
       " 'A5817.mat',\n",
       " 'A6038.mat',\n",
       " 'A4093.mat',\n",
       " 'A3453.mat',\n",
       " 'A4899.mat',\n",
       " 'A2586.mat',\n",
       " 'A6775.mat',\n",
       " 'A4581.mat',\n",
       " 'A3775.mat',\n",
       " 'A2152.mat',\n",
       " 'A5590.mat',\n",
       " 'A2039.mat',\n",
       " 'A0263.mat',\n",
       " 'A4069.mat',\n",
       " 'A0665.mat',\n",
       " 'A4722.mat',\n",
       " 'A1562.mat',\n",
       " 'A1969.mat',\n",
       " 'A0742.mat',\n",
       " 'A1964.mat',\n",
       " 'A2313.mat',\n",
       " 'A0319.mat',\n",
       " 'A5592.mat',\n",
       " 'A2238.mat',\n",
       " 'A1917.mat',\n",
       " 'A6607.mat',\n",
       " 'A1401.mat',\n",
       " 'A1259.mat',\n",
       " 'A5628.mat',\n",
       " 'A0516.mat',\n",
       " 'A1989.mat',\n",
       " 'A4742.mat',\n",
       " 'A4893.mat',\n",
       " 'A4188.mat',\n",
       " 'A1121.mat',\n",
       " 'A6094.mat',\n",
       " 'A5060.mat',\n",
       " 'A1370.mat',\n",
       " 'A0710.mat',\n",
       " 'A5001.mat',\n",
       " 'A2602.mat',\n",
       " 'A5396.mat',\n",
       " 'A0721.mat',\n",
       " 'A4601.mat',\n",
       " 'A5272.mat',\n",
       " 'A3506.mat',\n",
       " 'A2765.mat',\n",
       " 'A1248.mat',\n",
       " 'A5183.mat',\n",
       " 'A3089.mat',\n",
       " 'A3576.mat',\n",
       " 'A3343.mat',\n",
       " 'A0218.mat',\n",
       " 'A3962.mat',\n",
       " 'A2740.mat',\n",
       " 'A1668.mat',\n",
       " 'A4314.mat',\n",
       " 'A3134.mat',\n",
       " 'A5860.mat',\n",
       " 'A1636.mat',\n",
       " 'A5969.mat',\n",
       " 'A1197.mat',\n",
       " 'A3501.mat',\n",
       " 'A6441.mat',\n",
       " 'A2650.mat',\n",
       " 'A0073.mat',\n",
       " 'A0749.mat',\n",
       " 'A0031.mat',\n",
       " 'A2286.mat',\n",
       " 'A3341.mat',\n",
       " 'A0442.mat',\n",
       " 'A6678.mat',\n",
       " 'A0724.mat',\n",
       " 'A6695.mat',\n",
       " 'A6059.mat',\n",
       " 'A4586.mat',\n",
       " 'A3587.mat',\n",
       " 'A0942.mat',\n",
       " 'A2453.mat',\n",
       " 'A2756.mat',\n",
       " 'A0748.mat',\n",
       " 'A4242.mat',\n",
       " 'A1923.mat',\n",
       " 'A6555.mat',\n",
       " 'A5985.mat',\n",
       " 'A5893.mat',\n",
       " 'A5751.mat',\n",
       " 'A5930.mat',\n",
       " 'A3591.mat',\n",
       " 'A2796.mat',\n",
       " 'A2419.mat',\n",
       " 'A4133.mat',\n",
       " 'A6107.mat',\n",
       " 'A2308.mat',\n",
       " 'A4470.mat',\n",
       " 'A0523.mat',\n",
       " 'A3723.mat',\n",
       " 'A3666.mat',\n",
       " 'A1522.mat',\n",
       " 'A4978.mat',\n",
       " 'A2518.mat',\n",
       " 'A6505.mat',\n",
       " 'A2898.mat',\n",
       " 'A4519.mat',\n",
       " 'A0803.mat',\n",
       " 'A3266.mat',\n",
       " 'A3036.mat',\n",
       " 'A1338.mat',\n",
       " 'A2970.mat',\n",
       " 'A4587.mat',\n",
       " 'A2716.mat',\n",
       " 'A5553.mat',\n",
       " 'A6126.mat',\n",
       " 'A5333.mat',\n",
       " 'A3856.mat',\n",
       " 'A0650.mat',\n",
       " 'A0597.mat',\n",
       " 'A2278.mat',\n",
       " 'A4800.mat',\n",
       " 'A5052.mat',\n",
       " 'A4761.mat',\n",
       " 'A2331.mat',\n",
       " 'A6385.mat',\n",
       " 'A2031.mat',\n",
       " 'A3526.mat',\n",
       " 'A2304.mat',\n",
       " 'A5376.mat',\n",
       " 'A1882.mat',\n",
       " 'A0846.mat',\n",
       " 'A1405.mat',\n",
       " 'A1784.mat',\n",
       " 'A3555.mat',\n",
       " 'A4338.mat',\n",
       " 'A4078.mat',\n",
       " 'A1588.mat',\n",
       " 'A2207.mat',\n",
       " 'A2022.mat',\n",
       " 'A3148.mat',\n",
       " 'A0948.mat',\n",
       " 'A5745.mat',\n",
       " 'A6815.mat',\n",
       " 'A1037.mat',\n",
       " 'A4200.mat',\n",
       " 'A5508.mat',\n",
       " 'A1983.mat',\n",
       " 'A2147.mat',\n",
       " 'A0152.mat',\n",
       " 'A1999.mat',\n",
       " 'A3983.mat',\n",
       " 'A6530.mat',\n",
       " 'A4808.mat',\n",
       " 'A5960.mat',\n",
       " 'A3169.mat',\n",
       " 'A2778.mat',\n",
       " 'A4089.mat',\n",
       " 'A6132.mat',\n",
       " 'A4210.mat',\n",
       " 'A6745.mat',\n",
       " 'A4369.mat',\n",
       " 'A2795.mat',\n",
       " 'A6170.mat',\n",
       " 'A4351.mat',\n",
       " 'A6019.mat',\n",
       " 'A5173.mat',\n",
       " 'A0858.mat',\n",
       " 'A4841.mat',\n",
       " 'A2440.mat',\n",
       " 'A6615.mat',\n",
       " 'A4603.mat',\n",
       " 'A4915.mat',\n",
       " 'A6296.mat',\n",
       " 'A6210.mat',\n",
       " 'A4267.mat',\n",
       " 'A4983.mat',\n",
       " 'A6790.mat',\n",
       " 'A1312.mat',\n",
       " 'A1293.mat',\n",
       " 'A2852.mat',\n",
       " 'A1837.mat',\n",
       " 'A5453.mat',\n",
       " 'A0865.mat',\n",
       " 'A6302.mat',\n",
       " 'A3347.mat',\n",
       " 'A0109.mat',\n",
       " 'A2781.mat',\n",
       " 'A3577.mat',\n",
       " 'A6850.mat',\n",
       " 'A0050.mat',\n",
       " 'A1993.mat',\n",
       " 'A3478.mat',\n",
       " 'A1896.mat',\n",
       " 'A4937.mat',\n",
       " 'A0205.mat',\n",
       " 'A2107.mat',\n",
       " 'A6483.mat',\n",
       " 'A4061.mat',\n",
       " 'A5469.mat',\n",
       " 'A4395.mat',\n",
       " 'A4967.mat',\n",
       " 'A4329.mat',\n",
       " 'A4993.mat',\n",
       " 'A4432.mat',\n",
       " 'A6407.mat',\n",
       " 'A5954.mat',\n",
       " 'A5811.mat',\n",
       " 'A2742.mat',\n",
       " 'A1307.mat',\n",
       " 'A1535.mat',\n",
       " 'A1925.mat',\n",
       " 'A3556.mat',\n",
       " 'A2252.mat',\n",
       " 'A5596.mat',\n",
       " 'A1178.mat',\n",
       " 'A4094.mat',\n",
       " 'A1959.mat',\n",
       " 'A3542.mat',\n",
       " 'A6495.mat',\n",
       " 'A5163.mat',\n",
       " 'A0727.mat',\n",
       " 'A0010.mat',\n",
       " 'A5192.mat',\n",
       " 'A3282.mat',\n",
       " 'A2684.mat',\n",
       " 'A3602.mat',\n",
       " 'A2200.mat',\n",
       " 'A4246.mat',\n",
       " 'A2850.mat',\n",
       " 'A5607.mat',\n",
       " 'A6663.mat',\n",
       " 'A4862.mat',\n",
       " 'A0612.mat',\n",
       " 'A0939.mat',\n",
       " 'A5133.mat',\n",
       " 'A1596.mat',\n",
       " 'A3122.mat',\n",
       " 'A0899.mat',\n",
       " 'A0551.mat',\n",
       " 'A3781.mat',\n",
       " 'A3032.mat',\n",
       " 'A1655.mat',\n",
       " 'A5360.mat',\n",
       " 'A4826.mat',\n",
       " 'A5724.mat',\n",
       " 'A1665.mat',\n",
       " 'A6734.mat',\n",
       " 'A1243.mat',\n",
       " 'A4335.mat',\n",
       " 'A2871.mat',\n",
       " 'A6365.mat',\n",
       " 'A6494.mat',\n",
       " 'A3443.mat',\n",
       " 'A1858.mat',\n",
       " 'A1961.mat',\n",
       " 'A3568.mat',\n",
       " 'A0931.mat',\n",
       " 'A3605.mat',\n",
       " 'A3381.mat',\n",
       " 'A2640.mat',\n",
       " 'A1680.mat',\n",
       " 'A6341.mat',\n",
       " 'A0953.mat',\n",
       " 'A3228.mat',\n",
       " 'A5197.mat',\n",
       " 'A4765.mat',\n",
       " 'A1977.mat',\n",
       " 'A2436.mat',\n",
       " 'A5865.mat',\n",
       " 'A5576.mat',\n",
       " 'A6538.mat',\n",
       " 'A6043.mat',\n",
       " 'A5676.mat',\n",
       " 'A2403.mat',\n",
       " 'A5987.mat',\n",
       " 'A3774.mat',\n",
       " 'A4111.mat',\n",
       " 'A5511.mat',\n",
       " 'A2167.mat',\n",
       " 'A4316.mat',\n",
       " 'A4617.mat',\n",
       " 'A6532.mat',\n",
       " 'A1877.mat',\n",
       " 'A1927.mat',\n",
       " 'A0624.mat',\n",
       " 'A1250.mat',\n",
       " 'A4853.mat',\n",
       " 'A4359.mat',\n",
       " 'A6022.mat',\n",
       " 'A4293.mat',\n",
       " 'A0777.mat',\n",
       " 'A6782.mat',\n",
       " 'A5692.mat',\n",
       " 'A5429.mat',\n",
       " 'A0924.mat',\n",
       " 'A2805.mat',\n",
       " 'A1406.mat',\n",
       " 'A5451.mat',\n",
       " 'A1323.mat',\n",
       " 'A3537.mat',\n",
       " 'A2986.mat',\n",
       " 'A6635.mat',\n",
       " 'A6331.mat',\n",
       " 'A2956.mat',\n",
       " 'A6601.mat',\n",
       " 'A3617.mat',\n",
       " 'A0981.mat',\n",
       " 'A2605.mat',\n",
       " 'A6187.mat',\n",
       " 'A0967.mat',\n",
       " 'A1253.mat',\n",
       " 'A0566.mat',\n",
       " 'A5235.mat',\n",
       " 'A6522.mat',\n",
       " 'A1676.mat',\n",
       " 'A2245.mat',\n",
       " 'A5971.mat',\n",
       " 'A6388.mat',\n",
       " 'A4227.mat',\n",
       " 'A3393.mat',\n",
       " 'A4265.mat',\n",
       " 'A3880.mat',\n",
       " 'A4170.mat',\n",
       " 'A6529.mat',\n",
       " 'A1158.mat',\n",
       " 'A2703.mat',\n",
       " 'A0989.mat',\n",
       " 'A1008.mat',\n",
       " 'A4454.mat',\n",
       " 'A2116.mat',\n",
       " 'A4409.mat',\n",
       " 'A1615.mat',\n",
       " 'A0632.mat',\n",
       " 'A6817.mat',\n",
       " 'A2935.mat',\n",
       " 'A2543.mat',\n",
       " 'A4075.mat',\n",
       " 'A3227.mat',\n",
       " 'A0781.mat',\n",
       " 'A0993.mat',\n",
       " 'A2001.mat',\n",
       " 'A4500.mat',\n",
       " 'A1765.mat',\n",
       " 'A5366.mat',\n",
       " 'A2931.mat',\n",
       " 'A2311.mat',\n",
       " 'A4254.mat',\n",
       " 'A3215.mat',\n",
       " 'A5868.mat',\n",
       " 'A4594.mat',\n",
       " 'A6436.mat',\n",
       " 'A1528.mat',\n",
       " 'A5430.mat',\n",
       " 'A3202.mat',\n",
       " 'A6196.mat',\n",
       " 'A5718.mat',\n",
       " 'A2889.mat',\n",
       " 'A3766.mat',\n",
       " 'A2212.mat',\n",
       " 'A2102.mat',\n",
       " 'A2980.mat',\n",
       " 'A3349.mat',\n",
       " 'A3812.mat',\n",
       " 'A1456.mat',\n",
       " 'A3707.mat',\n",
       " 'A3588.mat',\n",
       " 'A6834.mat',\n",
       " 'A4098.mat',\n",
       " 'A0352.mat',\n",
       " 'A2752.mat',\n",
       " 'A6231.mat',\n",
       " 'A0136.mat',\n",
       " 'A4016.mat',\n",
       " 'A6547.mat',\n",
       " 'A1644.mat',\n",
       " 'A0871.mat',\n",
       " 'A3255.mat',\n",
       " 'A0447.mat',\n",
       " 'A0315.mat',\n",
       " 'A3220.mat',\n",
       " 'A5799.mat',\n",
       " 'A6039.mat',\n",
       " 'A1865.mat',\n",
       " 'A5760.mat',\n",
       " 'A4236.mat',\n",
       " 'A1138.mat',\n",
       " 'A1080.mat',\n",
       " 'A2071.mat',\n",
       " 'A5621.mat',\n",
       " 'A2573.mat',\n",
       " 'A6262.mat',\n",
       " 'A2053.mat',\n",
       " 'A1172.mat',\n",
       " 'A4482.mat',\n",
       " 'A2009.mat',\n",
       " 'A6792.mat',\n",
       " 'A5936.mat',\n",
       " 'A6628.mat',\n",
       " 'A3424.mat',\n",
       " 'A5217.mat',\n",
       " 'A1064.mat',\n",
       " 'A3662.mat',\n",
       " 'A5703.mat',\n",
       " 'A1737.mat',\n",
       " 'A1072.mat',\n",
       " 'A0560.mat',\n",
       " 'A0185.mat',\n",
       " 'A3982.mat',\n",
       " 'A6023.mat',\n",
       " 'A2012.mat',\n",
       " 'A2801.mat',\n",
       " 'A2108.mat',\n",
       " 'A2070.mat',\n",
       " 'A4124.mat',\n",
       " 'A4332.mat',\n",
       " 'A2863.mat',\n",
       " 'A4465.mat',\n",
       " 'A6405.mat',\n",
       " 'A0176.mat',\n",
       " 'A6729.mat',\n",
       " 'A5816.mat',\n",
       " 'A2467.mat',\n",
       " 'A0726.mat',\n",
       " 'A4318.mat',\n",
       " 'A0194.mat',\n",
       " 'A4014.mat',\n",
       " 'A6523.mat',\n",
       " 'A3339.mat',\n",
       " 'A5432.mat',\n",
       " 'A2753.mat',\n",
       " 'A4034.mat',\n",
       " 'A5247.mat',\n",
       " 'A4652.mat',\n",
       " 'A6018.mat',\n",
       " 'A2575.mat',\n",
       " 'A0323.mat',\n",
       " 'A6383.mat',\n",
       " 'A5614.mat',\n",
       " 'A2616.mat',\n",
       " 'A4452.mat',\n",
       " 'A0714.mat',\n",
       " 'A2720.mat',\n",
       " 'A1705.mat',\n",
       " 'A2611.mat',\n",
       " 'A3033.mat',\n",
       " 'A2705.mat',\n",
       " 'A6236.mat',\n",
       " 'A3821.mat',\n",
       " 'A3552.mat',\n",
       " 'A3226.mat',\n",
       " 'A4309.mat',\n",
       " 'A2404.mat',\n",
       " 'A6682.mat',\n",
       " 'A4141.mat',\n",
       " 'A3332.mat',\n",
       " 'A3201.mat',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test # starts with A0349"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking how many multilabel in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366\n",
      "19\n",
      "91\n"
     ]
    }
   ],
   "source": [
    "multi_train, _, _ = searching_overlap(input_directory, class2index, data_train)\n",
    "multi_val, _, _ = searching_overlap(input_directory, class2index, data_val)\n",
    "multi_test, _, _ = searching_overlap(input_directory, class2index, data_test)\n",
    "print(len(multi_train))\n",
    "print(len(multi_val))\n",
    "print(len(multi_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For attention editting using CAM extracted from primitive ABN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, 12)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, None, 64)     2368        input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, 64)     256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 64)     12352       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 64)     256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, None, 64)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 128)    24704       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, 128)    512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 128)    49280       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, 128)    512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, None, 128)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 256)    98560       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, 256)    1024        conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 256)    196864      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, 256)    1024        conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 256)    196864      batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, 256)    1024        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, None, 256)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 512)    393728      max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, 512)    2048        conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, None, 512)    786944      batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, 512)    2048        conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, None, 512)    786944      batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, 512)    2048        conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, None, 512)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, None, 512)    786944      max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, 512)    2048        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, None, 256)    393472      batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, 256)    1024        conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, None, 128)    98432       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, 128)    512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, None, 128)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, None, 64)     8192        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, 64)     256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, None, 64)     12288       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, 64)     256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 64)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, None, 256)    16384       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, None, 256)    32768       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, None, 256)    1024        conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, 256)    1024        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, None, 256)    0           batch_normalization_16[0][0]     \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 256)    0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_bn_1 (BatchNor (None, None, 256)    1024        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_conv_1 (Conv1D (None, None, 9)      2304        attention_branch_bn_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_conv_1 (Co (None, None, 1)      9           attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_bn_1 (Batc (None, None, 1)      4           attention_branch_att_conv_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_sigmoid_1  (None, None, 1)      0           attention_branch_att_bn_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, None, 128)    0           max_pooling1d_4[0][0]            \n",
      "                                                                 attention_branch_att_sigmoid_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, None, 64)     8192        lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, None, 64)     256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, None, 64)     12288       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, None, 64)     256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, 64)     0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, None, 256)    16384       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, None, 256)    32768       lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, None, 256)    1024        conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, None, 256)    1024        conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, 256)    0           batch_normalization_20[0][0]     \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, 256)    0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_pred_conv_1 (C (None, None, 9)      81          attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_avgpool_1 (Gl (None, 256)          0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_gap_1 (GlobalA (None, 9)            0           attention_branch_pred_conv_1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_2 (Dens (None, 9)            2313        perception_branch_avgpool_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_output (Activa (None, 9)            0           attention_branch_gap_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_output (Activ (None, 9)            0           perception_branch_dense_2[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 3,991,911\n",
      "Trainable params: 3,981,669\n",
      "Non-trainable params: 10,242\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# CAMdir = '/home/taejoon/PhysioNetChallenge/results_20200622_ABN_multiclass_V4_primitiveABN'\n",
    "CAMdir = '/home/taejoon/PhysioNetChallenge/results_20200625_V4_ABN_primitive_n=1'\n",
    "p_model = primitive_ABN((None, 12), 9, minimum_len, out_ch=256, n=1)\n",
    "latest = tf.train.latest_checkpoint(CAMdir)\n",
    "p_model.load_weights(latest)\n",
    "\n",
    "conv_layer = 'activation_5'\n",
    "softmax_layer = 'perception_branch_dense_2'\n",
    "\n",
    "\n",
    "out_len=12\n",
    "get_conv_out = K.function(p_model.input, [p_model.get_layer(conv_layer).output, p_model.get_layer(softmax_layer).weights[0]])\n",
    "\n",
    "def CAM_conv1D(minimum_len, n_channels, x, y, out_len, get_conv_out):\n",
    "    \n",
    "    # x랑 y는 batch size만큼의 리스트 (32)\n",
    "    heatmaps=[]    \n",
    "    \n",
    "    curr_x = np.asarray(x)\n",
    "    curr_x = curr_x.reshape(len(x),minimum_len,n_channels)\n",
    "    \n",
    "    conv_out, softmax_weights = get_conv_out(curr_x)\n",
    "\n",
    "    for i in range(len(x)):\n",
    "\n",
    "        curr_classes = y[i]\n",
    "        class_index=[]\n",
    "        [class_index.append(j) for j in range(len(curr_classes)) if curr_classes[j]==1]\n",
    "        heatmap = np.zeros((1,12)) #heatmap=np.zeros((1,36)) # might need to fix this if GradCAM or primitive model changes\n",
    "\n",
    "        conv_out_ = conv_out[i] # (36, 128) / (32, 12, 256)\n",
    "#         print(conv_out.shape)\n",
    "\n",
    "        for label in class_index:  # multiclass일 경우 대비해서 for문\n",
    "            curr_weights = softmax_weights[:,label]\n",
    "            weighted_conv = conv_out_*curr_weights\n",
    "            \n",
    "            weighted_conv = weighted_conv.sum(axis=-1) # output = (1,36)\n",
    "            heatmap += weighted_conv\n",
    "            \n",
    "        heatmap %= len(class_index) # 단일 class일 경우 1로 나눠짐. 두개일 경우 더해진 heatmap들이 2로 나눠짐\n",
    "#         heatmap = np.resize(heatmap, (1,out_len))\n",
    "        heatmap = np.resize(heatmap, (out_len, 1))\n",
    "        heatmaps.append(heatmap)\n",
    "        \n",
    "    return heatmaps\n",
    "p_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, 12)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, None, 64)     2368        input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, None, 64)     256         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, None, 64)     12352       batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, None, 64)     256         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, None, 64)     0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, None, 128)    24704       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, None, 128)    512         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, None, 128)    49280       batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, None, 128)    512         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, None, 128)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, None, 256)    98560       max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, None, 256)    1024        conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, None, 256)    196864      batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, None, 256)    1024        conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, None, 256)    196864      batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, None, 256)    1024        conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, None, 256)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, None, 512)    393728      max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, None, 512)    2048        conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, None, 512)    786944      batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, None, 512)    2048        conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, None, 512)    786944      batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, None, 512)    2048        conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, None, 512)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, None, 512)    786944      max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, None, 512)    2048        conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, None, 256)    393472      batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, None, 256)    1024        conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, None, 128)    98432       batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, None, 128)    512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, None, 128)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, None, 64)     8192        max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, None, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, None, 64)     0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, None, 64)     12288       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, None, 64)     256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, 64)     0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, None, 256)    16384       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, None, 256)    32768       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, None, 256)    1024        conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, None, 256)    1024        conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, None, 256)    0           batch_normalization_37[0][0]     \n",
      "                                                                 batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, 256)    0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_bn_1 (BatchNor (None, None, 256)    1024        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_conv_1 (Conv1D (None, None, 9)      2304        attention_branch_bn_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_conv_1 (Co (None, None, 1)      9           attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_bn_1 (Batc (None, None, 1)      4           attention_branch_att_conv_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_sigmoid_1  (None, None, 1)      0           attention_branch_att_bn_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 128)    0           max_pooling1d_9[0][0]            \n",
      "                                                                 attention_branch_att_sigmoid_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, None, 64)     8192        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, None, 64)     256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, 64)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, None, 64)     12288       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, None, 64)     256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, 64)     0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, None, 256)    16384       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, None, 256)    32768       lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, None, 256)    1024        conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, None, 256)    1024        conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, None, 256)    0           batch_normalization_41[0][0]     \n",
      "                                                                 batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, None, 256)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_avgpool_1 (Gl (None, 256)          0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_pred_conv_1 (C (None, None, 9)      81          attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_1 (Dens (None, 512)          131584      perception_branch_avgpool_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_gap_1 (GlobalA (None, 9)            0           attention_branch_pred_conv_1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_2 (Dens (None, 9)            4617        perception_branch_dense_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_output (Activa (None, 9)            0           attention_branch_gap_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_output (Activ (None, 9)            0           perception_branch_dense_2[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 4,125,799\n",
      "Trainable params: 4,115,557\n",
      "Non-trainable params: 10,242\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# results_directory = os.path.join(rootdir, 'results_'+date+'_V5_ABN_primitive_leakyRELU_dropout_ver4')\n",
    "results_directory = os.path.join(rootdir, 'results_'+date+'_e2eABN:modify&gamma=1')\n",
    "gamma = 1#0.0001 # 0.001\n",
    "if not os.path.isdir(results_directory):\n",
    "    os.mkdir(results_directory)\n",
    "\n",
    "# model = primitive_ABN((None,12), len(unique_classes), minimum_len, n=1)\n",
    "# model = edit_ABN_model((None,12), len(unique_classes), minimum_len,n=1)\n",
    "# model, edit_loss = edit_ABN_model_loss((None,12), len(unique_classes), minimum_len,n=1, gamma = gamma)\n",
    "model, edit_loss = endtoend_edit_ABN_model_loss((None,12), len(unique_classes), minimum_len,n=1, gamma = gamma, batch_size = batch_size)\n",
    "\n",
    "# model.compile(loss=loss_function,\n",
    "#               optimizer=optimizers.Adam(lr=1e-5),           \n",
    "#               metrics=[score_f1])\n",
    "model.compile(loss=edit_loss,\n",
    "              optimizer=optimizers.Adam(lr=1e-5),           \n",
    "              metrics=[score_f1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 중단된 training 이어돌리기위해 임시로 사용\n",
    "# results_directory = results_directory.replace(\"0608\", \"0604\") # 날짜 달라졌을때\n",
    "# latest = tf.train.latest_checkpoint(results_directory)\n",
    "# latest\n",
    "# model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-08 11:34:17.995837\n",
      "../results_20200708_e2eABN:modify&gamma=1\n",
      "\n",
      "Epoch 0 train_loss: 2.090 train_f1: 0.226 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1511: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 0 valid_f1: 0.250 best_f1: 0.250 mean accuracy:0.116 \t\n",
      "\n",
      "Epoch 1 train_loss: 1.921 train_f1: 0.264 \t\n",
      "\n",
      "Validation 1 valid_f1: 0.310 best_f1: 0.310 mean accuracy:0.163 \t\n",
      "\n",
      "Epoch 2 train_loss: 1.850 train_f1: 0.299 \t\n",
      "\n",
      "Validation 2 valid_f1: 0.378 best_f1: 0.378 mean accuracy:0.221 \t\n",
      "\n",
      "Epoch 3 train_loss: 1.809 train_f1: 0.326 \t\n",
      "\n",
      "Validation 3 valid_f1: 0.415 best_f1: 0.415 mean accuracy:0.261 \t\n",
      "\n",
      "Epoch 4 train_loss: 1.778 train_f1: 0.360 \t\n",
      "\n",
      "Validation 4 valid_f1: 0.455 best_f1: 0.455 mean accuracy:0.304 \t\n",
      "\n",
      "Epoch 5 train_loss: 1.753 train_f1: 0.382 \t\n",
      "\n",
      "Validation 5 valid_f1: 0.486 best_f1: 0.486 mean accuracy:0.330 \t\n",
      "\n",
      "Epoch 6 train_loss: 1.729 train_f1: 0.406 \t\n",
      "\n",
      "Validation 6 valid_f1: 0.516 best_f1: 0.516 mean accuracy:0.359 \t\n",
      "\n",
      "Epoch 7 train_loss: 1.710 train_f1: 0.423 \t\n",
      "\n",
      "Validation 7 valid_f1: 0.532 best_f1: 0.532 mean accuracy:0.373 \t\n",
      "\n",
      "Epoch 8 train_loss: 1.690 train_f1: 0.440 \t\n",
      "\n",
      "Validation 8 valid_f1: 0.557 best_f1: 0.557 mean accuracy:0.399 \t\n",
      "\n",
      "Epoch 9 train_loss: 1.675 train_f1: 0.456 \t\n",
      "\n",
      "Validation 9 valid_f1: 0.576 best_f1: 0.576 mean accuracy:0.438 \t\n",
      "\n",
      "Epoch 10 train_loss: 1.659 train_f1: 0.477 \t\n",
      "\n",
      "Validation 10 valid_f1: 0.621 best_f1: 0.621 mean accuracy:0.457 \t\n",
      "\n",
      "Epoch 11 train_loss: 1.645 train_f1: 0.491 \t\n",
      "\n",
      "Validation 11 valid_f1: 0.617 best_f1: 0.621 mean accuracy:0.467 \t\n",
      "\n",
      "Epoch 12 train_loss: 1.635 train_f1: 0.503 \t\n",
      "\n",
      "Validation 12 valid_f1: 0.655 best_f1: 0.655 mean accuracy:0.504 \t\n",
      "\n",
      "Epoch 13 train_loss: 1.624 train_f1: 0.512 \t\n",
      "\n",
      "Validation 13 valid_f1: 0.637 best_f1: 0.655 mean accuracy:0.482 \t\n",
      "\n",
      "Epoch 14 train_loss: 1.613 train_f1: 0.526 \t\n",
      "\n",
      "Validation 14 valid_f1: 0.658 best_f1: 0.658 mean accuracy:0.504 \t\n",
      "\n",
      "Epoch 15 train_loss: 1.606 train_f1: 0.534 \t\n",
      "\n",
      "Validation 15 valid_f1: 0.662 best_f1: 0.662 mean accuracy:0.518 \t\n",
      "\n",
      "Epoch 16 train_loss: 1.595 train_f1: 0.542 \t\n",
      "\n",
      "Validation 16 valid_f1: 0.665 best_f1: 0.665 mean accuracy:0.514 \t\n",
      "\n",
      "Epoch 17 train_loss: 1.589 train_f1: 0.546 \t\n",
      "\n",
      "Validation 17 valid_f1: 0.665 best_f1: 0.665 mean accuracy:0.518 \t\n",
      "\n",
      "Epoch 18 train_loss: 1.579 train_f1: 0.551 \t\n",
      "\n",
      "Validation 18 valid_f1: 0.686 best_f1: 0.686 mean accuracy:0.554 \t\n",
      "\n",
      "Epoch 19 train_loss: 1.573 train_f1: 0.561 \t\n",
      "\n",
      "Validation 19 valid_f1: 0.668 best_f1: 0.686 mean accuracy:0.518 \t\n",
      "\n",
      "Epoch 20 train_loss: 1.565 train_f1: 0.569 \t\n",
      "\n",
      "Validation 20 valid_f1: 0.685 best_f1: 0.686 mean accuracy:0.540 \t\n",
      "\n",
      "Epoch 21 train_loss: 1.557 train_f1: 0.573 \t\n",
      "\n",
      "Validation 21 valid_f1: 0.684 best_f1: 0.686 mean accuracy:0.551 \t\n",
      "\n",
      "Epoch 22 train_loss: 1.553 train_f1: 0.578 \t\n",
      "\n",
      "Validation 22 valid_f1: 0.691 best_f1: 0.691 mean accuracy:0.562 \t\n",
      "\n",
      "Epoch 23 train_loss: 1.541 train_f1: 0.583 \t\n",
      "\n",
      "Validation 23 valid_f1: 0.699 best_f1: 0.699 mean accuracy:0.569 \t\n",
      "\n",
      "Epoch 24 train_loss: 1.538 train_f1: 0.592 \t\n",
      "\n",
      "Validation 24 valid_f1: 0.680 best_f1: 0.699 mean accuracy:0.540 \t\n",
      "\n",
      "Epoch 25 train_loss: 1.533 train_f1: 0.593 \t\n",
      "\n",
      "Validation 25 valid_f1: 0.698 best_f1: 0.699 mean accuracy:0.562 \t\n",
      "\n",
      "Epoch 26 train_loss: 1.527 train_f1: 0.599 \t\n",
      "\n",
      "Validation 26 valid_f1: 0.672 best_f1: 0.699 mean accuracy:0.536 \t\n",
      "\n",
      "Epoch 27 train_loss: 1.522 train_f1: 0.609 \t\n",
      "\n",
      "Validation 27 valid_f1: 0.695 best_f1: 0.699 mean accuracy:0.562 \t\n",
      "\n",
      "Epoch 28 train_loss: 1.514 train_f1: 0.615 \t\n",
      "\n",
      "Validation 28 valid_f1: 0.703 best_f1: 0.703 mean accuracy:0.572 \t\n",
      "\n",
      "Epoch 29 train_loss: 1.508 train_f1: 0.620 \t\n",
      "\n",
      "Validation 29 valid_f1: 0.715 best_f1: 0.715 mean accuracy:0.598 \t\n",
      "\n",
      "Epoch 30 train_loss: 1.502 train_f1: 0.627 \t\n",
      "\n",
      "Validation 30 valid_f1: 0.713 best_f1: 0.715 mean accuracy:0.583 \t\n",
      "\n",
      "Epoch 31 train_loss: 1.499 train_f1: 0.629 \t\n",
      "\n",
      "Validation 31 valid_f1: 0.720 best_f1: 0.720 mean accuracy:0.601 \t\n",
      "\n",
      "Epoch 32 train_loss: 1.493 train_f1: 0.633 \t\n",
      "\n",
      "Validation 32 valid_f1: 0.710 best_f1: 0.720 mean accuracy:0.583 \t\n",
      "\n",
      "Epoch 33 train_loss: 1.486 train_f1: 0.645 \t\n",
      "\n",
      "Validation 33 valid_f1: 0.703 best_f1: 0.720 mean accuracy:0.569 \t\n",
      "\n",
      "Epoch 34 train_loss: 1.482 train_f1: 0.641 \t\n",
      "\n",
      "Validation 34 valid_f1: 0.715 best_f1: 0.720 mean accuracy:0.591 \t\n",
      "\n",
      "Epoch 35 train_loss: 1.478 train_f1: 0.647 \t\n",
      "\n",
      "Validation 35 valid_f1: 0.726 best_f1: 0.726 mean accuracy:0.605 \t\n",
      "\n",
      "Epoch 36 train_loss: 1.476 train_f1: 0.651 \t\n",
      "\n",
      "Validation 36 valid_f1: 0.698 best_f1: 0.726 mean accuracy:0.562 \t\n",
      "\n",
      "Epoch 37 train_loss: 1.467 train_f1: 0.661 \t\n",
      "\n",
      "Validation 37 valid_f1: 0.711 best_f1: 0.726 mean accuracy:0.591 \t\n",
      "\n",
      "Epoch 38 train_loss: 1.462 train_f1: 0.670 \t\n",
      "\n",
      "Validation 38 valid_f1: 0.718 best_f1: 0.726 mean accuracy:0.587 \t\n",
      "\n",
      "Epoch 39 train_loss: 1.458 train_f1: 0.672 \t\n",
      "\n",
      "Validation 39 valid_f1: 0.707 best_f1: 0.726 mean accuracy:0.583 \t\n",
      "\n",
      "Epoch 40 train_loss: 1.454 train_f1: 0.677 \t\n",
      "\n",
      "Validation 40 valid_f1: 0.709 best_f1: 0.726 mean accuracy:0.583 \t\n",
      "\n",
      "Epoch 41 train_loss: 1.449 train_f1: 0.681 \t\n",
      "\n",
      "Validation 41 valid_f1: 0.713 best_f1: 0.726 mean accuracy:0.598 \t\n",
      "\n",
      "Epoch 42 train_loss: 1.444 train_f1: 0.686 \t\n",
      "\n",
      "Validation 42 valid_f1: 0.719 best_f1: 0.726 mean accuracy:0.591 \t\n",
      "\n",
      "Epoch 43 train_loss: 1.442 train_f1: 0.690 \t\n",
      "\n",
      "Validation 43 valid_f1: 0.715 best_f1: 0.726 mean accuracy:0.587 \t\n",
      "\n",
      "Epoch 44 train_loss: 1.436 train_f1: 0.698 \t\n",
      "\n",
      "Validation 44 valid_f1: 0.707 best_f1: 0.726 mean accuracy:0.580 \t\n",
      "\n",
      "Epoch 45 train_loss: 1.429 train_f1: 0.707 \t\n",
      "\n",
      "Validation 45 valid_f1: 0.724 best_f1: 0.726 mean accuracy:0.594 \t\n",
      "\n",
      "Epoch 46 train_loss: 1.427 train_f1: 0.706 \t\n",
      "\n",
      "Validation 46 valid_f1: 0.723 best_f1: 0.726 mean accuracy:0.598 \t\n",
      "\n",
      "Epoch 47 train_loss: 1.422 train_f1: 0.714 \t\n",
      "\n",
      "Validation 47 valid_f1: 0.733 best_f1: 0.733 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 48 train_loss: 1.422 train_f1: 0.714 \t\n",
      "\n",
      "Validation 48 valid_f1: 0.721 best_f1: 0.733 mean accuracy:0.587 \t\n",
      "\n",
      "Epoch 49 train_loss: 1.416 train_f1: 0.721 \t\n",
      "\n",
      "Validation 49 valid_f1: 0.727 best_f1: 0.733 mean accuracy:0.598 \t\n",
      "\n",
      "Epoch 50 train_loss: 1.411 train_f1: 0.727 \t\n",
      "\n",
      "Validation 50 valid_f1: 0.725 best_f1: 0.733 mean accuracy:0.605 \t\n",
      "\n",
      "Epoch 51 train_loss: 1.406 train_f1: 0.730 \t\n",
      "\n",
      "Validation 51 valid_f1: 0.732 best_f1: 0.733 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 52 train_loss: 1.405 train_f1: 0.731 \t\n",
      "\n",
      "Validation 52 valid_f1: 0.720 best_f1: 0.733 mean accuracy:0.594 \t\n",
      "\n",
      "Epoch 53 train_loss: 1.399 train_f1: 0.739 \t\n",
      "\n",
      "Validation 53 valid_f1: 0.716 best_f1: 0.733 mean accuracy:0.591 \t\n",
      "\n",
      "Epoch 54 train_loss: 1.396 train_f1: 0.739 \t\n",
      "\n",
      "Validation 54 valid_f1: 0.730 best_f1: 0.733 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 55 train_loss: 1.390 train_f1: 0.747 \t\n",
      "\n",
      "Validation 55 valid_f1: 0.723 best_f1: 0.733 mean accuracy:0.609 \t\n",
      "\n",
      "Epoch 56 train_loss: 1.386 train_f1: 0.750 \t\n",
      "\n",
      "Validation 56 valid_f1: 0.738 best_f1: 0.738 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 57 train_loss: 1.387 train_f1: 0.748 \t\n",
      "\n",
      "Validation 57 valid_f1: 0.734 best_f1: 0.738 mean accuracy:0.609 \t\n",
      "\n",
      "Epoch 58 train_loss: 1.381 train_f1: 0.751 \t\n",
      "\n",
      "Validation 58 valid_f1: 0.734 best_f1: 0.738 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 59 train_loss: 1.381 train_f1: 0.749 \t\n",
      "\n",
      "Validation 59 valid_f1: 0.732 best_f1: 0.738 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 60 train_loss: 1.375 train_f1: 0.751 \t\n",
      "\n",
      "Validation 60 valid_f1: 0.741 best_f1: 0.741 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 61 train_loss: 1.373 train_f1: 0.755 \t\n",
      "\n",
      "Validation 61 valid_f1: 0.737 best_f1: 0.741 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 62 train_loss: 1.371 train_f1: 0.755 \t\n",
      "\n",
      "Validation 62 valid_f1: 0.754 best_f1: 0.754 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 63 train_loss: 1.364 train_f1: 0.763 \t\n",
      "\n",
      "Validation 63 valid_f1: 0.736 best_f1: 0.754 mean accuracy:0.616 \t\n",
      "\n",
      "Epoch 64 train_loss: 1.359 train_f1: 0.766 \t\n",
      "\n",
      "Validation 64 valid_f1: 0.726 best_f1: 0.754 mean accuracy:0.601 \t\n",
      "\n",
      "Epoch 65 train_loss: 1.356 train_f1: 0.768 \t\n",
      "\n",
      "Validation 65 valid_f1: 0.734 best_f1: 0.754 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 66 train_loss: 1.355 train_f1: 0.770 \t\n",
      "\n",
      "Validation 66 valid_f1: 0.740 best_f1: 0.754 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 67 train_loss: 1.352 train_f1: 0.766 \t\n",
      "\n",
      "Validation 67 valid_f1: 0.738 best_f1: 0.754 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 68 train_loss: 1.348 train_f1: 0.773 \t\n",
      "\n",
      "Validation 68 valid_f1: 0.727 best_f1: 0.754 mean accuracy:0.609 \t\n",
      "\n",
      "Epoch 69 train_loss: 1.348 train_f1: 0.763 \t\n",
      "\n",
      "Validation 69 valid_f1: 0.742 best_f1: 0.754 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 70 train_loss: 1.341 train_f1: 0.777 \t\n",
      "\n",
      "Validation 70 valid_f1: 0.753 best_f1: 0.754 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 71 train_loss: 1.341 train_f1: 0.773 \t\n",
      "\n",
      "Validation 71 valid_f1: 0.755 best_f1: 0.755 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 72 train_loss: 1.335 train_f1: 0.779 \t\n",
      "\n",
      "Validation 72 valid_f1: 0.737 best_f1: 0.755 mean accuracy:0.627 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73 train_loss: 1.334 train_f1: 0.777 \t\n",
      "\n",
      "Validation 73 valid_f1: 0.745 best_f1: 0.755 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 74 train_loss: 1.331 train_f1: 0.779 \t\n",
      "\n",
      "Validation 74 valid_f1: 0.740 best_f1: 0.755 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 75 train_loss: 1.329 train_f1: 0.781 \t\n",
      "\n",
      "Validation 75 valid_f1: 0.738 best_f1: 0.755 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 76 train_loss: 1.327 train_f1: 0.778 \t\n",
      "\n",
      "Validation 76 valid_f1: 0.737 best_f1: 0.755 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 77 train_loss: 1.324 train_f1: 0.780 \t\n",
      "\n",
      "Validation 77 valid_f1: 0.751 best_f1: 0.755 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 78 train_loss: 1.320 train_f1: 0.786 \t\n",
      "\n",
      "Validation 78 valid_f1: 0.753 best_f1: 0.755 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 79 train_loss: 1.314 train_f1: 0.790 \t\n",
      "\n",
      "Validation 79 valid_f1: 0.749 best_f1: 0.755 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 80 train_loss: 1.315 train_f1: 0.788 \t\n",
      "\n",
      "Validation 80 valid_f1: 0.748 best_f1: 0.755 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 81 train_loss: 1.310 train_f1: 0.790 \t\n",
      "\n",
      "Validation 81 valid_f1: 0.757 best_f1: 0.757 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 82 train_loss: 1.307 train_f1: 0.794 \t\n",
      "\n",
      "Validation 82 valid_f1: 0.742 best_f1: 0.757 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 83 train_loss: 1.303 train_f1: 0.797 \t\n",
      "\n",
      "Validation 83 valid_f1: 0.743 best_f1: 0.757 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 84 train_loss: 1.303 train_f1: 0.794 \t\n",
      "\n",
      "Validation 84 valid_f1: 0.744 best_f1: 0.757 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 85 train_loss: 1.302 train_f1: 0.794 \t\n",
      "\n",
      "Validation 85 valid_f1: 0.749 best_f1: 0.757 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 86 train_loss: 1.296 train_f1: 0.800 \t\n",
      "\n",
      "Validation 86 valid_f1: 0.750 best_f1: 0.757 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 87 train_loss: 1.293 train_f1: 0.803 \t\n",
      "\n",
      "Validation 87 valid_f1: 0.757 best_f1: 0.757 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 88 train_loss: 1.292 train_f1: 0.802 \t\n",
      "\n",
      "Validation 88 valid_f1: 0.754 best_f1: 0.757 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 89 train_loss: 1.290 train_f1: 0.804 \t\n",
      "\n",
      "Validation 89 valid_f1: 0.735 best_f1: 0.757 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 90 train_loss: 1.288 train_f1: 0.801 \t\n",
      "\n",
      "Validation 90 valid_f1: 0.761 best_f1: 0.761 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 91 train_loss: 1.284 train_f1: 0.804 \t\n",
      "\n",
      "Validation 91 valid_f1: 0.751 best_f1: 0.761 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 92 train_loss: 1.278 train_f1: 0.810 \t\n",
      "\n",
      "Validation 92 valid_f1: 0.742 best_f1: 0.761 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 93 train_loss: 1.278 train_f1: 0.810 \t\n",
      "\n",
      "Validation 93 valid_f1: 0.750 best_f1: 0.761 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 94 train_loss: 1.277 train_f1: 0.809 \t\n",
      "\n",
      "Validation 94 valid_f1: 0.746 best_f1: 0.761 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 95 train_loss: 1.275 train_f1: 0.806 \t\n",
      "\n",
      "Validation 95 valid_f1: 0.754 best_f1: 0.761 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 96 train_loss: 1.270 train_f1: 0.813 \t\n",
      "\n",
      "Validation 96 valid_f1: 0.744 best_f1: 0.761 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 97 train_loss: 1.269 train_f1: 0.809 \t\n",
      "\n",
      "Validation 97 valid_f1: 0.758 best_f1: 0.761 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 98 train_loss: 1.265 train_f1: 0.816 \t\n",
      "\n",
      "Validation 98 valid_f1: 0.766 best_f1: 0.766 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 99 train_loss: 1.263 train_f1: 0.815 \t\n",
      "\n",
      "Validation 99 valid_f1: 0.752 best_f1: 0.766 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 100 train_loss: 1.260 train_f1: 0.818 \t\n",
      "\n",
      "Validation 100 valid_f1: 0.757 best_f1: 0.766 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 101 train_loss: 1.257 train_f1: 0.822 \t\n",
      "\n",
      "Validation 101 valid_f1: 0.755 best_f1: 0.766 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 102 train_loss: 1.254 train_f1: 0.820 \t\n",
      "\n",
      "Validation 102 valid_f1: 0.750 best_f1: 0.766 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 103 train_loss: 1.255 train_f1: 0.817 \t\n",
      "\n",
      "Validation 103 valid_f1: 0.736 best_f1: 0.766 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 104 train_loss: 1.255 train_f1: 0.815 \t\n",
      "\n",
      "Validation 104 valid_f1: 0.754 best_f1: 0.766 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 105 train_loss: 1.248 train_f1: 0.822 \t\n",
      "\n",
      "Validation 105 valid_f1: 0.756 best_f1: 0.766 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 106 train_loss: 1.246 train_f1: 0.825 \t\n",
      "\n",
      "Validation 106 valid_f1: 0.755 best_f1: 0.766 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 107 train_loss: 1.245 train_f1: 0.819 \t\n",
      "\n",
      "Validation 107 valid_f1: 0.740 best_f1: 0.766 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 108 train_loss: 1.243 train_f1: 0.823 \t\n",
      "\n",
      "Validation 108 valid_f1: 0.760 best_f1: 0.766 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 109 train_loss: 1.237 train_f1: 0.826 \t\n",
      "\n",
      "Validation 109 valid_f1: 0.752 best_f1: 0.766 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 110 train_loss: 1.234 train_f1: 0.829 \t\n",
      "\n",
      "Validation 110 valid_f1: 0.742 best_f1: 0.766 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 111 train_loss: 1.234 train_f1: 0.830 \t\n",
      "\n",
      "Validation 111 valid_f1: 0.767 best_f1: 0.767 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 112 train_loss: 1.230 train_f1: 0.833 \t\n",
      "\n",
      "Validation 112 valid_f1: 0.749 best_f1: 0.767 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 113 train_loss: 1.229 train_f1: 0.829 \t\n",
      "\n",
      "Validation 113 valid_f1: 0.750 best_f1: 0.767 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 114 train_loss: 1.224 train_f1: 0.834 \t\n",
      "\n",
      "Validation 114 valid_f1: 0.742 best_f1: 0.767 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 115 train_loss: 1.224 train_f1: 0.836 \t\n",
      "\n",
      "Validation 115 valid_f1: 0.765 best_f1: 0.767 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 116 train_loss: 1.221 train_f1: 0.834 \t\n",
      "\n",
      "Validation 116 valid_f1: 0.750 best_f1: 0.767 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 117 train_loss: 1.221 train_f1: 0.834 \t\n",
      "\n",
      "Validation 117 valid_f1: 0.750 best_f1: 0.767 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 118 train_loss: 1.217 train_f1: 0.836 \t\n",
      "\n",
      "Validation 118 valid_f1: 0.754 best_f1: 0.767 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 119 train_loss: 1.212 train_f1: 0.839 \t\n",
      "\n",
      "Validation 119 valid_f1: 0.765 best_f1: 0.767 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 120 train_loss: 1.210 train_f1: 0.841 \t\n",
      "\n",
      "Validation 120 valid_f1: 0.744 best_f1: 0.767 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 121 train_loss: 1.213 train_f1: 0.839 \t\n",
      "\n",
      "Validation 121 valid_f1: 0.748 best_f1: 0.767 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 122 train_loss: 1.206 train_f1: 0.842 \t\n",
      "\n",
      "Validation 122 valid_f1: 0.735 best_f1: 0.767 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 123 train_loss: 1.205 train_f1: 0.844 \t\n",
      "\n",
      "Validation 123 valid_f1: 0.746 best_f1: 0.767 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 124 train_loss: 1.203 train_f1: 0.841 \t\n",
      "\n",
      "Validation 124 valid_f1: 0.768 best_f1: 0.768 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 125 train_loss: 1.203 train_f1: 0.839 \t\n",
      "\n",
      "Validation 125 valid_f1: 0.743 best_f1: 0.768 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 126 train_loss: 1.198 train_f1: 0.845 \t\n",
      "\n",
      "Validation 126 valid_f1: 0.742 best_f1: 0.768 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 127 train_loss: 1.196 train_f1: 0.844 \t\n",
      "\n",
      "Validation 127 valid_f1: 0.741 best_f1: 0.768 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 128 train_loss: 1.192 train_f1: 0.847 \t\n",
      "\n",
      "Validation 128 valid_f1: 0.767 best_f1: 0.768 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 129 train_loss: 1.192 train_f1: 0.844 \t\n",
      "\n",
      "Validation 129 valid_f1: 0.738 best_f1: 0.768 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 130 train_loss: 1.192 train_f1: 0.848 \t\n",
      "\n",
      "Validation 130 valid_f1: 0.755 best_f1: 0.768 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 131 train_loss: 1.191 train_f1: 0.846 \t\n",
      "\n",
      "Validation 131 valid_f1: 0.749 best_f1: 0.768 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 132 train_loss: 1.187 train_f1: 0.848 \t\n",
      "\n",
      "Validation 132 valid_f1: 0.768 best_f1: 0.768 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 133 train_loss: 1.182 train_f1: 0.850 \t\n",
      "\n",
      "Validation 133 valid_f1: 0.752 best_f1: 0.768 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 134 train_loss: 1.181 train_f1: 0.853 \t\n",
      "\n",
      "Validation 134 valid_f1: 0.742 best_f1: 0.768 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 135 train_loss: 1.179 train_f1: 0.854 \t\n",
      "\n",
      "Validation 135 valid_f1: 0.753 best_f1: 0.768 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 136 train_loss: 1.177 train_f1: 0.853 \t\n",
      "\n",
      "Validation 136 valid_f1: 0.734 best_f1: 0.768 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 137 train_loss: 1.173 train_f1: 0.858 \t\n",
      "\n",
      "Validation 137 valid_f1: 0.758 best_f1: 0.768 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 138 train_loss: 1.172 train_f1: 0.854 \t\n",
      "\n",
      "Validation 138 valid_f1: 0.737 best_f1: 0.768 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 139 train_loss: 1.170 train_f1: 0.857 \t\n",
      "\n",
      "Validation 139 valid_f1: 0.738 best_f1: 0.768 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 140 train_loss: 1.169 train_f1: 0.856 \t\n",
      "\n",
      "Validation 140 valid_f1: 0.745 best_f1: 0.768 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 141 train_loss: 1.166 train_f1: 0.858 \t\n",
      "\n",
      "Validation 141 valid_f1: 0.744 best_f1: 0.768 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 142 train_loss: 1.161 train_f1: 0.862 \t\n",
      "\n",
      "Validation 142 valid_f1: 0.764 best_f1: 0.768 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 143 train_loss: 1.160 train_f1: 0.863 \t\n",
      "\n",
      "Validation 143 valid_f1: 0.768 best_f1: 0.768 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 144 train_loss: 1.159 train_f1: 0.859 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 144 valid_f1: 0.768 best_f1: 0.768 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 145 train_loss: 1.158 train_f1: 0.861 \t\n",
      "\n",
      "Validation 145 valid_f1: 0.752 best_f1: 0.768 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 146 train_loss: 1.156 train_f1: 0.862 \t\n",
      "\n",
      "Validation 146 valid_f1: 0.764 best_f1: 0.768 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 147 train_loss: 1.151 train_f1: 0.866 \t\n",
      "\n",
      "Validation 147 valid_f1: 0.750 best_f1: 0.768 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 148 train_loss: 1.148 train_f1: 0.869 \t\n",
      "\n",
      "Validation 148 valid_f1: 0.764 best_f1: 0.768 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 149 train_loss: 1.144 train_f1: 0.871 \t\n",
      "\n",
      "Validation 149 valid_f1: 0.746 best_f1: 0.768 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 150 train_loss: 1.146 train_f1: 0.868 \t\n",
      "\n",
      "Validation 150 valid_f1: 0.759 best_f1: 0.768 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 151 train_loss: 1.144 train_f1: 0.868 \t\n",
      "\n",
      "Validation 151 valid_f1: 0.771 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 152 train_loss: 1.143 train_f1: 0.870 \t\n",
      "\n",
      "Validation 152 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 153 train_loss: 1.139 train_f1: 0.870 \t\n",
      "\n",
      "Validation 153 valid_f1: 0.754 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 154 train_loss: 1.138 train_f1: 0.872 \t\n",
      "\n",
      "Validation 154 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 155 train_loss: 1.135 train_f1: 0.874 \t\n",
      "\n",
      "Validation 155 valid_f1: 0.770 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 156 train_loss: 1.134 train_f1: 0.872 \t\n",
      "\n",
      "Validation 156 valid_f1: 0.764 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 157 train_loss: 1.133 train_f1: 0.873 \t\n",
      "\n",
      "Validation 157 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 158 train_loss: 1.129 train_f1: 0.876 \t\n",
      "\n",
      "Validation 158 valid_f1: 0.760 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 159 train_loss: 1.131 train_f1: 0.874 \t\n",
      "\n",
      "Validation 159 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 160 train_loss: 1.125 train_f1: 0.877 \t\n",
      "\n",
      "Validation 160 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 161 train_loss: 1.122 train_f1: 0.876 \t\n",
      "\n",
      "Validation 161 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 162 train_loss: 1.121 train_f1: 0.877 \t\n",
      "\n",
      "Validation 162 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 163 train_loss: 1.118 train_f1: 0.882 \t\n",
      "\n",
      "Validation 163 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 164 train_loss: 1.116 train_f1: 0.879 \t\n",
      "\n",
      "Validation 164 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 165 train_loss: 1.116 train_f1: 0.877 \t\n",
      "\n",
      "Validation 165 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 166 train_loss: 1.112 train_f1: 0.884 \t\n",
      "\n",
      "Validation 166 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 167 train_loss: 1.110 train_f1: 0.880 \t\n",
      "\n",
      "Validation 167 valid_f1: 0.761 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 168 train_loss: 1.110 train_f1: 0.882 \t\n",
      "\n",
      "Validation 168 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 169 train_loss: 1.107 train_f1: 0.884 \t\n",
      "\n",
      "Validation 169 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 170 train_loss: 1.105 train_f1: 0.884 \t\n",
      "\n",
      "Validation 170 valid_f1: 0.766 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 171 train_loss: 1.101 train_f1: 0.887 \t\n",
      "\n",
      "Validation 171 valid_f1: 0.754 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 172 train_loss: 1.099 train_f1: 0.888 \t\n",
      "\n",
      "Validation 172 valid_f1: 0.756 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 173 train_loss: 1.096 train_f1: 0.889 \t\n",
      "\n",
      "Validation 173 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 174 train_loss: 1.096 train_f1: 0.887 \t\n",
      "\n",
      "Validation 174 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 175 train_loss: 1.095 train_f1: 0.890 \t\n",
      "\n",
      "Validation 175 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 176 train_loss: 1.093 train_f1: 0.889 \t\n",
      "\n",
      "Validation 176 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 177 train_loss: 1.093 train_f1: 0.888 \t\n",
      "\n",
      "Validation 177 valid_f1: 0.768 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 178 train_loss: 1.088 train_f1: 0.891 \t\n",
      "\n",
      "Validation 178 valid_f1: 0.761 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 179 train_loss: 1.084 train_f1: 0.893 \t\n",
      "\n",
      "Validation 179 valid_f1: 0.757 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 180 train_loss: 1.083 train_f1: 0.893 \t\n",
      "\n",
      "Validation 180 valid_f1: 0.757 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 181 train_loss: 1.082 train_f1: 0.893 \t\n",
      "\n",
      "Validation 181 valid_f1: 0.763 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 182 train_loss: 1.077 train_f1: 0.898 \t\n",
      "\n",
      "Validation 182 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 183 train_loss: 1.074 train_f1: 0.895 \t\n",
      "\n",
      "Validation 183 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 184 train_loss: 1.074 train_f1: 0.897 \t\n",
      "\n",
      "Validation 184 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 185 train_loss: 1.075 train_f1: 0.894 \t\n",
      "\n",
      "Validation 185 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 186 train_loss: 1.070 train_f1: 0.900 \t\n",
      "\n",
      "Validation 186 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 187 train_loss: 1.069 train_f1: 0.898 \t\n",
      "\n",
      "Validation 187 valid_f1: 0.766 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 188 train_loss: 1.064 train_f1: 0.905 \t\n",
      "\n",
      "Validation 188 valid_f1: 0.766 best_f1: 0.771 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 189 train_loss: 1.066 train_f1: 0.898 \t\n",
      "\n",
      "Validation 189 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 190 train_loss: 1.069 train_f1: 0.893 \t\n",
      "\n",
      "Validation 190 valid_f1: 0.761 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 191 train_loss: 1.065 train_f1: 0.896 \t\n",
      "\n",
      "Validation 191 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 192 train_loss: 1.060 train_f1: 0.900 \t\n",
      "\n",
      "Validation 192 valid_f1: 0.761 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 193 train_loss: 1.060 train_f1: 0.903 \t\n",
      "\n",
      "Validation 193 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 194 train_loss: 1.057 train_f1: 0.903 \t\n",
      "\n",
      "Validation 194 valid_f1: 0.759 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 195 train_loss: 1.055 train_f1: 0.902 \t\n",
      "\n",
      "Validation 195 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 196 train_loss: 1.053 train_f1: 0.900 \t\n",
      "\n",
      "Validation 196 valid_f1: 0.762 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 197 train_loss: 1.047 train_f1: 0.909 \t\n",
      "\n",
      "Validation 197 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 198 train_loss: 1.050 train_f1: 0.904 \t\n",
      "\n",
      "Validation 198 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 199 train_loss: 1.049 train_f1: 0.903 \t\n",
      "\n",
      "Validation 199 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 200 train_loss: 1.045 train_f1: 0.907 \t\n",
      "\n",
      "Validation 200 valid_f1: 0.754 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 201 train_loss: 1.044 train_f1: 0.901 \t\n",
      "\n",
      "Validation 201 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 202 train_loss: 1.042 train_f1: 0.907 \t\n",
      "\n",
      "Validation 202 valid_f1: 0.761 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 203 train_loss: 1.041 train_f1: 0.905 \t\n",
      "\n",
      "Validation 203 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 204 train_loss: 1.035 train_f1: 0.910 \t\n",
      "\n",
      "Validation 204 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 205 train_loss: 1.035 train_f1: 0.912 \t\n",
      "\n",
      "Validation 205 valid_f1: 0.770 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 206 train_loss: 1.033 train_f1: 0.913 \t\n",
      "\n",
      "Validation 206 valid_f1: 0.725 best_f1: 0.771 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 207 train_loss: 1.030 train_f1: 0.910 \t\n",
      "\n",
      "Validation 207 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 208 train_loss: 1.030 train_f1: 0.911 \t\n",
      "\n",
      "Validation 208 valid_f1: 0.762 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 209 train_loss: 1.029 train_f1: 0.914 \t\n",
      "\n",
      "Validation 209 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 210 train_loss: 1.023 train_f1: 0.915 \t\n",
      "\n",
      "Validation 210 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 211 train_loss: 1.021 train_f1: 0.915 \t\n",
      "\n",
      "Validation 211 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 212 train_loss: 1.021 train_f1: 0.917 \t\n",
      "\n",
      "Validation 212 valid_f1: 0.754 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 213 train_loss: 1.020 train_f1: 0.917 \t\n",
      "\n",
      "Validation 213 valid_f1: 0.760 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 214 train_loss: 1.019 train_f1: 0.916 \t\n",
      "\n",
      "Validation 214 valid_f1: 0.771 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 215 train_loss: 1.017 train_f1: 0.915 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 215 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 216 train_loss: 1.016 train_f1: 0.915 \t\n",
      "\n",
      "Validation 216 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 217 train_loss: 1.012 train_f1: 0.916 \t\n",
      "\n",
      "Validation 217 valid_f1: 0.762 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 218 train_loss: 1.009 train_f1: 0.920 \t\n",
      "\n",
      "Validation 218 valid_f1: 0.765 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 219 train_loss: 1.010 train_f1: 0.917 \t\n",
      "\n",
      "Validation 219 valid_f1: 0.745 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 220 train_loss: 1.005 train_f1: 0.921 \t\n",
      "\n",
      "Validation 220 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 221 train_loss: 1.007 train_f1: 0.916 \t\n",
      "\n",
      "Validation 221 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 222 train_loss: 1.001 train_f1: 0.921 \t\n",
      "\n",
      "Validation 222 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 223 train_loss: 1.006 train_f1: 0.913 \t\n",
      "\n",
      "Validation 223 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 224 train_loss: 0.998 train_f1: 0.922 \t\n",
      "\n",
      "Validation 224 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 225 train_loss: 0.999 train_f1: 0.918 \t\n",
      "\n",
      "Validation 225 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 226 train_loss: 0.997 train_f1: 0.919 \t\n",
      "\n",
      "Validation 226 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 227 train_loss: 0.995 train_f1: 0.919 \t\n",
      "\n",
      "Validation 227 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 228 train_loss: 0.994 train_f1: 0.922 \t\n",
      "\n",
      "Validation 228 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 229 train_loss: 0.993 train_f1: 0.920 \t\n",
      "\n",
      "Validation 229 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 230 train_loss: 0.993 train_f1: 0.920 \t\n",
      "\n",
      "Validation 230 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 231 train_loss: 0.987 train_f1: 0.924 \t\n",
      "\n",
      "Validation 231 valid_f1: 0.756 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 232 train_loss: 0.987 train_f1: 0.925 \t\n",
      "\n",
      "Validation 232 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 233 train_loss: 0.984 train_f1: 0.925 \t\n",
      "\n",
      "Validation 233 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 234 train_loss: 0.980 train_f1: 0.927 \t\n",
      "\n",
      "Validation 234 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 235 train_loss: 0.980 train_f1: 0.928 \t\n",
      "\n",
      "Validation 235 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 236 train_loss: 0.977 train_f1: 0.928 \t\n",
      "\n",
      "Validation 236 valid_f1: 0.725 best_f1: 0.771 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 237 train_loss: 0.976 train_f1: 0.930 \t\n",
      "\n",
      "Validation 237 valid_f1: 0.745 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 238 train_loss: 0.977 train_f1: 0.924 \t\n",
      "\n",
      "Validation 238 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 239 train_loss: 0.977 train_f1: 0.921 \t\n",
      "\n",
      "Validation 239 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 240 train_loss: 0.971 train_f1: 0.929 \t\n",
      "\n",
      "Validation 240 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 241 train_loss: 0.969 train_f1: 0.931 \t\n",
      "\n",
      "Validation 241 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 242 train_loss: 0.967 train_f1: 0.930 \t\n",
      "\n",
      "Validation 242 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 243 train_loss: 0.963 train_f1: 0.931 \t\n",
      "\n",
      "Validation 243 valid_f1: 0.730 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 244 train_loss: 0.962 train_f1: 0.934 \t\n",
      "\n",
      "Validation 244 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 245 train_loss: 0.964 train_f1: 0.929 \t\n",
      "\n",
      "Validation 245 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 246 train_loss: 0.961 train_f1: 0.930 \t\n",
      "\n",
      "Validation 246 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 247 train_loss: 0.960 train_f1: 0.926 \t\n",
      "\n",
      "Validation 247 valid_f1: 0.733 best_f1: 0.771 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 248 train_loss: 0.957 train_f1: 0.935 \t\n",
      "\n",
      "Validation 248 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 249 train_loss: 0.957 train_f1: 0.933 \t\n",
      "\n",
      "Validation 249 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 250 train_loss: 0.953 train_f1: 0.935 \t\n",
      "\n",
      "Validation 250 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 251 train_loss: 0.952 train_f1: 0.933 \t\n",
      "\n",
      "Validation 251 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 252 train_loss: 0.949 train_f1: 0.936 \t\n",
      "\n",
      "Validation 252 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 253 train_loss: 0.948 train_f1: 0.937 \t\n",
      "\n",
      "Validation 253 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 254 train_loss: 0.946 train_f1: 0.938 \t\n",
      "\n",
      "Validation 254 valid_f1: 0.762 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 255 train_loss: 0.944 train_f1: 0.937 \t\n",
      "\n",
      "Validation 255 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 256 train_loss: 0.946 train_f1: 0.932 \t\n",
      "\n",
      "Validation 256 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 257 train_loss: 0.940 train_f1: 0.937 \t\n",
      "\n",
      "Validation 257 valid_f1: 0.756 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 258 train_loss: 0.941 train_f1: 0.935 \t\n",
      "\n",
      "Validation 258 valid_f1: 0.714 best_f1: 0.771 mean accuracy:0.616 \t\n",
      "\n",
      "Epoch 259 train_loss: 0.938 train_f1: 0.937 \t\n",
      "\n",
      "Validation 259 valid_f1: 0.759 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 260 train_loss: 0.940 train_f1: 0.937 \t\n",
      "\n",
      "Validation 260 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 261 train_loss: 0.940 train_f1: 0.931 \t\n",
      "\n",
      "Validation 261 valid_f1: 0.728 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 262 train_loss: 0.935 train_f1: 0.935 \t\n",
      "\n",
      "Validation 262 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 263 train_loss: 0.932 train_f1: 0.936 \t\n",
      "\n",
      "Validation 263 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 264 train_loss: 0.931 train_f1: 0.940 \t\n",
      "\n",
      "Validation 264 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 265 train_loss: 0.931 train_f1: 0.936 \t\n",
      "\n",
      "Validation 265 valid_f1: 0.732 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 266 train_loss: 0.927 train_f1: 0.941 \t\n",
      "\n",
      "Validation 266 valid_f1: 0.723 best_f1: 0.771 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 267 train_loss: 0.924 train_f1: 0.940 \t\n",
      "\n",
      "Validation 267 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 268 train_loss: 0.923 train_f1: 0.943 \t\n",
      "\n",
      "Validation 268 valid_f1: 0.733 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 269 train_loss: 0.921 train_f1: 0.940 \t\n",
      "\n",
      "Validation 269 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 270 train_loss: 0.923 train_f1: 0.937 \t\n",
      "\n",
      "Validation 270 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 271 train_loss: 0.920 train_f1: 0.939 \t\n",
      "\n",
      "Validation 271 valid_f1: 0.712 best_f1: 0.771 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 272 train_loss: 0.919 train_f1: 0.943 \t\n",
      "\n",
      "Validation 272 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 273 train_loss: 0.915 train_f1: 0.941 \t\n",
      "\n",
      "Validation 273 valid_f1: 0.763 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 274 train_loss: 0.915 train_f1: 0.941 \t\n",
      "\n",
      "Validation 274 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 275 train_loss: 0.911 train_f1: 0.944 \t\n",
      "\n",
      "Validation 275 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 276 train_loss: 0.909 train_f1: 0.944 \t\n",
      "\n",
      "Validation 276 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 277 train_loss: 0.907 train_f1: 0.942 \t\n",
      "\n",
      "Validation 277 valid_f1: 0.727 best_f1: 0.771 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 278 train_loss: 0.907 train_f1: 0.944 \t\n",
      "\n",
      "Validation 278 valid_f1: 0.732 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 279 train_loss: 0.905 train_f1: 0.944 \t\n",
      "\n",
      "Validation 279 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 280 train_loss: 0.904 train_f1: 0.945 \t\n",
      "\n",
      "Validation 280 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 281 train_loss: 0.902 train_f1: 0.945 \t\n",
      "\n",
      "Validation 281 valid_f1: 0.759 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 282 train_loss: 0.902 train_f1: 0.943 \t\n",
      "\n",
      "Validation 282 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 283 train_loss: 0.898 train_f1: 0.945 \t\n",
      "\n",
      "Validation 283 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 284 train_loss: 0.900 train_f1: 0.945 \t\n",
      "\n",
      "Validation 284 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 285 train_loss: 0.892 train_f1: 0.949 \t\n",
      "\n",
      "Validation 285 valid_f1: 0.723 best_f1: 0.771 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 286 train_loss: 0.891 train_f1: 0.949 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 286 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 287 train_loss: 0.893 train_f1: 0.946 \t\n",
      "\n",
      "Validation 287 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 288 train_loss: 0.888 train_f1: 0.950 \t\n",
      "\n",
      "Validation 288 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 289 train_loss: 0.891 train_f1: 0.944 \t\n",
      "\n",
      "Validation 289 valid_f1: 0.745 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 290 train_loss: 0.887 train_f1: 0.946 \t\n",
      "\n",
      "Validation 290 valid_f1: 0.756 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 291 train_loss: 0.885 train_f1: 0.950 \t\n",
      "\n",
      "Validation 291 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 292 train_loss: 0.885 train_f1: 0.946 \t\n",
      "\n",
      "Validation 292 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 293 train_loss: 0.881 train_f1: 0.952 \t\n",
      "\n",
      "Validation 293 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 294 train_loss: 0.879 train_f1: 0.951 \t\n",
      "\n",
      "Validation 294 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 295 train_loss: 0.884 train_f1: 0.941 \t\n",
      "\n",
      "Validation 295 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 296 train_loss: 0.876 train_f1: 0.953 \t\n",
      "\n",
      "Validation 296 valid_f1: 0.756 best_f1: 0.771 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 297 train_loss: 0.875 train_f1: 0.948 \t\n",
      "\n",
      "Validation 297 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 298 train_loss: 0.876 train_f1: 0.948 \t\n",
      "\n",
      "Validation 298 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 299 train_loss: 0.873 train_f1: 0.951 \t\n",
      "\n",
      "Validation 299 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 300 train_loss: 0.873 train_f1: 0.949 \t\n",
      "\n",
      "Validation 300 valid_f1: 0.732 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 301 train_loss: 0.868 train_f1: 0.953 \t\n",
      "\n",
      "Validation 301 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 302 train_loss: 0.871 train_f1: 0.949 \t\n",
      "\n",
      "Validation 302 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 303 train_loss: 0.867 train_f1: 0.952 \t\n",
      "\n",
      "Validation 303 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 304 train_loss: 0.864 train_f1: 0.953 \t\n",
      "\n",
      "Validation 304 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 305 train_loss: 0.859 train_f1: 0.955 \t\n",
      "\n",
      "Validation 305 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 306 train_loss: 0.861 train_f1: 0.953 \t\n",
      "\n",
      "Validation 306 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 307 train_loss: 0.858 train_f1: 0.954 \t\n",
      "\n",
      "Validation 307 valid_f1: 0.727 best_f1: 0.771 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 308 train_loss: 0.856 train_f1: 0.953 \t\n",
      "\n",
      "Validation 308 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 309 train_loss: 0.857 train_f1: 0.955 \t\n",
      "\n",
      "Validation 309 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 310 train_loss: 0.856 train_f1: 0.953 \t\n",
      "\n",
      "Validation 310 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 311 train_loss: 0.853 train_f1: 0.953 \t\n",
      "\n",
      "Validation 311 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 312 train_loss: 0.852 train_f1: 0.956 \t\n",
      "\n",
      "Validation 312 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 313 train_loss: 0.852 train_f1: 0.952 \t\n",
      "\n",
      "Validation 313 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 314 train_loss: 0.848 train_f1: 0.956 \t\n",
      "\n",
      "Validation 314 valid_f1: 0.745 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 315 train_loss: 0.847 train_f1: 0.956 \t\n",
      "\n",
      "Validation 315 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 316 train_loss: 0.843 train_f1: 0.956 \t\n",
      "\n",
      "Validation 316 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 317 train_loss: 0.845 train_f1: 0.955 \t\n",
      "\n",
      "Validation 317 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 318 train_loss: 0.842 train_f1: 0.957 \t\n",
      "\n",
      "Validation 318 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 319 train_loss: 0.841 train_f1: 0.957 \t\n",
      "\n",
      "Validation 319 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 320 train_loss: 0.837 train_f1: 0.960 \t\n",
      "\n",
      "Validation 320 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 321 train_loss: 0.840 train_f1: 0.953 \t\n",
      "\n",
      "Validation 321 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 322 train_loss: 0.839 train_f1: 0.957 \t\n",
      "\n",
      "Validation 322 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 323 train_loss: 0.833 train_f1: 0.960 \t\n",
      "\n",
      "Validation 323 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 324 train_loss: 0.830 train_f1: 0.960 \t\n",
      "\n",
      "Validation 324 valid_f1: 0.757 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 325 train_loss: 0.833 train_f1: 0.958 \t\n",
      "\n",
      "Validation 325 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 326 train_loss: 0.827 train_f1: 0.963 \t\n",
      "\n",
      "Validation 326 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 327 train_loss: 0.831 train_f1: 0.953 \t\n",
      "\n",
      "Validation 327 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 328 train_loss: 0.826 train_f1: 0.962 \t\n",
      "\n",
      "Validation 328 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 329 train_loss: 0.824 train_f1: 0.961 \t\n",
      "\n",
      "Validation 329 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 330 train_loss: 0.824 train_f1: 0.959 \t\n",
      "\n",
      "Validation 330 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 331 train_loss: 0.823 train_f1: 0.960 \t\n",
      "\n",
      "Validation 331 valid_f1: 0.717 best_f1: 0.771 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 332 train_loss: 0.821 train_f1: 0.957 \t\n",
      "\n",
      "Validation 332 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 333 train_loss: 0.817 train_f1: 0.962 \t\n",
      "\n",
      "Validation 333 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 334 train_loss: 0.819 train_f1: 0.960 \t\n",
      "\n",
      "Validation 334 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 335 train_loss: 0.818 train_f1: 0.959 \t\n",
      "\n",
      "Validation 335 valid_f1: 0.732 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 336 train_loss: 0.814 train_f1: 0.958 \t\n",
      "\n",
      "Validation 336 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 337 train_loss: 0.816 train_f1: 0.959 \t\n",
      "\n",
      "Validation 337 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 338 train_loss: 0.810 train_f1: 0.961 \t\n",
      "\n",
      "Validation 338 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 339 train_loss: 0.810 train_f1: 0.961 \t\n",
      "\n",
      "Validation 339 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 340 train_loss: 0.811 train_f1: 0.957 \t\n",
      "\n",
      "Validation 340 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 341 train_loss: 0.809 train_f1: 0.957 \t\n",
      "\n",
      "Validation 341 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 342 train_loss: 0.807 train_f1: 0.962 \t\n",
      "\n",
      "Validation 342 valid_f1: 0.728 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 343 train_loss: 0.802 train_f1: 0.964 \t\n",
      "\n",
      "Validation 343 valid_f1: 0.745 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 344 train_loss: 0.803 train_f1: 0.960 \t\n",
      "\n",
      "Validation 344 valid_f1: 0.726 best_f1: 0.771 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 345 train_loss: 0.800 train_f1: 0.963 \t\n",
      "\n",
      "Validation 345 valid_f1: 0.754 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 346 train_loss: 0.802 train_f1: 0.962 \t\n",
      "\n",
      "Validation 346 valid_f1: 0.757 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 347 train_loss: 0.800 train_f1: 0.961 \t\n",
      "\n",
      "Validation 347 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 348 train_loss: 0.795 train_f1: 0.965 \t\n",
      "\n",
      "Validation 348 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 349 train_loss: 0.795 train_f1: 0.962 \t\n",
      "\n",
      "Validation 349 valid_f1: 0.723 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 350 train_loss: 0.795 train_f1: 0.960 \t\n",
      "\n",
      "Validation 350 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 351 train_loss: 0.791 train_f1: 0.962 \t\n",
      "\n",
      "Validation 351 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 352 train_loss: 0.793 train_f1: 0.962 \t\n",
      "\n",
      "Validation 352 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 353 train_loss: 0.798 train_f1: 0.956 \t\n",
      "\n",
      "Validation 353 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 354 train_loss: 0.788 train_f1: 0.965 \t\n",
      "\n",
      "Validation 354 valid_f1: 0.731 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 355 train_loss: 0.788 train_f1: 0.965 \t\n",
      "\n",
      "Validation 355 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 356 train_loss: 0.787 train_f1: 0.964 \t\n",
      "\n",
      "Validation 356 valid_f1: 0.732 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 357 train_loss: 0.780 train_f1: 0.968 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 357 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 358 train_loss: 0.780 train_f1: 0.966 \t\n",
      "\n",
      "Validation 358 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 359 train_loss: 0.779 train_f1: 0.964 \t\n",
      "\n",
      "Validation 359 valid_f1: 0.754 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 360 train_loss: 0.779 train_f1: 0.964 \t\n",
      "\n",
      "Validation 360 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 361 train_loss: 0.778 train_f1: 0.963 \t\n",
      "\n",
      "Validation 361 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 362 train_loss: 0.776 train_f1: 0.965 \t\n",
      "\n",
      "Validation 362 valid_f1: 0.767 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 363 train_loss: 0.775 train_f1: 0.963 \t\n",
      "\n",
      "Validation 363 valid_f1: 0.765 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 364 train_loss: 0.776 train_f1: 0.964 \t\n",
      "\n",
      "Validation 364 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 365 train_loss: 0.774 train_f1: 0.966 \t\n",
      "\n",
      "Validation 365 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 366 train_loss: 0.772 train_f1: 0.962 \t\n",
      "\n",
      "Validation 366 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 367 train_loss: 0.769 train_f1: 0.967 \t\n",
      "\n",
      "Validation 367 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 368 train_loss: 0.767 train_f1: 0.966 \t\n",
      "\n",
      "Validation 368 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 369 train_loss: 0.767 train_f1: 0.966 \t\n",
      "\n",
      "Validation 369 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 370 train_loss: 0.767 train_f1: 0.966 \t\n",
      "\n",
      "Validation 370 valid_f1: 0.754 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 371 train_loss: 0.765 train_f1: 0.967 \t\n",
      "\n",
      "Validation 371 valid_f1: 0.762 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 372 train_loss: 0.763 train_f1: 0.967 \t\n",
      "\n",
      "Validation 372 valid_f1: 0.730 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 373 train_loss: 0.760 train_f1: 0.969 \t\n",
      "\n",
      "Validation 373 valid_f1: 0.745 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 374 train_loss: 0.757 train_f1: 0.969 \t\n",
      "\n",
      "Validation 374 valid_f1: 0.745 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 375 train_loss: 0.758 train_f1: 0.966 \t\n",
      "\n",
      "Validation 375 valid_f1: 0.731 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 376 train_loss: 0.757 train_f1: 0.966 \t\n",
      "\n",
      "Validation 376 valid_f1: 0.766 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 377 train_loss: 0.757 train_f1: 0.965 \t\n",
      "\n",
      "Validation 377 valid_f1: 0.732 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 378 train_loss: 0.755 train_f1: 0.967 \t\n",
      "\n",
      "Validation 378 valid_f1: 0.754 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 379 train_loss: 0.751 train_f1: 0.967 \t\n",
      "\n",
      "Validation 379 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 380 train_loss: 0.750 train_f1: 0.970 \t\n",
      "\n",
      "Validation 380 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 381 train_loss: 0.750 train_f1: 0.967 \t\n",
      "\n",
      "Validation 381 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 382 train_loss: 0.746 train_f1: 0.969 \t\n",
      "\n",
      "Validation 382 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 383 train_loss: 0.747 train_f1: 0.967 \t\n",
      "\n",
      "Validation 383 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 384 train_loss: 0.744 train_f1: 0.970 \t\n",
      "\n",
      "Validation 384 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 385 train_loss: 0.742 train_f1: 0.970 \t\n",
      "\n",
      "Validation 385 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 386 train_loss: 0.742 train_f1: 0.970 \t\n",
      "\n",
      "Validation 386 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 387 train_loss: 0.740 train_f1: 0.969 \t\n",
      "\n",
      "Validation 387 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 388 train_loss: 0.739 train_f1: 0.969 \t\n",
      "\n",
      "Validation 388 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 389 train_loss: 0.737 train_f1: 0.970 \t\n",
      "\n",
      "Validation 389 valid_f1: 0.761 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 390 train_loss: 0.738 train_f1: 0.968 \t\n",
      "\n",
      "Validation 390 valid_f1: 0.728 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 391 train_loss: 0.732 train_f1: 0.973 \t\n",
      "\n",
      "Validation 391 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 392 train_loss: 0.731 train_f1: 0.971 \t\n",
      "\n",
      "Validation 392 valid_f1: 0.725 best_f1: 0.771 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 393 train_loss: 0.731 train_f1: 0.971 \t\n",
      "\n",
      "Validation 393 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 394 train_loss: 0.730 train_f1: 0.973 \t\n",
      "\n",
      "Validation 394 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 395 train_loss: 0.725 train_f1: 0.972 \t\n",
      "\n",
      "Validation 395 valid_f1: 0.760 best_f1: 0.771 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 396 train_loss: 0.727 train_f1: 0.971 \t\n",
      "\n",
      "Validation 396 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 397 train_loss: 0.725 train_f1: 0.972 \t\n",
      "\n",
      "Validation 397 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 398 train_loss: 0.723 train_f1: 0.972 \t\n",
      "\n",
      "Validation 398 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 399 train_loss: 0.727 train_f1: 0.969 \t\n",
      "\n",
      "Validation 399 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 400 train_loss: 0.721 train_f1: 0.975 \t\n",
      "\n",
      "Validation 400 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 401 train_loss: 0.721 train_f1: 0.972 \t\n",
      "\n",
      "Validation 401 valid_f1: 0.727 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 402 train_loss: 0.715 train_f1: 0.974 \t\n",
      "\n",
      "Validation 402 valid_f1: 0.761 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 403 train_loss: 0.714 train_f1: 0.977 \t\n",
      "\n",
      "Validation 403 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 404 train_loss: 0.719 train_f1: 0.970 \t\n",
      "\n",
      "Validation 404 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 405 train_loss: 0.718 train_f1: 0.967 \t\n",
      "\n",
      "Validation 405 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 406 train_loss: 0.713 train_f1: 0.972 \t\n",
      "\n",
      "Validation 406 valid_f1: 0.754 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 407 train_loss: 0.714 train_f1: 0.972 \t\n",
      "\n",
      "Validation 407 valid_f1: 0.729 best_f1: 0.771 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 408 train_loss: 0.710 train_f1: 0.973 \t\n",
      "\n",
      "Validation 408 valid_f1: 0.726 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 409 train_loss: 0.707 train_f1: 0.974 \t\n",
      "\n",
      "Validation 409 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 410 train_loss: 0.707 train_f1: 0.974 \t\n",
      "\n",
      "Validation 410 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 411 train_loss: 0.710 train_f1: 0.970 \t\n",
      "\n",
      "Validation 411 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 412 train_loss: 0.703 train_f1: 0.976 \t\n",
      "\n",
      "Validation 412 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 413 train_loss: 0.704 train_f1: 0.972 \t\n",
      "\n",
      "Validation 413 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 414 train_loss: 0.705 train_f1: 0.971 \t\n",
      "\n",
      "Validation 414 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 415 train_loss: 0.704 train_f1: 0.972 \t\n",
      "\n",
      "Validation 415 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 416 train_loss: 0.698 train_f1: 0.973 \t\n",
      "\n",
      "Validation 416 valid_f1: 0.745 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 417 train_loss: 0.699 train_f1: 0.973 \t\n",
      "\n",
      "Validation 417 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 418 train_loss: 0.696 train_f1: 0.974 \t\n",
      "\n",
      "Validation 418 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 419 train_loss: 0.696 train_f1: 0.974 \t\n",
      "\n",
      "Validation 419 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 420 train_loss: 0.693 train_f1: 0.975 \t\n",
      "\n",
      "Validation 420 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 421 train_loss: 0.693 train_f1: 0.972 \t\n",
      "\n",
      "Validation 421 valid_f1: 0.707 best_f1: 0.771 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 422 train_loss: 0.693 train_f1: 0.973 \t\n",
      "\n",
      "Validation 422 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 423 train_loss: 0.691 train_f1: 0.972 \t\n",
      "\n",
      "Validation 423 valid_f1: 0.762 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 424 train_loss: 0.689 train_f1: 0.975 \t\n",
      "\n",
      "Validation 424 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 425 train_loss: 0.689 train_f1: 0.975 \t\n",
      "\n",
      "Validation 425 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 426 train_loss: 0.683 train_f1: 0.977 \t\n",
      "\n",
      "Validation 426 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 427 train_loss: 0.684 train_f1: 0.976 \t\n",
      "\n",
      "Validation 427 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 428 train_loss: 0.683 train_f1: 0.974 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 428 valid_f1: 0.754 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 429 train_loss: 0.679 train_f1: 0.977 \t\n",
      "\n",
      "Validation 429 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 430 train_loss: 0.679 train_f1: 0.977 \t\n",
      "\n",
      "Validation 430 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 431 train_loss: 0.680 train_f1: 0.973 \t\n",
      "\n",
      "Validation 431 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 432 train_loss: 0.674 train_f1: 0.980 \t\n",
      "\n",
      "Validation 432 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 433 train_loss: 0.680 train_f1: 0.970 \t\n",
      "\n",
      "Validation 433 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 434 train_loss: 0.678 train_f1: 0.975 \t\n",
      "\n",
      "Validation 434 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 435 train_loss: 0.675 train_f1: 0.973 \t\n",
      "\n",
      "Validation 435 valid_f1: 0.728 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 436 train_loss: 0.671 train_f1: 0.977 \t\n",
      "\n",
      "Validation 436 valid_f1: 0.733 best_f1: 0.771 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 437 train_loss: 0.673 train_f1: 0.975 \t\n",
      "\n",
      "Validation 437 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 438 train_loss: 0.669 train_f1: 0.978 \t\n",
      "\n",
      "Validation 438 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 439 train_loss: 0.668 train_f1: 0.977 \t\n",
      "\n",
      "Validation 439 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 440 train_loss: 0.665 train_f1: 0.976 \t\n",
      "\n",
      "Validation 440 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 441 train_loss: 0.660 train_f1: 0.981 \t\n",
      "\n",
      "Validation 441 valid_f1: 0.761 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 442 train_loss: 0.666 train_f1: 0.976 \t\n",
      "\n",
      "Validation 442 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 443 train_loss: 0.666 train_f1: 0.974 \t\n",
      "\n",
      "Validation 443 valid_f1: 0.722 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 444 train_loss: 0.665 train_f1: 0.973 \t\n",
      "\n",
      "Validation 444 valid_f1: 0.756 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 445 train_loss: 0.660 train_f1: 0.976 \t\n",
      "\n",
      "Validation 445 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 446 train_loss: 0.660 train_f1: 0.976 \t\n",
      "\n",
      "Validation 446 valid_f1: 0.729 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 447 train_loss: 0.659 train_f1: 0.976 \t\n",
      "\n",
      "Validation 447 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 448 train_loss: 0.658 train_f1: 0.975 \t\n",
      "\n",
      "Validation 448 valid_f1: 0.732 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 449 train_loss: 0.657 train_f1: 0.977 \t\n",
      "\n",
      "Validation 449 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 450 train_loss: 0.658 train_f1: 0.976 \t\n",
      "\n",
      "Validation 450 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 451 train_loss: 0.654 train_f1: 0.977 \t\n",
      "\n",
      "Validation 451 valid_f1: 0.718 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 452 train_loss: 0.651 train_f1: 0.976 \t\n",
      "\n",
      "Validation 452 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 453 train_loss: 0.650 train_f1: 0.978 \t\n",
      "\n",
      "Validation 453 valid_f1: 0.760 best_f1: 0.771 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 454 train_loss: 0.653 train_f1: 0.976 \t\n",
      "\n",
      "Validation 454 valid_f1: 0.713 best_f1: 0.771 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 455 train_loss: 0.650 train_f1: 0.976 \t\n",
      "\n",
      "Validation 455 valid_f1: 0.731 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 456 train_loss: 0.647 train_f1: 0.978 \t\n",
      "\n",
      "Validation 456 valid_f1: 0.720 best_f1: 0.771 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 457 train_loss: 0.644 train_f1: 0.979 \t\n",
      "\n",
      "Validation 457 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 458 train_loss: 0.647 train_f1: 0.975 \t\n",
      "\n",
      "Validation 458 valid_f1: 0.730 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 459 train_loss: 0.642 train_f1: 0.975 \t\n",
      "\n",
      "Validation 459 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 460 train_loss: 0.641 train_f1: 0.980 \t\n",
      "\n",
      "Validation 460 valid_f1: 0.745 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 461 train_loss: 0.639 train_f1: 0.977 \t\n",
      "\n",
      "Validation 461 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 462 train_loss: 0.640 train_f1: 0.979 \t\n",
      "\n",
      "Validation 462 valid_f1: 0.718 best_f1: 0.771 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 463 train_loss: 0.637 train_f1: 0.978 \t\n",
      "\n",
      "Validation 463 valid_f1: 0.733 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 464 train_loss: 0.642 train_f1: 0.974 \t\n",
      "\n",
      "Validation 464 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 465 train_loss: 0.636 train_f1: 0.980 \t\n",
      "\n",
      "Validation 465 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 466 train_loss: 0.632 train_f1: 0.980 \t\n",
      "\n",
      "Validation 466 valid_f1: 0.745 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 467 train_loss: 0.632 train_f1: 0.981 \t\n",
      "\n",
      "Validation 467 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 468 train_loss: 0.630 train_f1: 0.979 \t\n",
      "\n",
      "Validation 468 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 469 train_loss: 0.632 train_f1: 0.976 \t\n",
      "\n",
      "Validation 469 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 470 train_loss: 0.635 train_f1: 0.976 \t\n",
      "\n",
      "Validation 470 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 471 train_loss: 0.628 train_f1: 0.979 \t\n",
      "\n",
      "Validation 471 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 472 train_loss: 0.626 train_f1: 0.977 \t\n",
      "\n",
      "Validation 472 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 473 train_loss: 0.623 train_f1: 0.981 \t\n",
      "\n",
      "Validation 473 valid_f1: 0.729 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 474 train_loss: 0.624 train_f1: 0.977 \t\n",
      "\n",
      "Validation 474 valid_f1: 0.732 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 475 train_loss: 0.620 train_f1: 0.981 \t\n",
      "\n",
      "Validation 475 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 476 train_loss: 0.621 train_f1: 0.979 \t\n",
      "\n",
      "Validation 476 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 477 train_loss: 0.619 train_f1: 0.981 \t\n",
      "\n",
      "Validation 477 valid_f1: 0.731 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 478 train_loss: 0.622 train_f1: 0.978 \t\n",
      "\n",
      "Validation 478 valid_f1: 0.734 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 479 train_loss: 0.617 train_f1: 0.981 \t\n",
      "\n",
      "Validation 479 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 480 train_loss: 0.620 train_f1: 0.973 \t\n",
      "\n",
      "Validation 480 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 481 train_loss: 0.620 train_f1: 0.976 \t\n",
      "\n",
      "Validation 481 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 482 train_loss: 0.613 train_f1: 0.980 \t\n",
      "\n",
      "Validation 482 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 483 train_loss: 0.614 train_f1: 0.980 \t\n",
      "\n",
      "Validation 483 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 484 train_loss: 0.612 train_f1: 0.979 \t\n",
      "\n",
      "Validation 484 valid_f1: 0.726 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 485 train_loss: 0.611 train_f1: 0.978 \t\n",
      "\n",
      "Validation 485 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 486 train_loss: 0.608 train_f1: 0.981 \t\n",
      "\n",
      "Validation 486 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 487 train_loss: 0.609 train_f1: 0.978 \t\n",
      "\n",
      "Validation 487 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 488 train_loss: 0.610 train_f1: 0.975 \t\n",
      "\n",
      "Validation 488 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 489 train_loss: 0.607 train_f1: 0.977 \t\n",
      "\n",
      "Validation 489 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 490 train_loss: 0.603 train_f1: 0.977 \t\n",
      "\n",
      "Validation 490 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 491 train_loss: 0.606 train_f1: 0.979 \t\n",
      "\n",
      "Validation 491 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 492 train_loss: 0.609 train_f1: 0.975 \t\n",
      "\n",
      "Validation 492 valid_f1: 0.730 best_f1: 0.771 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 493 train_loss: 0.598 train_f1: 0.981 \t\n",
      "\n",
      "Validation 493 valid_f1: 0.719 best_f1: 0.771 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 494 train_loss: 0.600 train_f1: 0.982 \t\n",
      "\n",
      "Validation 494 valid_f1: 0.722 best_f1: 0.771 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 495 train_loss: 0.594 train_f1: 0.984 \t\n",
      "\n",
      "Validation 495 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 496 train_loss: 0.598 train_f1: 0.980 \t\n",
      "\n",
      "Validation 496 valid_f1: 0.726 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 497 train_loss: 0.598 train_f1: 0.977 \t\n",
      "\n",
      "Validation 497 valid_f1: 0.732 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 498 train_loss: 0.594 train_f1: 0.982 \t\n",
      "\n",
      "Validation 498 valid_f1: 0.714 best_f1: 0.771 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 499 train_loss: 0.592 train_f1: 0.982 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 499 valid_f1: 0.722 best_f1: 0.771 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 500 train_loss: 0.595 train_f1: 0.976 \t\n",
      "\n",
      "Validation 500 valid_f1: 0.732 best_f1: 0.771 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 501 train_loss: 0.589 train_f1: 0.981 \t\n",
      "\n",
      "Validation 501 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 502 train_loss: 0.591 train_f1: 0.978 \t\n",
      "\n",
      "Validation 502 valid_f1: 0.732 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 503 train_loss: 0.590 train_f1: 0.978 \t\n",
      "\n",
      "Validation 503 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 504 train_loss: 0.585 train_f1: 0.982 \t\n",
      "\n",
      "Validation 504 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 505 train_loss: 0.583 train_f1: 0.979 \t\n",
      "\n",
      "Validation 505 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 506 train_loss: 0.583 train_f1: 0.983 \t\n",
      "\n",
      "Validation 506 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 507 train_loss: 0.578 train_f1: 0.983 \t\n",
      "\n",
      "Validation 507 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 508 train_loss: 0.582 train_f1: 0.978 \t\n",
      "\n",
      "Validation 508 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 509 train_loss: 0.580 train_f1: 0.983 \t\n",
      "\n",
      "Validation 509 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 510 train_loss: 0.578 train_f1: 0.981 \t\n",
      "\n",
      "Validation 510 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 511 train_loss: 0.580 train_f1: 0.980 \t\n",
      "\n",
      "Validation 511 valid_f1: 0.713 best_f1: 0.771 mean accuracy:0.616 \t\n",
      "\n",
      "Epoch 512 train_loss: 0.576 train_f1: 0.983 \t\n",
      "\n",
      "Validation 512 valid_f1: 0.722 best_f1: 0.771 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 513 train_loss: 0.577 train_f1: 0.979 \t\n",
      "\n",
      "Validation 513 valid_f1: 0.730 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 514 train_loss: 0.573 train_f1: 0.980 \t\n",
      "\n",
      "Validation 514 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 515 train_loss: 0.572 train_f1: 0.984 \t\n",
      "\n",
      "Validation 515 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 516 train_loss: 0.569 train_f1: 0.983 \t\n",
      "\n",
      "Validation 516 valid_f1: 0.730 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 517 train_loss: 0.569 train_f1: 0.982 \t\n",
      "\n",
      "Validation 517 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 518 train_loss: 0.570 train_f1: 0.979 \t\n",
      "\n",
      "Validation 518 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 519 train_loss: 0.571 train_f1: 0.980 \t\n",
      "\n",
      "Validation 519 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 520 train_loss: 0.567 train_f1: 0.982 \t\n",
      "\n",
      "Validation 520 valid_f1: 0.725 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 521 train_loss: 0.564 train_f1: 0.983 \t\n",
      "\n",
      "Validation 521 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 522 train_loss: 0.560 train_f1: 0.985 \t\n",
      "\n",
      "Validation 522 valid_f1: 0.734 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 523 train_loss: 0.566 train_f1: 0.980 \t\n",
      "\n",
      "Validation 523 valid_f1: 0.694 best_f1: 0.771 mean accuracy:0.598 \t\n",
      "\n",
      "Epoch 524 train_loss: 0.564 train_f1: 0.982 \t\n",
      "\n",
      "Validation 524 valid_f1: 0.733 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 525 train_loss: 0.562 train_f1: 0.981 \t\n",
      "\n",
      "Validation 525 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 526 train_loss: 0.562 train_f1: 0.978 \t\n",
      "\n",
      "Validation 526 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 527 train_loss: 0.557 train_f1: 0.983 \t\n",
      "\n",
      "Validation 527 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 528 train_loss: 0.556 train_f1: 0.984 \t\n",
      "\n",
      "Validation 528 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 529 train_loss: 0.553 train_f1: 0.985 \t\n",
      "\n",
      "Validation 529 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 530 train_loss: 0.554 train_f1: 0.983 \t\n",
      "\n",
      "Validation 530 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 531 train_loss: 0.556 train_f1: 0.983 \t\n",
      "\n",
      "Validation 531 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 532 train_loss: 0.557 train_f1: 0.980 \t\n",
      "\n",
      "Validation 532 valid_f1: 0.734 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 533 train_loss: 0.553 train_f1: 0.982 \t\n",
      "\n",
      "Validation 533 valid_f1: 0.721 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 534 train_loss: 0.549 train_f1: 0.983 \t\n",
      "\n",
      "Validation 534 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 535 train_loss: 0.547 train_f1: 0.983 \t\n",
      "\n",
      "Validation 535 valid_f1: 0.729 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 536 train_loss: 0.547 train_f1: 0.982 \t\n",
      "\n",
      "Validation 536 valid_f1: 0.727 best_f1: 0.771 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 537 train_loss: 0.543 train_f1: 0.983 \t\n",
      "\n",
      "Validation 537 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 538 train_loss: 0.546 train_f1: 0.983 \t\n",
      "\n",
      "Validation 538 valid_f1: 0.765 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 539 train_loss: 0.546 train_f1: 0.984 \t\n",
      "\n",
      "Validation 539 valid_f1: 0.745 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 540 train_loss: 0.543 train_f1: 0.982 \t\n",
      "\n",
      "Validation 540 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 541 train_loss: 0.543 train_f1: 0.982 \t\n",
      "\n",
      "Validation 541 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 542 train_loss: 0.538 train_f1: 0.986 \t\n",
      "\n",
      "Validation 542 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 543 train_loss: 0.541 train_f1: 0.982 \t\n",
      "\n",
      "Validation 543 valid_f1: 0.734 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 544 train_loss: 0.538 train_f1: 0.984 \t\n",
      "\n",
      "Validation 544 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 545 train_loss: 0.539 train_f1: 0.982 \t\n",
      "\n",
      "Validation 545 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 546 train_loss: 0.537 train_f1: 0.982 \t\n",
      "\n",
      "Validation 546 valid_f1: 0.731 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 547 train_loss: 0.536 train_f1: 0.981 \t\n",
      "\n",
      "Validation 547 valid_f1: 0.720 best_f1: 0.771 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 548 train_loss: 0.535 train_f1: 0.983 \t\n",
      "\n",
      "Validation 548 valid_f1: 0.727 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 549 train_loss: 0.535 train_f1: 0.979 \t\n",
      "\n",
      "Validation 549 valid_f1: 0.717 best_f1: 0.771 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 550 train_loss: 0.537 train_f1: 0.979 \t\n",
      "\n",
      "Validation 550 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 551 train_loss: 0.530 train_f1: 0.985 \t\n",
      "\n",
      "Validation 551 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 552 train_loss: 0.529 train_f1: 0.983 \t\n",
      "\n",
      "Validation 552 valid_f1: 0.721 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 553 train_loss: 0.526 train_f1: 0.985 \t\n",
      "\n",
      "Validation 553 valid_f1: 0.754 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 554 train_loss: 0.530 train_f1: 0.981 \t\n",
      "\n",
      "Validation 554 valid_f1: 0.732 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 555 train_loss: 0.527 train_f1: 0.982 \t\n",
      "\n",
      "Validation 555 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 556 train_loss: 0.526 train_f1: 0.982 \t\n",
      "\n",
      "Validation 556 valid_f1: 0.730 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 557 train_loss: 0.522 train_f1: 0.985 \t\n",
      "\n",
      "Validation 557 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 558 train_loss: 0.525 train_f1: 0.984 \t\n",
      "\n",
      "Validation 558 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 559 train_loss: 0.520 train_f1: 0.984 \t\n",
      "\n",
      "Validation 559 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 560 train_loss: 0.520 train_f1: 0.985 \t\n",
      "\n",
      "Validation 560 valid_f1: 0.723 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 561 train_loss: 0.518 train_f1: 0.985 \t\n",
      "\n",
      "Validation 561 valid_f1: 0.745 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 562 train_loss: 0.520 train_f1: 0.981 \t\n",
      "\n",
      "Validation 562 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 563 train_loss: 0.518 train_f1: 0.985 \t\n",
      "\n",
      "Validation 563 valid_f1: 0.722 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 564 train_loss: 0.518 train_f1: 0.983 \t\n",
      "\n",
      "Validation 564 valid_f1: 0.761 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 565 train_loss: 0.517 train_f1: 0.981 \t\n",
      "\n",
      "Validation 565 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 566 train_loss: 0.513 train_f1: 0.985 \t\n",
      "\n",
      "Validation 566 valid_f1: 0.734 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 567 train_loss: 0.516 train_f1: 0.981 \t\n",
      "\n",
      "Validation 567 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 568 train_loss: 0.515 train_f1: 0.981 \t\n",
      "\n",
      "Validation 568 valid_f1: 0.761 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 569 train_loss: 0.509 train_f1: 0.984 \t\n",
      "\n",
      "Validation 569 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 570 train_loss: 0.515 train_f1: 0.980 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 570 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 571 train_loss: 0.508 train_f1: 0.983 \t\n",
      "\n",
      "Validation 571 valid_f1: 0.732 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 572 train_loss: 0.510 train_f1: 0.981 \t\n",
      "\n",
      "Validation 572 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 573 train_loss: 0.505 train_f1: 0.983 \t\n",
      "\n",
      "Validation 573 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 574 train_loss: 0.502 train_f1: 0.986 \t\n",
      "\n",
      "Validation 574 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 575 train_loss: 0.502 train_f1: 0.984 \t\n",
      "\n",
      "Validation 575 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 576 train_loss: 0.505 train_f1: 0.980 \t\n",
      "\n",
      "Validation 576 valid_f1: 0.734 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 577 train_loss: 0.505 train_f1: 0.983 \t\n",
      "\n",
      "Validation 577 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 578 train_loss: 0.503 train_f1: 0.983 \t\n",
      "\n",
      "Validation 578 valid_f1: 0.757 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 579 train_loss: 0.501 train_f1: 0.982 \t\n",
      "\n",
      "Validation 579 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 580 train_loss: 0.498 train_f1: 0.987 \t\n",
      "\n",
      "Validation 580 valid_f1: 0.733 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 581 train_loss: 0.499 train_f1: 0.986 \t\n",
      "\n",
      "Validation 581 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 582 train_loss: 0.498 train_f1: 0.983 \t\n",
      "\n",
      "Validation 582 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 583 train_loss: 0.495 train_f1: 0.986 \t\n",
      "\n",
      "Validation 583 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 584 train_loss: 0.497 train_f1: 0.982 \t\n",
      "\n",
      "Validation 584 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 585 train_loss: 0.496 train_f1: 0.982 \t\n",
      "\n",
      "Validation 585 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 586 train_loss: 0.491 train_f1: 0.985 \t\n",
      "\n",
      "Validation 586 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 587 train_loss: 0.490 train_f1: 0.986 \t\n",
      "\n",
      "Validation 587 valid_f1: 0.767 best_f1: 0.771 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 588 train_loss: 0.491 train_f1: 0.983 \t\n",
      "\n",
      "Validation 588 valid_f1: 0.765 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 589 train_loss: 0.486 train_f1: 0.983 \t\n",
      "\n",
      "Validation 589 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 590 train_loss: 0.487 train_f1: 0.985 \t\n",
      "\n",
      "Validation 590 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 591 train_loss: 0.489 train_f1: 0.984 \t\n",
      "\n",
      "Validation 591 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 592 train_loss: 0.483 train_f1: 0.987 \t\n",
      "\n",
      "Validation 592 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 593 train_loss: 0.486 train_f1: 0.984 \t\n",
      "\n",
      "Validation 593 valid_f1: 0.761 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 594 train_loss: 0.489 train_f1: 0.981 \t\n",
      "\n",
      "Validation 594 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 595 train_loss: 0.481 train_f1: 0.985 \t\n",
      "\n",
      "Validation 595 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 596 train_loss: 0.479 train_f1: 0.986 \t\n",
      "\n",
      "Validation 596 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 597 train_loss: 0.485 train_f1: 0.984 \t\n",
      "\n",
      "Validation 597 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 598 train_loss: 0.482 train_f1: 0.980 \t\n",
      "\n",
      "Validation 598 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 599 train_loss: 0.479 train_f1: 0.984 \t\n",
      "\n",
      "Validation 599 valid_f1: 0.718 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 600 train_loss: 0.479 train_f1: 0.983 \t\n",
      "\n",
      "Validation 600 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 601 train_loss: 0.476 train_f1: 0.984 \t\n",
      "\n",
      "Validation 601 valid_f1: 0.733 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 602 train_loss: 0.471 train_f1: 0.987 \t\n",
      "\n",
      "Validation 602 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 603 train_loss: 0.472 train_f1: 0.988 \t\n",
      "\n",
      "Validation 603 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 604 train_loss: 0.473 train_f1: 0.985 \t\n",
      "\n",
      "Validation 604 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 605 train_loss: 0.473 train_f1: 0.985 \t\n",
      "\n",
      "Validation 605 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 606 train_loss: 0.468 train_f1: 0.985 \t\n",
      "\n",
      "Validation 606 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 607 train_loss: 0.471 train_f1: 0.986 \t\n",
      "\n",
      "Validation 607 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 608 train_loss: 0.468 train_f1: 0.986 \t\n",
      "\n",
      "Validation 608 valid_f1: 0.733 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 609 train_loss: 0.466 train_f1: 0.985 \t\n",
      "\n",
      "Validation 609 valid_f1: 0.733 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 610 train_loss: 0.466 train_f1: 0.983 \t\n",
      "\n",
      "Validation 610 valid_f1: 0.757 best_f1: 0.771 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 611 train_loss: 0.461 train_f1: 0.987 \t\n",
      "\n",
      "Validation 611 valid_f1: 0.731 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 612 train_loss: 0.462 train_f1: 0.987 \t\n",
      "\n",
      "Validation 612 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 613 train_loss: 0.463 train_f1: 0.983 \t\n",
      "\n",
      "Validation 613 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 614 train_loss: 0.465 train_f1: 0.982 \t\n",
      "\n",
      "Validation 614 valid_f1: 0.762 best_f1: 0.771 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 615 train_loss: 0.460 train_f1: 0.985 \t\n",
      "\n",
      "Validation 615 valid_f1: 0.733 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 616 train_loss: 0.459 train_f1: 0.986 \t\n",
      "\n",
      "Validation 616 valid_f1: 0.756 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 617 train_loss: 0.459 train_f1: 0.984 \t\n",
      "\n",
      "Validation 617 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 618 train_loss: 0.459 train_f1: 0.985 \t\n",
      "\n",
      "Validation 618 valid_f1: 0.757 best_f1: 0.771 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 619 train_loss: 0.457 train_f1: 0.988 \t\n",
      "\n",
      "Validation 619 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 620 train_loss: 0.455 train_f1: 0.987 \t\n",
      "\n",
      "Validation 620 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 621 train_loss: 0.454 train_f1: 0.986 \t\n",
      "\n",
      "Validation 621 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 622 train_loss: 0.453 train_f1: 0.986 \t\n",
      "\n",
      "Validation 622 valid_f1: 0.733 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 623 train_loss: 0.453 train_f1: 0.987 \t\n",
      "\n",
      "Validation 623 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 624 train_loss: 0.452 train_f1: 0.984 \t\n",
      "\n",
      "Validation 624 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 625 train_loss: 0.453 train_f1: 0.985 \t\n",
      "\n",
      "Validation 625 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 626 train_loss: 0.452 train_f1: 0.986 \t\n",
      "\n",
      "Validation 626 valid_f1: 0.760 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 627 train_loss: 0.448 train_f1: 0.985 \t\n",
      "\n",
      "Validation 627 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 628 train_loss: 0.446 train_f1: 0.986 \t\n",
      "\n",
      "Validation 628 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 629 train_loss: 0.445 train_f1: 0.986 \t\n",
      "\n",
      "Validation 629 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 630 train_loss: 0.451 train_f1: 0.980 \t\n",
      "\n",
      "Validation 630 valid_f1: 0.731 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 631 train_loss: 0.447 train_f1: 0.984 \t\n",
      "\n",
      "Validation 631 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 632 train_loss: 0.442 train_f1: 0.986 \t\n",
      "\n",
      "Validation 632 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 633 train_loss: 0.443 train_f1: 0.987 \t\n",
      "\n",
      "Validation 633 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 634 train_loss: 0.440 train_f1: 0.987 \t\n",
      "\n",
      "Validation 634 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 635 train_loss: 0.438 train_f1: 0.985 \t\n",
      "\n",
      "Validation 635 valid_f1: 0.754 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 636 train_loss: 0.441 train_f1: 0.987 \t\n",
      "\n",
      "Validation 636 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 637 train_loss: 0.440 train_f1: 0.984 \t\n",
      "\n",
      "Validation 637 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 638 train_loss: 0.437 train_f1: 0.986 \t\n",
      "\n",
      "Validation 638 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 639 train_loss: 0.436 train_f1: 0.986 \t\n",
      "\n",
      "Validation 639 valid_f1: 0.717 best_f1: 0.771 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 640 train_loss: 0.435 train_f1: 0.987 \t\n",
      "\n",
      "Validation 640 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 641 train_loss: 0.436 train_f1: 0.986 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 641 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 642 train_loss: 0.433 train_f1: 0.986 \t\n",
      "\n",
      "Validation 642 valid_f1: 0.732 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 643 train_loss: 0.435 train_f1: 0.982 \t\n",
      "\n",
      "Validation 643 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 644 train_loss: 0.437 train_f1: 0.982 \t\n",
      "\n",
      "Validation 644 valid_f1: 0.709 best_f1: 0.771 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 645 train_loss: 0.431 train_f1: 0.986 \t\n",
      "\n",
      "Validation 645 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 646 train_loss: 0.428 train_f1: 0.985 \t\n",
      "\n",
      "Validation 646 valid_f1: 0.727 best_f1: 0.771 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 647 train_loss: 0.430 train_f1: 0.985 \t\n",
      "\n",
      "Validation 647 valid_f1: 0.729 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 648 train_loss: 0.423 train_f1: 0.988 \t\n",
      "\n",
      "Validation 648 valid_f1: 0.733 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 649 train_loss: 0.423 train_f1: 0.988 \t\n",
      "\n",
      "Validation 649 valid_f1: 0.725 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 650 train_loss: 0.425 train_f1: 0.988 \t\n",
      "\n",
      "Validation 650 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 651 train_loss: 0.424 train_f1: 0.985 \t\n",
      "\n",
      "Validation 651 valid_f1: 0.721 best_f1: 0.771 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 652 train_loss: 0.425 train_f1: 0.986 \t\n",
      "\n",
      "Validation 652 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 653 train_loss: 0.425 train_f1: 0.985 \t\n",
      "\n",
      "Validation 653 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 654 train_loss: 0.422 train_f1: 0.983 \t\n",
      "\n",
      "Validation 654 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 655 train_loss: 0.418 train_f1: 0.987 \t\n",
      "\n",
      "Validation 655 valid_f1: 0.765 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 656 train_loss: 0.419 train_f1: 0.986 \t\n",
      "\n",
      "Validation 656 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 657 train_loss: 0.419 train_f1: 0.984 \t\n",
      "\n",
      "Validation 657 valid_f1: 0.754 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 658 train_loss: 0.415 train_f1: 0.987 \t\n",
      "\n",
      "Validation 658 valid_f1: 0.730 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 659 train_loss: 0.415 train_f1: 0.986 \t\n",
      "\n",
      "Validation 659 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 660 train_loss: 0.411 train_f1: 0.987 \t\n",
      "\n",
      "Validation 660 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 661 train_loss: 0.416 train_f1: 0.986 \t\n",
      "\n",
      "Validation 661 valid_f1: 0.759 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 662 train_loss: 0.411 train_f1: 0.988 \t\n",
      "\n",
      "Validation 662 valid_f1: 0.730 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 663 train_loss: 0.409 train_f1: 0.986 \t\n",
      "\n",
      "Validation 663 valid_f1: 0.728 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 664 train_loss: 0.409 train_f1: 0.987 \t\n",
      "\n",
      "Validation 664 valid_f1: 0.731 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 665 train_loss: 0.411 train_f1: 0.987 \t\n",
      "\n",
      "Validation 665 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 666 train_loss: 0.408 train_f1: 0.988 \t\n",
      "\n",
      "Validation 666 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 667 train_loss: 0.405 train_f1: 0.987 \t\n",
      "\n",
      "Validation 667 valid_f1: 0.734 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 668 train_loss: 0.406 train_f1: 0.988 \t\n",
      "\n",
      "Validation 668 valid_f1: 0.730 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 669 train_loss: 0.407 train_f1: 0.987 \t\n",
      "\n",
      "Validation 669 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 670 train_loss: 0.404 train_f1: 0.989 \t\n",
      "\n",
      "Validation 670 valid_f1: 0.717 best_f1: 0.771 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 671 train_loss: 0.405 train_f1: 0.985 \t\n",
      "\n",
      "Validation 671 valid_f1: 0.732 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 672 train_loss: 0.406 train_f1: 0.985 \t\n",
      "\n",
      "Validation 672 valid_f1: 0.761 best_f1: 0.771 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 673 train_loss: 0.400 train_f1: 0.987 \t\n",
      "\n",
      "Validation 673 valid_f1: 0.715 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 674 train_loss: 0.402 train_f1: 0.986 \t\n",
      "\n",
      "Validation 674 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 675 train_loss: 0.404 train_f1: 0.983 \t\n",
      "\n",
      "Validation 675 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 676 train_loss: 0.398 train_f1: 0.988 \t\n",
      "\n",
      "Validation 676 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 677 train_loss: 0.401 train_f1: 0.985 \t\n",
      "\n",
      "Validation 677 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 678 train_loss: 0.396 train_f1: 0.988 \t\n",
      "\n",
      "Validation 678 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 679 train_loss: 0.401 train_f1: 0.983 \t\n",
      "\n",
      "Validation 679 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 680 train_loss: 0.400 train_f1: 0.983 \t\n",
      "\n",
      "Validation 680 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 681 train_loss: 0.401 train_f1: 0.984 \t\n",
      "\n",
      "Validation 681 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 682 train_loss: 0.393 train_f1: 0.986 \t\n",
      "\n",
      "Validation 682 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 683 train_loss: 0.390 train_f1: 0.988 \t\n",
      "\n",
      "Validation 683 valid_f1: 0.728 best_f1: 0.771 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 684 train_loss: 0.391 train_f1: 0.988 \t\n",
      "\n",
      "Validation 684 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 685 train_loss: 0.388 train_f1: 0.989 \t\n",
      "\n",
      "Validation 685 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 686 train_loss: 0.392 train_f1: 0.986 \t\n",
      "\n",
      "Validation 686 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 687 train_loss: 0.387 train_f1: 0.988 \t\n",
      "\n",
      "Validation 687 valid_f1: 0.729 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 688 train_loss: 0.388 train_f1: 0.987 \t\n",
      "\n",
      "Validation 688 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 689 train_loss: 0.383 train_f1: 0.990 \t\n",
      "\n",
      "Validation 689 valid_f1: 0.763 best_f1: 0.771 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 690 train_loss: 0.386 train_f1: 0.987 \t\n",
      "\n",
      "Validation 690 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 691 train_loss: 0.382 train_f1: 0.990 \t\n",
      "\n",
      "Validation 691 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 692 train_loss: 0.387 train_f1: 0.985 \t\n",
      "\n",
      "Validation 692 valid_f1: 0.759 best_f1: 0.771 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 693 train_loss: 0.389 train_f1: 0.984 \t\n",
      "\n",
      "Validation 693 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 694 train_loss: 0.387 train_f1: 0.983 \t\n",
      "\n",
      "Validation 694 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 695 train_loss: 0.384 train_f1: 0.985 \t\n",
      "\n",
      "Validation 695 valid_f1: 0.764 best_f1: 0.771 mean accuracy:0.696 \t\n",
      "\n",
      "Epoch 696 train_loss: 0.386 train_f1: 0.984 \t\n",
      "\n",
      "Validation 696 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 697 train_loss: 0.378 train_f1: 0.988 \t\n",
      "\n",
      "Validation 697 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 698 train_loss: 0.379 train_f1: 0.989 \t\n",
      "\n",
      "Validation 698 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 699 train_loss: 0.379 train_f1: 0.986 \t\n",
      "\n",
      "Validation 699 valid_f1: 0.745 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 700 train_loss: 0.378 train_f1: 0.988 \t\n",
      "\n",
      "Validation 700 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 701 train_loss: 0.378 train_f1: 0.987 \t\n",
      "\n",
      "Validation 701 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 702 train_loss: 0.375 train_f1: 0.986 \t\n",
      "\n",
      "Validation 702 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 703 train_loss: 0.372 train_f1: 0.990 \t\n",
      "\n",
      "Validation 703 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 704 train_loss: 0.370 train_f1: 0.989 \t\n",
      "\n",
      "Validation 704 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 705 train_loss: 0.374 train_f1: 0.987 \t\n",
      "\n",
      "Validation 705 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 706 train_loss: 0.373 train_f1: 0.987 \t\n",
      "\n",
      "Validation 706 valid_f1: 0.762 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 707 train_loss: 0.370 train_f1: 0.989 \t\n",
      "\n",
      "Validation 707 valid_f1: 0.761 best_f1: 0.771 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 708 train_loss: 0.367 train_f1: 0.990 \t\n",
      "\n",
      "Validation 708 valid_f1: 0.766 best_f1: 0.771 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 709 train_loss: 0.365 train_f1: 0.992 \t\n",
      "\n",
      "Validation 709 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 710 train_loss: 0.366 train_f1: 0.989 \t\n",
      "\n",
      "Validation 710 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 711 train_loss: 0.365 train_f1: 0.989 \t\n",
      "\n",
      "Validation 711 valid_f1: 0.734 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 712 train_loss: 0.363 train_f1: 0.987 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 712 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 713 train_loss: 0.363 train_f1: 0.989 \t\n",
      "\n",
      "Validation 713 valid_f1: 0.733 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 714 train_loss: 0.367 train_f1: 0.986 \t\n",
      "\n",
      "Validation 714 valid_f1: 0.734 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 715 train_loss: 0.362 train_f1: 0.988 \t\n",
      "\n",
      "Validation 715 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 716 train_loss: 0.359 train_f1: 0.990 \t\n",
      "\n",
      "Validation 716 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 717 train_loss: 0.360 train_f1: 0.989 \t\n",
      "\n",
      "Validation 717 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 718 train_loss: 0.362 train_f1: 0.988 \t\n",
      "\n",
      "Validation 718 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 719 train_loss: 0.361 train_f1: 0.986 \t\n",
      "\n",
      "Validation 719 valid_f1: 0.760 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 720 train_loss: 0.360 train_f1: 0.989 \t\n",
      "\n",
      "Validation 720 valid_f1: 0.754 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 721 train_loss: 0.362 train_f1: 0.984 \t\n",
      "\n",
      "Validation 721 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 722 train_loss: 0.355 train_f1: 0.986 \t\n",
      "\n",
      "Validation 722 valid_f1: 0.729 best_f1: 0.771 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 723 train_loss: 0.356 train_f1: 0.988 \t\n",
      "\n",
      "Validation 723 valid_f1: 0.767 best_f1: 0.771 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 724 train_loss: 0.353 train_f1: 0.990 \t\n",
      "\n",
      "Validation 724 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 725 train_loss: 0.356 train_f1: 0.988 \t\n",
      "\n",
      "Validation 725 valid_f1: 0.762 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 726 train_loss: 0.352 train_f1: 0.989 \t\n",
      "\n",
      "Validation 726 valid_f1: 0.754 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 727 train_loss: 0.352 train_f1: 0.989 \t\n",
      "\n",
      "Validation 727 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 728 train_loss: 0.355 train_f1: 0.984 \t\n",
      "\n",
      "Validation 728 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 729 train_loss: 0.352 train_f1: 0.988 \t\n",
      "\n",
      "Validation 729 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 730 train_loss: 0.348 train_f1: 0.988 \t\n",
      "\n",
      "Validation 730 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 731 train_loss: 0.349 train_f1: 0.988 \t\n",
      "\n",
      "Validation 731 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 732 train_loss: 0.349 train_f1: 0.986 \t\n",
      "\n",
      "Validation 732 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 733 train_loss: 0.346 train_f1: 0.987 \t\n",
      "\n",
      "Validation 733 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 734 train_loss: 0.348 train_f1: 0.985 \t\n",
      "\n",
      "Validation 734 valid_f1: 0.745 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 735 train_loss: 0.346 train_f1: 0.988 \t\n",
      "\n",
      "Validation 735 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 736 train_loss: 0.347 train_f1: 0.987 \t\n",
      "\n",
      "Validation 736 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 737 train_loss: 0.345 train_f1: 0.987 \t\n",
      "\n",
      "Validation 737 valid_f1: 0.718 best_f1: 0.771 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 738 train_loss: 0.342 train_f1: 0.987 \t\n",
      "\n",
      "Validation 738 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 739 train_loss: 0.342 train_f1: 0.987 \t\n",
      "\n",
      "Validation 739 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 740 train_loss: 0.344 train_f1: 0.985 \t\n",
      "\n",
      "Validation 740 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 741 train_loss: 0.342 train_f1: 0.989 \t\n",
      "\n",
      "Validation 741 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 742 train_loss: 0.334 train_f1: 0.991 \t\n",
      "\n",
      "Validation 742 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 743 train_loss: 0.341 train_f1: 0.986 \t\n",
      "\n",
      "Validation 743 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 744 train_loss: 0.339 train_f1: 0.987 \t\n",
      "\n",
      "Validation 744 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 745 train_loss: 0.341 train_f1: 0.987 \t\n",
      "\n",
      "Validation 745 valid_f1: 0.720 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 746 train_loss: 0.335 train_f1: 0.988 \t\n",
      "\n",
      "Validation 746 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 747 train_loss: 0.336 train_f1: 0.989 \t\n",
      "\n",
      "Validation 747 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 748 train_loss: 0.338 train_f1: 0.985 \t\n",
      "\n",
      "Validation 748 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 749 train_loss: 0.335 train_f1: 0.986 \t\n",
      "\n",
      "Validation 749 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 750 train_loss: 0.331 train_f1: 0.989 \t\n",
      "\n",
      "Validation 750 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 751 train_loss: 0.331 train_f1: 0.988 \t\n",
      "\n",
      "Validation 751 valid_f1: 0.734 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 752 train_loss: 0.331 train_f1: 0.988 \t\n",
      "\n",
      "Validation 752 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 753 train_loss: 0.329 train_f1: 0.988 \t\n",
      "\n",
      "Validation 753 valid_f1: 0.734 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 754 train_loss: 0.326 train_f1: 0.989 \t\n",
      "\n",
      "Validation 754 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 755 train_loss: 0.328 train_f1: 0.986 \t\n",
      "\n",
      "Validation 755 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.696 \t\n",
      "\n",
      "Epoch 756 train_loss: 0.327 train_f1: 0.988 \t\n",
      "\n",
      "Validation 756 valid_f1: 0.756 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 757 train_loss: 0.325 train_f1: 0.989 \t\n",
      "\n",
      "Validation 757 valid_f1: 0.756 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 758 train_loss: 0.326 train_f1: 0.986 \t\n",
      "\n",
      "Validation 758 valid_f1: 0.761 best_f1: 0.771 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 759 train_loss: 0.325 train_f1: 0.989 \t\n",
      "\n",
      "Validation 759 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 760 train_loss: 0.323 train_f1: 0.989 \t\n",
      "\n",
      "Validation 760 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 761 train_loss: 0.325 train_f1: 0.987 \t\n",
      "\n",
      "Validation 761 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 762 train_loss: 0.321 train_f1: 0.989 \t\n",
      "\n",
      "Validation 762 valid_f1: 0.756 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 763 train_loss: 0.322 train_f1: 0.989 \t\n",
      "\n",
      "Validation 763 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 764 train_loss: 0.316 train_f1: 0.989 \t\n",
      "\n",
      "Validation 764 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 765 train_loss: 0.317 train_f1: 0.989 \t\n",
      "\n",
      "Validation 765 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 766 train_loss: 0.319 train_f1: 0.988 \t\n",
      "\n",
      "Validation 766 valid_f1: 0.756 best_f1: 0.771 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 767 train_loss: 0.320 train_f1: 0.987 \t\n",
      "\n",
      "Validation 767 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 768 train_loss: 0.317 train_f1: 0.988 \t\n",
      "\n",
      "Validation 768 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 769 train_loss: 0.315 train_f1: 0.987 \t\n",
      "\n",
      "Validation 769 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 770 train_loss: 0.315 train_f1: 0.988 \t\n",
      "\n",
      "Validation 770 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 771 train_loss: 0.315 train_f1: 0.987 \t\n",
      "\n",
      "Validation 771 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 772 train_loss: 0.311 train_f1: 0.991 \t\n",
      "\n",
      "Validation 772 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 773 train_loss: 0.310 train_f1: 0.988 \t\n",
      "\n",
      "Validation 773 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 774 train_loss: 0.310 train_f1: 0.990 \t\n",
      "\n",
      "Validation 774 valid_f1: 0.757 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 775 train_loss: 0.315 train_f1: 0.987 \t\n",
      "\n",
      "Validation 775 valid_f1: 0.734 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 776 train_loss: 0.311 train_f1: 0.986 \t\n",
      "\n",
      "Validation 776 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 777 train_loss: 0.313 train_f1: 0.986 \t\n",
      "\n",
      "Validation 777 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 778 train_loss: 0.308 train_f1: 0.988 \t\n",
      "\n",
      "Validation 778 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 779 train_loss: 0.309 train_f1: 0.988 \t\n",
      "\n",
      "Validation 779 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 780 train_loss: 0.307 train_f1: 0.987 \t\n",
      "\n",
      "Validation 780 valid_f1: 0.733 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 781 train_loss: 0.306 train_f1: 0.989 \t\n",
      "\n",
      "Validation 781 valid_f1: 0.745 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 782 train_loss: 0.305 train_f1: 0.990 \t\n",
      "\n",
      "Validation 782 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 783 train_loss: 0.306 train_f1: 0.988 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 783 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 784 train_loss: 0.305 train_f1: 0.987 \t\n",
      "\n",
      "Validation 784 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 785 train_loss: 0.304 train_f1: 0.988 \t\n",
      "\n",
      "Validation 785 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 786 train_loss: 0.300 train_f1: 0.990 \t\n",
      "\n",
      "Validation 786 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 787 train_loss: 0.302 train_f1: 0.988 \t\n",
      "\n",
      "Validation 787 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 788 train_loss: 0.303 train_f1: 0.987 \t\n",
      "\n",
      "Validation 788 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 789 train_loss: 0.297 train_f1: 0.989 \t\n",
      "\n",
      "Validation 789 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 790 train_loss: 0.294 train_f1: 0.990 \t\n",
      "\n",
      "Validation 790 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 791 train_loss: 0.297 train_f1: 0.987 \t\n",
      "\n",
      "Validation 791 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 792 train_loss: 0.293 train_f1: 0.991 \t\n",
      "\n",
      "Validation 792 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 793 train_loss: 0.296 train_f1: 0.989 \t\n",
      "\n",
      "Validation 793 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 794 train_loss: 0.296 train_f1: 0.988 \t\n",
      "\n",
      "Validation 794 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 795 train_loss: 0.298 train_f1: 0.987 \t\n",
      "\n",
      "Validation 795 valid_f1: 0.757 best_f1: 0.771 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 796 train_loss: 0.295 train_f1: 0.989 \t\n",
      "\n",
      "Validation 796 valid_f1: 0.759 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 797 train_loss: 0.296 train_f1: 0.988 \t\n",
      "\n",
      "Validation 797 valid_f1: 0.763 best_f1: 0.771 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 798 train_loss: 0.292 train_f1: 0.988 \t\n",
      "\n",
      "Validation 798 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 799 train_loss: 0.289 train_f1: 0.990 \t\n",
      "\n",
      "Validation 799 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 800 train_loss: 0.287 train_f1: 0.990 \t\n",
      "\n",
      "Validation 800 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 801 train_loss: 0.291 train_f1: 0.990 \t\n",
      "\n",
      "Validation 801 valid_f1: 0.760 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 802 train_loss: 0.288 train_f1: 0.990 \t\n",
      "\n",
      "Validation 802 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 803 train_loss: 0.286 train_f1: 0.990 \t\n",
      "\n",
      "Validation 803 valid_f1: 0.728 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 804 train_loss: 0.286 train_f1: 0.989 \t\n",
      "\n",
      "Validation 804 valid_f1: 0.760 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 805 train_loss: 0.287 train_f1: 0.990 \t\n",
      "\n",
      "Validation 805 valid_f1: 0.757 best_f1: 0.771 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 806 train_loss: 0.285 train_f1: 0.990 \t\n",
      "\n",
      "Validation 806 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 807 train_loss: 0.280 train_f1: 0.991 \t\n",
      "\n",
      "Validation 807 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 808 train_loss: 0.295 train_f1: 0.983 \t\n",
      "\n",
      "Validation 808 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 809 train_loss: 0.282 train_f1: 0.989 \t\n",
      "\n",
      "Validation 809 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 810 train_loss: 0.282 train_f1: 0.990 \t\n",
      "\n",
      "Validation 810 valid_f1: 0.760 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 811 train_loss: 0.281 train_f1: 0.991 \t\n",
      "\n",
      "Validation 811 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 812 train_loss: 0.280 train_f1: 0.991 \t\n",
      "\n",
      "Validation 812 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 813 train_loss: 0.279 train_f1: 0.988 \t\n",
      "\n",
      "Validation 813 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 814 train_loss: 0.279 train_f1: 0.990 \t\n",
      "\n",
      "Validation 814 valid_f1: 0.763 best_f1: 0.771 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 815 train_loss: 0.278 train_f1: 0.989 \t\n",
      "\n",
      "Validation 815 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 816 train_loss: 0.276 train_f1: 0.990 \t\n",
      "\n",
      "Validation 816 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 817 train_loss: 0.273 train_f1: 0.993 \t\n",
      "\n",
      "Validation 817 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 818 train_loss: 0.273 train_f1: 0.989 \t\n",
      "\n",
      "Validation 818 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 819 train_loss: 0.277 train_f1: 0.989 \t\n",
      "\n",
      "Validation 819 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 820 train_loss: 0.276 train_f1: 0.987 \t\n",
      "\n",
      "Validation 820 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 821 train_loss: 0.271 train_f1: 0.989 \t\n",
      "\n",
      "Validation 821 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 822 train_loss: 0.275 train_f1: 0.989 \t\n",
      "\n",
      "Validation 822 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 823 train_loss: 0.276 train_f1: 0.987 \t\n",
      "\n",
      "Validation 823 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 824 train_loss: 0.268 train_f1: 0.990 \t\n",
      "\n",
      "Validation 824 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 825 train_loss: 0.273 train_f1: 0.989 \t\n",
      "\n",
      "Validation 825 valid_f1: 0.760 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 826 train_loss: 0.267 train_f1: 0.988 \t\n",
      "\n",
      "Validation 826 valid_f1: 0.762 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 827 train_loss: 0.267 train_f1: 0.990 \t\n",
      "\n",
      "Validation 827 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 828 train_loss: 0.274 train_f1: 0.989 \t\n",
      "\n",
      "Validation 828 valid_f1: 0.760 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 829 train_loss: 0.272 train_f1: 0.986 \t\n",
      "\n",
      "Validation 829 valid_f1: 0.754 best_f1: 0.771 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 830 train_loss: 0.265 train_f1: 0.991 \t\n",
      "\n",
      "Validation 830 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 831 train_loss: 0.265 train_f1: 0.989 \t\n",
      "\n",
      "Validation 831 valid_f1: 0.769 best_f1: 0.771 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 832 train_loss: 0.268 train_f1: 0.990 \t\n",
      "\n",
      "Validation 832 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 833 train_loss: 0.264 train_f1: 0.992 \t\n",
      "\n",
      "Validation 833 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 834 train_loss: 0.265 train_f1: 0.990 \t\n",
      "\n",
      "Validation 834 valid_f1: 0.759 best_f1: 0.771 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 835 train_loss: 0.262 train_f1: 0.990 \t\n",
      "\n",
      "Validation 835 valid_f1: 0.756 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 836 train_loss: 0.261 train_f1: 0.990 \t\n",
      "\n",
      "Validation 836 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 837 train_loss: 0.260 train_f1: 0.990 \t\n",
      "\n",
      "Validation 837 valid_f1: 0.754 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 838 train_loss: 0.260 train_f1: 0.991 \t\n",
      "\n",
      "Validation 838 valid_f1: 0.758 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 839 train_loss: 0.263 train_f1: 0.988 \t\n",
      "\n",
      "Validation 839 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 840 train_loss: 0.262 train_f1: 0.991 \t\n",
      "\n",
      "Validation 840 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 841 train_loss: 0.261 train_f1: 0.988 \t\n",
      "\n",
      "Validation 841 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 842 train_loss: 0.260 train_f1: 0.988 \t\n",
      "\n",
      "Validation 842 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 843 train_loss: 0.255 train_f1: 0.990 \t\n",
      "\n",
      "Validation 843 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 844 train_loss: 0.257 train_f1: 0.992 \t\n",
      "\n",
      "Validation 844 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 845 train_loss: 0.256 train_f1: 0.989 \t\n",
      "\n",
      "Validation 845 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 846 train_loss: 0.252 train_f1: 0.990 \t\n",
      "\n",
      "Validation 846 valid_f1: 0.760 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 847 train_loss: 0.255 train_f1: 0.988 \t\n",
      "\n",
      "Validation 847 valid_f1: 0.734 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 848 train_loss: 0.253 train_f1: 0.990 \t\n",
      "\n",
      "Validation 848 valid_f1: 0.745 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 849 train_loss: 0.252 train_f1: 0.988 \t\n",
      "\n",
      "Validation 849 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 850 train_loss: 0.252 train_f1: 0.991 \t\n",
      "\n",
      "Validation 850 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 851 train_loss: 0.251 train_f1: 0.988 \t\n",
      "\n",
      "Validation 851 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 852 train_loss: 0.253 train_f1: 0.987 \t\n",
      "\n",
      "Validation 852 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 853 train_loss: 0.254 train_f1: 0.985 \t\n",
      "\n",
      "Validation 853 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 854 train_loss: 0.253 train_f1: 0.987 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 854 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 855 train_loss: 0.251 train_f1: 0.988 \t\n",
      "\n",
      "Validation 855 valid_f1: 0.756 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 856 train_loss: 0.249 train_f1: 0.989 \t\n",
      "\n",
      "Validation 856 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 857 train_loss: 0.246 train_f1: 0.986 \t\n",
      "\n",
      "Validation 857 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 858 train_loss: 0.245 train_f1: 0.987 \t\n",
      "\n",
      "Validation 858 valid_f1: 0.733 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 859 train_loss: 0.244 train_f1: 0.989 \t\n",
      "\n",
      "Validation 859 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 860 train_loss: 0.242 train_f1: 0.989 \t\n",
      "\n",
      "Validation 860 valid_f1: 0.731 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 861 train_loss: 0.243 train_f1: 0.991 \t\n",
      "\n",
      "Validation 861 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 862 train_loss: 0.242 train_f1: 0.991 \t\n",
      "\n",
      "Validation 862 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 863 train_loss: 0.242 train_f1: 0.990 \t\n",
      "\n",
      "Validation 863 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 864 train_loss: 0.241 train_f1: 0.991 \t\n",
      "\n",
      "Validation 864 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 865 train_loss: 0.243 train_f1: 0.987 \t\n",
      "\n",
      "Validation 865 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 866 train_loss: 0.245 train_f1: 0.986 \t\n",
      "\n",
      "Validation 866 valid_f1: 0.745 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 867 train_loss: 0.237 train_f1: 0.990 \t\n",
      "\n",
      "Validation 867 valid_f1: 0.729 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 868 train_loss: 0.242 train_f1: 0.989 \t\n",
      "\n",
      "Validation 868 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 869 train_loss: 0.238 train_f1: 0.989 \t\n",
      "\n",
      "Validation 869 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 870 train_loss: 0.239 train_f1: 0.991 \t\n",
      "\n",
      "Validation 870 valid_f1: 0.760 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 871 train_loss: 0.247 train_f1: 0.983 \t\n",
      "\n",
      "Validation 871 valid_f1: 0.729 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 872 train_loss: 0.238 train_f1: 0.988 \t\n",
      "\n",
      "Validation 872 valid_f1: 0.745 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 873 train_loss: 0.241 train_f1: 0.988 \t\n",
      "\n",
      "Validation 873 valid_f1: 0.738 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 874 train_loss: 0.240 train_f1: 0.990 \t\n",
      "\n",
      "Validation 874 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 875 train_loss: 0.238 train_f1: 0.986 \t\n",
      "\n",
      "Validation 875 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 876 train_loss: 0.233 train_f1: 0.989 \t\n",
      "\n",
      "Validation 876 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 877 train_loss: 0.233 train_f1: 0.990 \t\n",
      "\n",
      "Validation 877 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 878 train_loss: 0.233 train_f1: 0.990 \t\n",
      "\n",
      "Validation 878 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 879 train_loss: 0.233 train_f1: 0.991 \t\n",
      "\n",
      "Validation 879 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 880 train_loss: 0.229 train_f1: 0.991 \t\n",
      "\n",
      "Validation 880 valid_f1: 0.754 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 881 train_loss: 0.231 train_f1: 0.988 \t\n",
      "\n",
      "Validation 881 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 882 train_loss: 0.229 train_f1: 0.990 \t\n",
      "\n",
      "Validation 882 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 883 train_loss: 0.225 train_f1: 0.992 \t\n",
      "\n",
      "Validation 883 valid_f1: 0.749 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 884 train_loss: 0.226 train_f1: 0.992 \t\n",
      "\n",
      "Validation 884 valid_f1: 0.730 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 885 train_loss: 0.229 train_f1: 0.990 \t\n",
      "\n",
      "Validation 885 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 886 train_loss: 0.224 train_f1: 0.990 \t\n",
      "\n",
      "Validation 886 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 887 train_loss: 0.226 train_f1: 0.991 \t\n",
      "\n",
      "Validation 887 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 888 train_loss: 0.227 train_f1: 0.990 \t\n",
      "\n",
      "Validation 888 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 889 train_loss: 0.230 train_f1: 0.986 \t\n",
      "\n",
      "Validation 889 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 890 train_loss: 0.218 train_f1: 0.991 \t\n",
      "\n",
      "Validation 890 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 891 train_loss: 0.220 train_f1: 0.991 \t\n",
      "\n",
      "Validation 891 valid_f1: 0.736 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 892 train_loss: 0.223 train_f1: 0.991 \t\n",
      "\n",
      "Validation 892 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 893 train_loss: 0.223 train_f1: 0.988 \t\n",
      "\n",
      "Validation 893 valid_f1: 0.739 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 894 train_loss: 0.223 train_f1: 0.988 \t\n",
      "\n",
      "Validation 894 valid_f1: 0.732 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 895 train_loss: 0.223 train_f1: 0.986 \t\n",
      "\n",
      "Validation 895 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 896 train_loss: 0.220 train_f1: 0.990 \t\n",
      "\n",
      "Validation 896 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 897 train_loss: 0.224 train_f1: 0.988 \t\n",
      "\n",
      "Validation 897 valid_f1: 0.727 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 898 train_loss: 0.220 train_f1: 0.991 \t\n",
      "\n",
      "Validation 898 valid_f1: 0.722 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 899 train_loss: 0.220 train_f1: 0.989 \t\n",
      "\n",
      "Validation 899 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 900 train_loss: 0.219 train_f1: 0.989 \t\n",
      "\n",
      "Validation 900 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 901 train_loss: 0.216 train_f1: 0.990 \t\n",
      "\n",
      "Validation 901 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 902 train_loss: 0.216 train_f1: 0.991 \t\n",
      "\n",
      "Validation 902 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 903 train_loss: 0.212 train_f1: 0.991 \t\n",
      "\n",
      "Validation 903 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 904 train_loss: 0.220 train_f1: 0.988 \t\n",
      "\n",
      "Validation 904 valid_f1: 0.729 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 905 train_loss: 0.212 train_f1: 0.992 \t\n",
      "\n",
      "Validation 905 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 906 train_loss: 0.214 train_f1: 0.990 \t\n",
      "\n",
      "Validation 906 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 907 train_loss: 0.216 train_f1: 0.989 \t\n",
      "\n",
      "Validation 907 valid_f1: 0.728 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 908 train_loss: 0.213 train_f1: 0.989 \t\n",
      "\n",
      "Validation 908 valid_f1: 0.757 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 909 train_loss: 0.206 train_f1: 0.993 \t\n",
      "\n",
      "Validation 909 valid_f1: 0.733 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 910 train_loss: 0.211 train_f1: 0.989 \t\n",
      "\n",
      "Validation 910 valid_f1: 0.733 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 911 train_loss: 0.209 train_f1: 0.989 \t\n",
      "\n",
      "Validation 911 valid_f1: 0.728 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 912 train_loss: 0.208 train_f1: 0.991 \t\n",
      "\n",
      "Validation 912 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 913 train_loss: 0.210 train_f1: 0.990 \t\n",
      "\n",
      "Validation 913 valid_f1: 0.730 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 914 train_loss: 0.209 train_f1: 0.990 \t\n",
      "\n",
      "Validation 914 valid_f1: 0.730 best_f1: 0.771 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 915 train_loss: 0.208 train_f1: 0.989 \t\n",
      "\n",
      "Validation 915 valid_f1: 0.733 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 916 train_loss: 0.206 train_f1: 0.988 \t\n",
      "\n",
      "Validation 916 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 917 train_loss: 0.206 train_f1: 0.990 \t\n",
      "\n",
      "Validation 917 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 918 train_loss: 0.206 train_f1: 0.992 \t\n",
      "\n",
      "Validation 918 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 919 train_loss: 0.207 train_f1: 0.988 \t\n",
      "\n",
      "Validation 919 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 920 train_loss: 0.202 train_f1: 0.989 \t\n",
      "\n",
      "Validation 920 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 921 train_loss: 0.209 train_f1: 0.984 \t\n",
      "\n",
      "Validation 921 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 922 train_loss: 0.212 train_f1: 0.987 \t\n",
      "\n",
      "Validation 922 valid_f1: 0.753 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 923 train_loss: 0.215 train_f1: 0.982 \t\n",
      "\n",
      "Validation 923 valid_f1: 0.743 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 924 train_loss: 0.204 train_f1: 0.988 \t\n",
      "\n",
      "Validation 924 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 925 train_loss: 0.206 train_f1: 0.991 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 925 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 926 train_loss: 0.210 train_f1: 0.988 \t\n",
      "\n",
      "Validation 926 valid_f1: 0.756 best_f1: 0.771 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 927 train_loss: 0.206 train_f1: 0.988 \t\n",
      "\n",
      "Validation 927 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 928 train_loss: 0.203 train_f1: 0.989 \t\n",
      "\n",
      "Validation 928 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 929 train_loss: 0.200 train_f1: 0.992 \t\n",
      "\n",
      "Validation 929 valid_f1: 0.746 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 930 train_loss: 0.205 train_f1: 0.991 \t\n",
      "\n",
      "Validation 930 valid_f1: 0.742 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 931 train_loss: 0.198 train_f1: 0.989 \t\n",
      "\n",
      "Validation 931 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 932 train_loss: 0.199 train_f1: 0.991 \t\n",
      "\n",
      "Validation 932 valid_f1: 0.756 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 933 train_loss: 0.196 train_f1: 0.992 \t\n",
      "\n",
      "Validation 933 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 934 train_loss: 0.198 train_f1: 0.990 \t\n",
      "\n",
      "Validation 934 valid_f1: 0.757 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 935 train_loss: 0.197 train_f1: 0.991 \t\n",
      "\n",
      "Validation 935 valid_f1: 0.723 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 936 train_loss: 0.195 train_f1: 0.990 \t\n",
      "\n",
      "Validation 936 valid_f1: 0.751 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 937 train_loss: 0.198 train_f1: 0.988 \t\n",
      "\n",
      "Validation 937 valid_f1: 0.741 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 938 train_loss: 0.194 train_f1: 0.989 \t\n",
      "\n",
      "Validation 938 valid_f1: 0.754 best_f1: 0.771 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 939 train_loss: 0.191 train_f1: 0.991 \t\n",
      "\n",
      "Validation 939 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 940 train_loss: 0.189 train_f1: 0.990 \t\n",
      "\n",
      "Validation 940 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 941 train_loss: 0.191 train_f1: 0.990 \t\n",
      "\n",
      "Validation 941 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 942 train_loss: 0.195 train_f1: 0.990 \t\n",
      "\n",
      "Validation 942 valid_f1: 0.752 best_f1: 0.771 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 943 train_loss: 0.192 train_f1: 0.989 \t\n",
      "\n",
      "Validation 943 valid_f1: 0.744 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 944 train_loss: 0.192 train_f1: 0.988 \t\n",
      "\n",
      "Validation 944 valid_f1: 0.762 best_f1: 0.771 mean accuracy:0.696 \t\n",
      "\n",
      "Epoch 945 train_loss: 0.189 train_f1: 0.990 \t\n",
      "\n",
      "Validation 945 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 946 train_loss: 0.190 train_f1: 0.991 \t\n",
      "\n",
      "Validation 946 valid_f1: 0.778 best_f1: 0.778 mean accuracy:0.710 \t\n",
      "\n",
      "Epoch 947 train_loss: 0.189 train_f1: 0.991 \t\n",
      "\n",
      "Validation 947 valid_f1: 0.735 best_f1: 0.778 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 948 train_loss: 0.189 train_f1: 0.988 \t\n",
      "\n",
      "Validation 948 valid_f1: 0.750 best_f1: 0.778 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 949 train_loss: 0.189 train_f1: 0.990 \t\n",
      "\n",
      "Validation 949 valid_f1: 0.751 best_f1: 0.778 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 950 train_loss: 0.183 train_f1: 0.992 \t\n",
      "\n",
      "Validation 950 valid_f1: 0.759 best_f1: 0.778 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 951 train_loss: 0.189 train_f1: 0.992 \t\n",
      "\n",
      "Validation 951 valid_f1: 0.754 best_f1: 0.778 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 952 train_loss: 0.190 train_f1: 0.987 \t\n",
      "\n",
      "Validation 952 valid_f1: 0.742 best_f1: 0.778 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 953 train_loss: 0.187 train_f1: 0.989 \t\n",
      "\n",
      "Validation 953 valid_f1: 0.742 best_f1: 0.778 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 954 train_loss: 0.186 train_f1: 0.989 \t\n",
      "\n",
      "Validation 954 valid_f1: 0.764 best_f1: 0.778 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 955 train_loss: 0.185 train_f1: 0.989 \t\n",
      "\n",
      "Validation 955 valid_f1: 0.749 best_f1: 0.778 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 956 train_loss: 0.185 train_f1: 0.991 \t\n",
      "\n",
      "Validation 956 valid_f1: 0.758 best_f1: 0.778 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 957 train_loss: 0.183 train_f1: 0.991 \t\n",
      "\n",
      "Validation 957 valid_f1: 0.747 best_f1: 0.778 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 958 train_loss: 0.187 train_f1: 0.989 \t\n",
      "\n",
      "Validation 958 valid_f1: 0.741 best_f1: 0.778 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 959 train_loss: 0.184 train_f1: 0.989 \t\n",
      "\n",
      "Validation 959 valid_f1: 0.753 best_f1: 0.778 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 960 train_loss: 0.178 train_f1: 0.992 \t\n",
      "\n",
      "Validation 960 valid_f1: 0.747 best_f1: 0.778 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 961 train_loss: 0.179 train_f1: 0.990 \t\n",
      "\n",
      "Validation 961 valid_f1: 0.761 best_f1: 0.778 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 962 train_loss: 0.182 train_f1: 0.989 \t\n",
      "\n",
      "Validation 962 valid_f1: 0.757 best_f1: 0.778 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 963 train_loss: 0.179 train_f1: 0.990 \t\n",
      "\n",
      "Validation 963 valid_f1: 0.733 best_f1: 0.778 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 964 train_loss: 0.182 train_f1: 0.991 \t\n",
      "\n",
      "Validation 964 valid_f1: 0.731 best_f1: 0.778 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 965 train_loss: 0.181 train_f1: 0.986 \t\n",
      "\n",
      "Validation 965 valid_f1: 0.739 best_f1: 0.778 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 966 train_loss: 0.179 train_f1: 0.988 \t\n",
      "\n",
      "Validation 966 valid_f1: 0.751 best_f1: 0.778 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 967 train_loss: 0.173 train_f1: 0.991 \t\n",
      "\n",
      "Validation 967 valid_f1: 0.751 best_f1: 0.778 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 968 train_loss: 0.180 train_f1: 0.989 \t\n",
      "\n",
      "Validation 968 valid_f1: 0.753 best_f1: 0.778 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 969 train_loss: 0.177 train_f1: 0.990 \t\n",
      "\n",
      "Validation 969 valid_f1: 0.741 best_f1: 0.778 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 970 train_loss: 0.178 train_f1: 0.991 \t\n",
      "\n",
      "Validation 970 valid_f1: 0.753 best_f1: 0.778 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 971 train_loss: 0.176 train_f1: 0.991 \t\n",
      "\n",
      "Validation 971 valid_f1: 0.741 best_f1: 0.778 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 972 train_loss: 0.178 train_f1: 0.988 \t\n",
      "\n",
      "Validation 972 valid_f1: 0.743 best_f1: 0.778 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 973 train_loss: 0.178 train_f1: 0.988 \t\n",
      "\n",
      "Validation 973 valid_f1: 0.759 best_f1: 0.778 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 974 train_loss: 0.172 train_f1: 0.990 \t\n",
      "\n",
      "Validation 974 valid_f1: 0.748 best_f1: 0.778 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 975 train_loss: 0.172 train_f1: 0.992 \t\n",
      "\n",
      "Validation 975 valid_f1: 0.737 best_f1: 0.778 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 976 train_loss: 0.167 train_f1: 0.993 \t\n",
      "\n",
      "Validation 976 valid_f1: 0.741 best_f1: 0.778 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 977 train_loss: 0.173 train_f1: 0.990 \t\n",
      "\n",
      "Validation 977 valid_f1: 0.733 best_f1: 0.778 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 978 train_loss: 0.169 train_f1: 0.991 \t\n",
      "\n",
      "Validation 978 valid_f1: 0.706 best_f1: 0.778 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 979 train_loss: 0.175 train_f1: 0.990 \t\n",
      "\n",
      "Validation 979 valid_f1: 0.727 best_f1: 0.778 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 980 train_loss: 0.182 train_f1: 0.983 \t\n",
      "\n",
      "Validation 980 valid_f1: 0.750 best_f1: 0.778 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 981 train_loss: 0.169 train_f1: 0.988 \t\n",
      "\n",
      "Validation 981 valid_f1: 0.748 best_f1: 0.778 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 982 train_loss: 0.174 train_f1: 0.990 \t\n",
      "\n",
      "Validation 982 valid_f1: 0.736 best_f1: 0.778 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 983 train_loss: 0.171 train_f1: 0.990 \t\n",
      "\n",
      "Validation 983 valid_f1: 0.753 best_f1: 0.778 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 984 train_loss: 0.172 train_f1: 0.989 \t\n",
      "\n",
      "Validation 984 valid_f1: 0.739 best_f1: 0.778 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 985 train_loss: 0.168 train_f1: 0.990 \t\n",
      "\n",
      "Validation 985 valid_f1: 0.750 best_f1: 0.778 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 986 train_loss: 0.176 train_f1: 0.988 \t\n",
      "\n",
      "Validation 986 valid_f1: 0.745 best_f1: 0.778 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 987 train_loss: 0.169 train_f1: 0.990 \t\n",
      "\n",
      "Validation 987 valid_f1: 0.740 best_f1: 0.778 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 988 train_loss: 0.172 train_f1: 0.987 \t\n",
      "\n",
      "Validation 988 valid_f1: 0.757 best_f1: 0.778 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 989 train_loss: 0.163 train_f1: 0.991 \t\n",
      "\n",
      "Validation 989 valid_f1: 0.741 best_f1: 0.778 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 990 train_loss: 0.166 train_f1: 0.989 \t\n",
      "\n",
      "Validation 990 valid_f1: 0.739 best_f1: 0.778 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 991 train_loss: 0.165 train_f1: 0.992 \t\n",
      "\n",
      "Validation 991 valid_f1: 0.751 best_f1: 0.778 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 992 train_loss: 0.161 train_f1: 0.992 \t\n",
      "\n",
      "Validation 992 valid_f1: 0.750 best_f1: 0.778 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 993 train_loss: 0.163 train_f1: 0.991 \t\n",
      "\n",
      "Validation 993 valid_f1: 0.738 best_f1: 0.778 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 994 train_loss: 0.165 train_f1: 0.990 \t\n",
      "\n",
      "Validation 994 valid_f1: 0.744 best_f1: 0.778 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 995 train_loss: 0.162 train_f1: 0.991 \t\n",
      "\n",
      "Validation 995 valid_f1: 0.756 best_f1: 0.778 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 996 train_loss: 0.164 train_f1: 0.987 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 996 valid_f1: 0.740 best_f1: 0.778 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 997 train_loss: 0.161 train_f1: 0.990 \t\n",
      "\n",
      "Validation 997 valid_f1: 0.731 best_f1: 0.778 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 998 train_loss: 0.165 train_f1: 0.992 \t\n",
      "\n",
      "Validation 998 valid_f1: 0.736 best_f1: 0.778 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 999 train_loss: 0.166 train_f1: 0.987 \t\n",
      "\n",
      "Validation 999 valid_f1: 0.737 best_f1: 0.778 mean accuracy:0.645 \t\n",
      "2020-07-08 14:55:07.688301\n"
     ]
    }
   ],
   "source": [
    "# DISP DATETIME FOR CHECKING TIME\n",
    "print(dt.datetime.now())\n",
    "\n",
    "train_loss_sum=[]\n",
    "train_f1_sum=[]\n",
    "val_acc_sum=[]\n",
    "val_f1_sum=[]\n",
    "val_f1_classes=[]\n",
    "val_f1_min = 0\n",
    "print(results_directory)\n",
    "\n",
    "patience = 50\n",
    "\n",
    "for num_epoch in range(epochs):\n",
    "#     num_epoch += 32 # 중단된 코드 돌리기 위해 임의로 사용\n",
    "    random.shuffle(data_train)\n",
    "    train_loss, train_f1 = train(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final, model)\n",
    "#     train_loss, train_f1 = train_edit(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final, model)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    train_loss_sum.append(train_loss)\n",
    "    train_f1_sum.append(train_f1)\n",
    "    \n",
    "    print('\\nEpoch',num_epoch,'train_loss:',f'{train_loss:.3f}','train_f1:',f'{train_f1:.3f}',\"\\t\")\n",
    "    model_output = \"ECG_ABN_E%02dL%.2f\" % (num_epoch, train_loss)\n",
    "    save_name = os.path.join(results_directory, model_output)\n",
    "    \n",
    "    val_acc, f1_classes, f1,_,_,_ = test(data_val, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)\n",
    "#     val_acc, f1_classes, f1,_,_,_ = test_edit(data_val, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)\n",
    "    \n",
    "    \n",
    "    \n",
    "    val_acc_sum.append(val_acc)\n",
    "    val_f1_sum.append(f1)\n",
    "    val_f1_classes.append(f1_classes)\n",
    "\n",
    "    if f1 > val_f1_min:\n",
    "        val_f1_min = f1\n",
    "#         earlystop = 0\n",
    "        if val_acc > 0.6: # 너무 낮아서 어차피 안쓸것들은 제외하기 위한 것\n",
    "            model.save_weights(save_name.format(epoch=0))\n",
    "#     else: \n",
    "#         earlystop += 1\n",
    "#         if earlystop > patience: \n",
    "#             print(\"Early stopped training due to non-improved performance\")\n",
    "#             break\n",
    "    \n",
    "    print('\\nValidation', num_epoch, 'valid_f1:',f'{f1:.3f}', 'best_f1:',f'{val_f1_min:.3f}', 'mean accuracy:' f'{val_acc:.3f}',\"\\t\")\n",
    "#     print('\\nValidation mean accuracy: ', f'{val_acc:.3f}', \"\\t\")\n",
    "#     print('\\nValidation mean f1 by classes: ',f1_classes, \"\\t\")\n",
    "\n",
    "np.save(os.path.join(results_directory, 'train_loss_sum'), train_loss_sum)\n",
    "np.save(os.path.join(results_directory, 'train_f1_sum'), train_f1_sum)\n",
    "np.save(os.path.join(results_directory, 'val_acc_sum'), val_acc_sum)\n",
    "np.save(os.path.join(results_directory, 'val_f1_sum'), val_f1_sum)\n",
    "np.save(os.path.join(results_directory, 'val_f1_classes'), val_f1_classes)\n",
    "print(dt.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f50e0613c18>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gamma = 0.0001\n",
    "# test_model = primitive_ABN((None,12), len(unique_classes), minimum_len, n=1)\n",
    "# test_model = edit_ABN_model((None,12), len(unique_classes), minimum_len, n=1)\n",
    "# test_model, _ = edit_ABN_model_loss((None,12), len(unique_classes), minimum_len,n=1, gamma = gamma)\n",
    "test_model,_ = endtoend_edit_ABN_model_loss((None,12), len(unique_classes), minimum_len,n=1, gamma = gamma, batch_size = batch_size)\n",
    "\n",
    "# results_directory = os.path.join(rootdir, 'results_20200626_V4_ABN_primitive_editL1_gamma0.0001_primitive_updated')\n",
    "# results_directory = os.path.join(rootdir, 'results_20200622_ABN_multiclass_V4_ABN')\n",
    "# results_directory = os.path.join(rootdir, 'results_20200622_ABN_multiclass_V4_ABN_edit')\n",
    "# results_directory = os.path.join(rootdir, 'results_'+date+'_ABN_primitive_n=7')\n",
    "# results_directory = os.path.join(rootdir, 'results_20200626_V4_ABN_primitive_editL2_gamma0.0001_primitive_updated')\n",
    "\n",
    "# test_model =  Model(inputs=main_input, outputs=main_output)\n",
    "# test_model.load_weights(os.path.join(results_directory, 'ecg_mel_E829L0.17'))\n",
    "# results_directory = os.path.join(rootdir, 'results_20200624_V4_ABN_edit_loss')\n",
    "\n",
    "\n",
    "# test_model.load_weights(os.path.join(results_directory, 'ECG_ABN_E227L0.04'))\n",
    "\n",
    "latest_test = tf.train.latest_checkpoint(results_directory)\n",
    "test_model.load_weights(latest_test)\n",
    "# test_model.load_weights(os.path.join(results_directory, 'ECG_ABN_E237L0.10'))\n",
    "# test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_acc, f1_classes, f1, m_acc, m_f1, m_sum = test(data_test, mel_directory, input_directory, class2index, minimum_len, test_model, x_mean_final, x_std_final)\n",
    "# test_acc, f1_classes, f1, m_acc, m_f1, m_sum = test_edit_final(data_test, mel_directory, input_directory, class2index, minimum_len, test_model, x_mean_final, x_std_final, p_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.711"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(test_acc, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.882, 0.889, 0.875, 0.734, 0.351, 0.58 , 0.925, 0.745, 0.508])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_classes.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.784"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.791"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(m_acc, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.963, 0.857, 0.667, 0.   , 0.821, 0.966, 0.962, 0.88 , 1.   ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_f1.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.926"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_sum.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "617.813px",
    "left": "1545.27px",
    "right": "20px",
    "top": "80.75px",
    "width": "296.719px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
