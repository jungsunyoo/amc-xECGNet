{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numba/errors.py:137: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow.contrib.eager as tfe\n",
    "\n",
    "\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import keras\n",
    "# import datetime as dt\n",
    "from datetime import datetime\n",
    "import time\n",
    "# import datetime.datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "# from keras import optimizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from keras.applications.densenet import DenseNet121, DenseNet169\n",
    "#from keras.applications.nasnet import NASNetLarge\n",
    "# from keras_efficientnets import EfficientNetB7\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "# from keras import backend as K\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "random.seed(100)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0012702   0.00038759  0.00402317 -0.0029567   0.00051957  0.00116427\n",
      " -0.0008405  -0.00222516 -0.0019119  -0.00093502  0.00129789  0.00177598]\n",
      "(5225,) (276,) (1376,)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def score_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "def get_unique_classes(input_directory,files):\n",
    "\n",
    "    unique_classes=set()\n",
    "    for f in files:\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    for c in tmp:\n",
    "                        unique_classes.add(c.strip())\n",
    "    return sorted(unique_classes)\n",
    "\n",
    "def one_hot_encoding(one_hot_vector,y, class2index):\n",
    "    ind=class2index[y]\n",
    "    one_hot_vector[ind]=1\n",
    "    return one_hot_vector\n",
    "\n",
    "# Search for multi-label subjects\n",
    "def searching_overlap(input_directory,class2index, input_file_names):\n",
    "    multiclasses=[]\n",
    "    multisubjects=[]\n",
    "    number = []\n",
    "    for file in input_file_names:\n",
    "        f=file\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    if len(tmp)>1:\n",
    "                        one_hot_vector = [0]*(len(class2index))\n",
    "                        for c in tmp:\n",
    "                            one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                        multiclasses.append(one_hot_vector)\n",
    "                        multisubjects.append(g)\n",
    "                        number.append(len(tmp))\n",
    "    return multisubjects, multiclasses, number\n",
    "\n",
    "def block_feature(sequence_en, minimum_len): \n",
    "    new_en = []\n",
    "    if len(sequence_en) > minimum_len:  # 길이가 minimum보다 긴 경우\n",
    "        start = random.randint(0,len(sequence_en)-minimum_len)\n",
    "        #print(start)\n",
    "        new_en = sequence_en[start:start+minimum_len]\n",
    "    elif len(sequence_en) == minimum_len: # 길이가 minimum\n",
    "        new_en = sequence_en\n",
    "    else: \n",
    "        assert len(sequence_en) <= minimum_len\n",
    "    return new_en\n",
    "\n",
    "def exploratory_look(input_directory,file, class2index):\n",
    "    classes = []\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                print(tmp, len(tmp))\n",
    "    return tmp     \n",
    "\n",
    "# Get classes of sorted file names\n",
    "def get_labels(input_directory,file, class2index):\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                one_hot_vector = [0]*(len(class2index))\n",
    "                for c in tmp:\n",
    "                    one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                \n",
    "    return one_hot_vector\n",
    "\n",
    "def randextract_mels(curr_step, batch_size, data, mel_directory, class2index, minimum_len, x_mean_final, x_std_final):\n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    start = batch_size*curr_step\n",
    "    end = batch_size*(curr_step+1)\n",
    "    curr_file_indices = data[start:end]\n",
    "    for file in curr_file_indices:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        clip_file = block_feature(tmp_file, minimum_len)\n",
    "        #print(clip_file.shape)\n",
    "        #clip_file = tmp_file[:minimum_len]\n",
    "        clip_file -= x_mean_final\n",
    "        clip_file /= x_std_final\n",
    "        mel_files.append(clip_file)\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        classes.append(label)\n",
    "    \n",
    "    # YJS added for ABN -> should calculate 2 losses\n",
    "#     classes_abn = [classes,classes]\n",
    "    \n",
    "    concat = list(zip(mel_files, classes))\n",
    "#     concat = list(zip(mel_files, classes_abn))\n",
    "    random.shuffle(concat)\n",
    "    mel_files, classes = zip(*concat)\n",
    "    return mel_files, classes\n",
    "\n",
    "def train(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final): \n",
    "    loss=[]\n",
    "#     acc = []\n",
    "    f1 = []\n",
    "\n",
    "    total_steps = int(np.ceil(len(data_train)/batch_size))\n",
    "    for curr_step in range(total_steps): # loops over batches\n",
    "        batch_mels, batch_labels = randextract_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "\n",
    "        batch_mels = np.asarray(batch_mels)\n",
    "        batch_labels = np.asarray(np.squeeze(batch_labels))\n",
    "        \n",
    "        batch_labels = [batch_labels, batch_labels]\n",
    "\n",
    "\n",
    "# return of train\n",
    "# 0 = total loss (attention branch + perception branch)\n",
    "# 1 = loss of attention pred\n",
    "# 2 = loss of perception pred\n",
    "# 3 = f1 of attention pred\n",
    "# 4 = f1 of perception pred     \n",
    "\n",
    "\n",
    "        train_tmp = model.train_on_batch(batch_mels, batch_labels)\n",
    "        loss_ = train_tmp[0]/2\n",
    "        f1_ = np.mean((train_tmp[3], train_tmp[4]))\n",
    "        loss.append(loss_)\n",
    "        f1.append(f1_)\n",
    "\n",
    "    loss = np.mean(np.array(loss))\n",
    "    f1 = np.mean(np.array(f1))\n",
    "    return loss, f1\n",
    "\n",
    "def train_edit(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final): \n",
    "    loss=[]\n",
    "#     acc = []\n",
    "    f1 = []\n",
    "\n",
    "    total_steps = int(np.ceil(len(data_train)/batch_size))\n",
    "    for curr_step in range(total_steps): # loops over batches\n",
    "        batch_mels, batch_labels = randextract_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "\n",
    "        batch_mels = np.asarray(batch_mels)\n",
    "        batch_labels = np.asarray(np.squeeze(batch_labels))\n",
    "\n",
    "\n",
    "        heatmap = CAM_conv1D(p_model, conv_layer, softmax_layer,  x_mean_final, x_std_final, minimum_len, \n",
    "                                  n_channels, batch_mels, batch_labels, out_len)        \n",
    "        heatmap = np.asarray(heatmap)\n",
    "        \n",
    "        batch_labels = [batch_labels, batch_labels]\n",
    "\n",
    "\n",
    "# return of train\n",
    "# 0 = total loss (attention branch + perception branch)\n",
    "# 1 = loss of attention pred\n",
    "# 2 = loss of perception pred\n",
    "# 3 = f1 of attention pred\n",
    "# 4 = f1 of perception pred     \n",
    "\n",
    "        train_tmp = model.train_on_batch([batch_mels, heatmap], batch_labels)\n",
    "        loss_ = train_tmp[0]/2\n",
    "        f1_ = np.mean((train_tmp[3], train_tmp[4]))\n",
    "        loss.append(loss_)\n",
    "        f1.append(f1_)\n",
    "\n",
    "    loss = np.mean(np.array(loss))\n",
    "    f1 = np.mean(np.array(f1))\n",
    "    return loss, f1\n",
    "\n",
    "def test(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    for i, file in enumerate(data):\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "#         gradcam_files = []\n",
    "        heatmap_files=[]\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        label = [label]\n",
    "        for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "\n",
    "            clip_file = clip_file.reshape(1,minimum_len,n_channels)\n",
    "\n",
    "\n",
    "            mel_files.append(clip_file)    \n",
    "#             gradcam_files.append(gradcam)\n",
    "\n",
    "        mel_files = np.asarray(mel_files)\n",
    "\n",
    "        mel_files = mel_files.reshape(steps,minimum_len,n_channels)\n",
    "\n",
    "        logit = model.predict(mel_files)\n",
    "        # YJS changed on 2020-06-02: input으로 두개 들어가야하니까 predict도 수정?\n",
    "#         print(len(logit))\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "#         print(logit)\n",
    "        pred = np.argmax(logit)\n",
    "#         print('Pred={}'.format(pred))\n",
    "        \n",
    "        label = np.argmax(get_labels(input_directory, file, class2index))\n",
    "#         print('Label={}'.format(label))\n",
    "        #f1 = f1_score(label, logit)\n",
    "        #print(pred, label)\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        #total_f1 += f1\n",
    "    final_acc = total_acc / i\n",
    "    #final_f1 = total_f1 / i\n",
    "    return final_acc#, final_f1\n",
    "\n",
    "def test_edit(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    for i, file in enumerate(data):\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "#         gradcam_files = []\n",
    "        heatmap_files=[]\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        label = [label]\n",
    "        for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "#             heatmap = CAM_conv1D(p_model, conv_layer, softmax_layer,  x_mean_final, x_std_final, minimum_len, \n",
    "#                                       n_channels, clip_file, label, out_len)\n",
    "            \n",
    "#             heatmap = grad_cam_conv1D(p_model, layer_nm, x_mean_final, x_std_final, minimum_len, n_channels, \n",
    "#                                      clip_file, label, out_len, sample_weight=1, keras_phase=0)\n",
    "            clip_file = clip_file.reshape(1,minimum_len,n_channels)\n",
    "            heatmap = CAM_conv1D(p_model, conv_layer, softmax_layer,  x_mean_final, x_std_final, minimum_len, \n",
    "                                      n_channels, clip_file, label, out_len)\n",
    "#             print(len(heatmap))\n",
    "#             print(len(heatmap[0]))\n",
    "#             print(len(heatmap[0][0]))\n",
    "#             print(len(heatmap[0][0][0]))\n",
    "            mel_files.append(clip_file)    \n",
    "#             gradcam_files.append(gradcam)\n",
    "            heatmap_files.append(heatmap)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "        heatmap_files = np.asarray(heatmap_files)\n",
    "        \n",
    "#         print(heatmap_files.shape)\n",
    "        heatmap_files = heatmap_files.reshape(steps, out_len,1)\n",
    "        mel_files = mel_files.reshape(steps,minimum_len,n_channels)\n",
    "#         heatmap = np.asarray(heatmap)\n",
    "#         gradcam_files = np.asarray(gradcam_files)\n",
    "# grad_cam_conv1D(model, layer_nm,x_mean_final, x_std_final, minimum_len, n_channels, x, y,sample_weight=1,  keras_phase=0)        \n",
    "        \n",
    "# get_labels(input_directory,file, class2index)\n",
    "\n",
    "\n",
    "\n",
    "#         logit = model.predict(mel_files)\n",
    "#         logit = model.predict([mel_files, gradcam_files])\n",
    "        logit = model.predict([mel_files, heatmap_files])\n",
    "        # YJS changed on 2020-06-02: input으로 두개 들어가야하니까 predict도 수정?\n",
    "#         print(len(logit))\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "#         print(logit)\n",
    "        pred = np.argmax(logit)\n",
    "#         print('Pred={}'.format(pred))\n",
    "        \n",
    "        label = np.argmax(get_labels(input_directory, file, class2index))\n",
    "#         print('Label={}'.format(label))\n",
    "        #f1 = f1_score(label, logit)\n",
    "        #print(pred, label)\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        #total_f1 += f1\n",
    "    final_acc = total_acc / i\n",
    "    #final_f1 = total_f1 / i\n",
    "    return final_acc#, final_f1\n",
    "\n",
    "\n",
    "def test_short(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    for i, file in enumerate(data):\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "#         gradcam_files = []\n",
    "        heatmap_files=[]\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        label = [label]\n",
    "#         for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "        steps=1\n",
    "        block = 0\n",
    "        start = block*minimum_len\n",
    "        end = (block+1)*minimum_len\n",
    "        clip_file = tmp_file[start:end]\n",
    "        clip_file -= x_mean_final\n",
    "        clip_file /= x_std_final\n",
    "\n",
    "        clip_file = clip_file.reshape(1,minimum_len,n_channels)\n",
    "#         heatmap = CAM_conv1D(p_model, conv_layer, softmax_layer,  x_mean_final, x_std_final, minimum_len, \n",
    "#                                       n_channels, clip_file, label, out_len)\n",
    "\n",
    "        mel_files.append(clip_file)    \n",
    "#         heatmap_files.append(heatmap)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "#         heatmap_files = np.asarray(heatmap_files)\n",
    "#         heatmap_files = heatmap_files.reshape(steps,out_len,1) # changed for modified attention editting\n",
    "        mel_files = mel_files.reshape(steps,minimum_len,n_channels)\n",
    "\n",
    "        logit = model.predict([mel_files, heatmap_files])\n",
    "        # YJS changed on 2020-06-02: input으로 두개 들어가야하니까 predict도 수정?\n",
    "\n",
    "        \n",
    "        logit = np.mean(logit, axis=0)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        pred = np.argmax(logit)\n",
    "        \n",
    "        label = np.argmax(get_labels(input_directory, file, class2index))\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "\n",
    "        total_acc += acc\n",
    "\n",
    "    final_acc = total_acc / i\n",
    "\n",
    "    return final_acc#, final_f1\n",
    "batch_size = 32#16#20#32#5#2#1#10#32\n",
    "minimum_len = 2880\n",
    "epochs = 1000\n",
    "loss_function = 'binary_crossentropy' #'categorical_crossentropy'\n",
    "activation_function = 'sigmoid' # 'softmax'\n",
    "rootdir = '../'\n",
    "date = datetime.today().strftime(\"%Y%m%d\")\n",
    "input_directory = os.path.join(rootdir, 'Training_WFDB')\n",
    "mel_name = 'Raw_data_20200424' \n",
    "mel_directory = os.path.join(rootdir, mel_name)\n",
    "# results_directory = os.path.join(rootdir, 'results_'+date+'_0_IEEE_n=1')\n",
    "results_directory = os.path.join(rootdir, 'results_'+date+'_editting_CAM_multiclass')\n",
    "if not os.path.isdir(input_directory):\n",
    "    os.mkdir(input_directory)\n",
    "if not os.path.isdir(mel_directory):\n",
    "    os.mkdir(mel_directory)\n",
    "if not os.path.isdir(results_directory):\n",
    "    os.mkdir(results_directory)\n",
    "        \n",
    "input_files = []\n",
    "for f in os.listdir(input_directory):\n",
    "    if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('mat'):\n",
    "        input_files.append(f)\n",
    "input_file_names = sorted(input_files)\n",
    "\n",
    "unique_classes = get_unique_classes(input_directory, input_files)\n",
    "class2index = {}\n",
    "for a, b in enumerate(unique_classes):\n",
    "    class2index[b] = a\n",
    "    \n",
    "# classes_orig= [x.replace('.mat', '.hea') for x in input_file_names] # total subjects\n",
    "# classes_multi, _, _ = searching_overlap(input_directory,class2index, input_file_names)\n",
    "# classes_single = [x for x in classes_orig if x not in classes_multi]\n",
    "# classes_single = [x.replace('.hea', '.mat') for x in classes_single]\n",
    "# classes_orig = \n",
    "\n",
    "# double-checking if classes_single have single-label\n",
    "# a, b, c  = searching_overlap(input_directory,class2index,classes_single)\n",
    "\n",
    "# we can safely use classes_single as input_file_names\n",
    "# input_file_names = classes_single\n",
    "random.shuffle(input_file_names)\n",
    "np.shape(input_file_names)\n",
    "\n",
    "x_mean_all = []\n",
    "x_std_all = []\n",
    "for file in input_file_names:\n",
    "    x = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "    x_mean = [np.mean(x[:,0]), np.mean(x[:,1]), np.mean(x[:,2]), np.mean(x[:,3]), np.mean(x[:,4]), np.mean(x[:,5]),\n",
    "             np.mean(x[:,6]), np.mean(x[:,7]), np.mean(x[:,8]), np.mean(x[:,9]), np.mean(x[:,10]), np.mean(x[:,11])]\n",
    "    \n",
    "    x_std = [np.std(x[:,0]), np.std(x[:,1]), np.std(x[:,2]), np.std(x[:,3]), np.std(x[:,4]), np.std(x[:,5]),\n",
    "             np.std(x[:,6]), np.std(x[:,7]), np.std(x[:,8]), np.std(x[:,9]), np.std(x[:,10]), np.std(x[:,11])]\n",
    "    #print(x_mean)\n",
    "    x_mean_all.append(x_mean)\n",
    "    x_std_all.append(x_std) # yjs corrected on 2020-05-25\n",
    "x_mean_final = np.mean(x_mean_all, axis=0)\n",
    "x_std_final = np.mean(x_std_all, axis=0)\n",
    "print(x_mean_final)\n",
    "\n",
    "data, data_test = train_test_split(input_file_names, test_size = 0.2, train_size = 0.8, shuffle=True)\n",
    "data_train, data_val = train_test_split(data, test_size = 0.05, train_size = 0.95, shuffle=True)\n",
    "print(np.shape(data_train), np.shape(data_val), np.shape(data_test))\n",
    "\n",
    "\n",
    "main_input = Input(shape=(minimum_len,12), dtype='float32', name='main_input')\n",
    "\n",
    "branch_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_orig= [x.replace('.mat', '.hea') for x in input_file_names] # total subjects\n",
    "classes_multi, _, _ = searching_overlap(input_directory,class2index, input_file_names)\n",
    "classes_single = [x for x in classes_orig if x not in classes_multi]\n",
    "classes_single = [x.replace('.hea', '.mat') for x in classes_single]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A4459.hea',\n",
       " 'A5188.hea',\n",
       " 'A0182.hea',\n",
       " 'A5051.hea',\n",
       " 'A6327.hea',\n",
       " 'A6807.hea',\n",
       " 'A2514.hea',\n",
       " 'A3895.hea',\n",
       " 'A6474.hea',\n",
       " 'A1027.hea',\n",
       " 'A2392.hea',\n",
       " 'A3620.hea',\n",
       " 'A3678.hea',\n",
       " 'A5190.hea',\n",
       " 'A5795.hea',\n",
       " 'A0224.hea',\n",
       " 'A1870.hea',\n",
       " 'A6232.hea',\n",
       " 'A2357.hea',\n",
       " 'A6360.hea',\n",
       " 'A6250.hea',\n",
       " 'A2027.hea',\n",
       " 'A6211.hea',\n",
       " 'A4075.hea',\n",
       " 'A2278.hea',\n",
       " 'A1043.hea',\n",
       " 'A1689.hea',\n",
       " 'A1164.hea',\n",
       " 'A3140.hea',\n",
       " 'A5441.hea',\n",
       " 'A0881.hea',\n",
       " 'A5412.hea',\n",
       " 'A1344.hea',\n",
       " 'A3681.hea',\n",
       " 'A2652.hea',\n",
       " 'A2078.hea',\n",
       " 'A2653.hea',\n",
       " 'A1326.hea',\n",
       " 'A2553.hea',\n",
       " 'A3955.hea',\n",
       " 'A2503.hea',\n",
       " 'A4716.hea',\n",
       " 'A4993.hea',\n",
       " 'A3512.hea',\n",
       " 'A0607.hea',\n",
       " 'A5971.hea',\n",
       " 'A4648.hea',\n",
       " 'A4536.hea',\n",
       " 'A3009.hea',\n",
       " 'A4178.hea',\n",
       " 'A5794.hea',\n",
       " 'A4992.hea',\n",
       " 'A6234.hea',\n",
       " 'A5181.hea',\n",
       " 'A6394.hea',\n",
       " 'A5488.hea',\n",
       " 'A0575.hea',\n",
       " 'A0835.hea',\n",
       " 'A4408.hea',\n",
       " 'A4352.hea',\n",
       " 'A4223.hea',\n",
       " 'A4829.hea',\n",
       " 'A4306.hea',\n",
       " 'A2330.hea',\n",
       " 'A1108.hea',\n",
       " 'A3231.hea',\n",
       " 'A2878.hea',\n",
       " 'A2282.hea',\n",
       " 'A2572.hea',\n",
       " 'A2898.hea',\n",
       " 'A3491.hea',\n",
       " 'A6138.hea',\n",
       " 'A6302.hea',\n",
       " 'A6220.hea',\n",
       " 'A0883.hea',\n",
       " 'A6161.hea',\n",
       " 'A1340.hea',\n",
       " 'A5413.hea',\n",
       " 'A3483.hea',\n",
       " 'A2699.hea',\n",
       " 'A5408.hea',\n",
       " 'A3065.hea',\n",
       " 'A4359.hea',\n",
       " 'A0667.hea',\n",
       " 'A0833.hea',\n",
       " 'A3119.hea',\n",
       " 'A3760.hea',\n",
       " 'A3205.hea',\n",
       " 'A5576.hea',\n",
       " 'A5029.hea',\n",
       " 'A3967.hea',\n",
       " 'A5810.hea',\n",
       " 'A0563.hea',\n",
       " 'A3652.hea',\n",
       " 'A2310.hea',\n",
       " 'A2520.hea',\n",
       " 'A5643.hea',\n",
       " 'A5489.hea',\n",
       " 'A6152.hea',\n",
       " 'A5581.hea',\n",
       " 'A1410.hea',\n",
       " 'A0877.hea',\n",
       " 'A2747.hea',\n",
       " 'A0557.hea',\n",
       " 'A2221.hea',\n",
       " 'A6038.hea',\n",
       " 'A5978.hea',\n",
       " 'A0695.hea',\n",
       " 'A5967.hea',\n",
       " 'A5042.hea',\n",
       " 'A1514.hea',\n",
       " 'A0163.hea',\n",
       " 'A2565.hea',\n",
       " 'A0353.hea',\n",
       " 'A6320.hea',\n",
       " 'A6382.hea',\n",
       " 'A2339.hea',\n",
       " 'A6174.hea',\n",
       " 'A5074.hea',\n",
       " 'A1890.hea',\n",
       " 'A6674.hea',\n",
       " 'A2791.hea',\n",
       " 'A1562.hea',\n",
       " 'A3063.hea',\n",
       " 'A2096.hea',\n",
       " 'A5141.hea',\n",
       " 'A2114.hea',\n",
       " 'A3761.hea',\n",
       " 'A2100.hea',\n",
       " 'A0213.hea',\n",
       " 'A0782.hea',\n",
       " 'A5885.hea',\n",
       " 'A2256.hea',\n",
       " 'A2173.hea',\n",
       " 'A4388.hea',\n",
       " 'A0218.hea',\n",
       " 'A4616.hea',\n",
       " 'A5578.hea',\n",
       " 'A5715.hea',\n",
       " 'A5142.hea',\n",
       " 'A3837.hea',\n",
       " 'A0202.hea',\n",
       " 'A1073.hea',\n",
       " 'A1074.hea',\n",
       " 'A0593.hea',\n",
       " 'A5781.hea',\n",
       " 'A5775.hea',\n",
       " 'A2794.hea',\n",
       " 'A4039.hea',\n",
       " 'A1044.hea',\n",
       " 'A0658.hea',\n",
       " 'A4873.hea',\n",
       " 'A5627.hea',\n",
       " 'A5589.hea',\n",
       " 'A5770.hea',\n",
       " 'A0502.hea',\n",
       " 'A6816.hea',\n",
       " 'A0971.hea',\n",
       " 'A2267.hea',\n",
       " 'A0079.hea',\n",
       " 'A5309.hea',\n",
       " 'A5683.hea',\n",
       " 'A2438.hea',\n",
       " 'A5590.hea',\n",
       " 'A4615.hea',\n",
       " 'A2654.hea',\n",
       " 'A4614.hea',\n",
       " 'A1794.hea',\n",
       " 'A6834.hea',\n",
       " 'A5010.hea',\n",
       " 'A0308.hea',\n",
       " 'A3595.hea',\n",
       " 'A4028.hea',\n",
       " 'A4565.hea',\n",
       " 'A5115.hea',\n",
       " 'A5193.hea',\n",
       " 'A5969.hea',\n",
       " 'A5801.hea',\n",
       " 'A4652.hea',\n",
       " 'A1487.hea',\n",
       " 'A6316.hea',\n",
       " 'A1171.hea',\n",
       " 'A6067.hea',\n",
       " 'A1544.hea',\n",
       " 'A1061.hea',\n",
       " 'A1094.hea',\n",
       " 'A0043.hea',\n",
       " 'A5308.hea',\n",
       " 'A3979.hea',\n",
       " 'A2430.hea',\n",
       " 'A6291.hea',\n",
       " 'A3536.hea',\n",
       " 'A4817.hea',\n",
       " 'A0670.hea',\n",
       " 'A3026.hea',\n",
       " 'A3309.hea',\n",
       " 'A1193.hea',\n",
       " 'A6219.hea',\n",
       " 'A3089.hea',\n",
       " 'A3360.hea',\n",
       " 'A1656.hea',\n",
       " 'A1937.hea',\n",
       " 'A2161.hea',\n",
       " 'A5597.hea',\n",
       " 'A0384.hea',\n",
       " 'A0237.hea',\n",
       " 'A2214.hea',\n",
       " 'A0796.hea',\n",
       " 'A5929.hea',\n",
       " 'A1666.hea',\n",
       " 'A1236.hea',\n",
       " 'A3209.hea',\n",
       " 'A1306.hea',\n",
       " 'A1841.hea',\n",
       " 'A1740.hea',\n",
       " 'A4344.hea',\n",
       " 'A1818.hea',\n",
       " 'A6872.hea',\n",
       " 'A0685.hea',\n",
       " 'A5362.hea',\n",
       " 'A0649.hea',\n",
       " 'A0439.hea',\n",
       " 'A6031.hea',\n",
       " 'A6790.hea',\n",
       " 'A4501.hea',\n",
       " 'A6862.hea',\n",
       " 'A1678.hea',\n",
       " 'A5650.hea',\n",
       " 'A3024.hea',\n",
       " 'A2795.hea',\n",
       " 'A4430.hea',\n",
       " 'A4112.hea',\n",
       " 'A5282.hea',\n",
       " 'A1228.hea',\n",
       " 'A6180.hea',\n",
       " 'A1576.hea',\n",
       " 'A1532.hea',\n",
       " 'A4292.hea',\n",
       " 'A4392.hea',\n",
       " 'A4815.hea',\n",
       " 'A6473.hea',\n",
       " 'A6527.hea',\n",
       " 'A4271.hea',\n",
       " 'A0152.hea',\n",
       " 'A3626.hea',\n",
       " 'A1047.hea',\n",
       " 'A4356.hea',\n",
       " 'A4919.hea',\n",
       " 'A1278.hea',\n",
       " 'A0919.hea',\n",
       " 'A1654.hea',\n",
       " 'A6486.hea',\n",
       " 'A6629.hea',\n",
       " 'A3596.hea',\n",
       " 'A2486.hea',\n",
       " 'A3995.hea',\n",
       " 'A0385.hea',\n",
       " 'A4753.hea',\n",
       " 'A0857.hea',\n",
       " 'A0503.hea',\n",
       " 'A1452.hea',\n",
       " 'A1328.hea',\n",
       " 'A2607.hea',\n",
       " 'A3741.hea',\n",
       " 'A2581.hea',\n",
       " 'A1780.hea',\n",
       " 'A4964.hea',\n",
       " 'A1559.hea',\n",
       " 'A3330.hea',\n",
       " 'A6149.hea',\n",
       " 'A6223.hea',\n",
       " 'A4998.hea',\n",
       " 'A2789.hea',\n",
       " 'A4308.hea',\n",
       " 'A5093.hea',\n",
       " 'A0300.hea',\n",
       " 'A5445.hea',\n",
       " 'A6177.hea',\n",
       " 'A2309.hea',\n",
       " 'A3264.hea',\n",
       " 'A4475.hea',\n",
       " 'A5799.hea',\n",
       " 'A6092.hea',\n",
       " 'A6116.hea',\n",
       " 'A5782.hea',\n",
       " 'A1003.hea',\n",
       " 'A4481.hea',\n",
       " 'A4199.hea',\n",
       " 'A2297.hea',\n",
       " 'A4710.hea',\n",
       " 'A3324.hea',\n",
       " 'A6048.hea',\n",
       " 'A0643.hea',\n",
       " 'A1408.hea',\n",
       " 'A1065.hea',\n",
       " 'A0109.hea',\n",
       " 'A1950.hea',\n",
       " 'A6237.hea',\n",
       " 'A5251.hea',\n",
       " 'A1118.hea',\n",
       " 'A1871.hea',\n",
       " 'A5724.hea',\n",
       " 'A0932.hea',\n",
       " 'A6075.hea',\n",
       " 'A5684.hea',\n",
       " 'A2080.hea',\n",
       " 'A2232.hea',\n",
       " 'A1434.hea',\n",
       " 'A4277.hea',\n",
       " 'A1317.hea',\n",
       " 'A6396.hea',\n",
       " 'A2436.hea',\n",
       " 'A3823.hea',\n",
       " 'A4991.hea',\n",
       " 'A4673.hea',\n",
       " 'A2079.hea',\n",
       " 'A6632.hea',\n",
       " 'A1648.hea',\n",
       " 'A6714.hea',\n",
       " 'A3798.hea',\n",
       " 'A2374.hea',\n",
       " 'A5823.hea',\n",
       " 'A2011.hea',\n",
       " 'A6817.hea',\n",
       " 'A4756.hea',\n",
       " 'A3391.hea',\n",
       " 'A2602.hea',\n",
       " 'A5815.hea',\n",
       " 'A0464.hea',\n",
       " 'A1185.hea',\n",
       " 'A4080.hea',\n",
       " 'A0692.hea',\n",
       " 'A6024.hea',\n",
       " 'A5135.hea',\n",
       " 'A3911.hea',\n",
       " 'A0884.hea',\n",
       " 'A2625.hea',\n",
       " 'A5211.hea',\n",
       " 'A2877.hea',\n",
       " 'A5762.hea',\n",
       " 'A4960.hea',\n",
       " 'A3107.hea',\n",
       " 'A4181.hea',\n",
       " 'A3407.hea',\n",
       " 'A6328.hea',\n",
       " 'A6099.hea',\n",
       " 'A5087.hea',\n",
       " 'A1055.hea',\n",
       " 'A2107.hea',\n",
       " 'A5113.hea',\n",
       " 'A4353.hea',\n",
       " 'A2964.hea',\n",
       " 'A3262.hea',\n",
       " 'A6592.hea',\n",
       " 'A5222.hea',\n",
       " 'A3040.hea',\n",
       " 'A0979.hea',\n",
       " 'A3167.hea',\n",
       " 'A5972.hea',\n",
       " 'A6856.hea',\n",
       " 'A1527.hea',\n",
       " 'A5952.hea',\n",
       " 'A0374.hea',\n",
       " 'A3460.hea',\n",
       " 'A1592.hea',\n",
       " 'A0414.hea',\n",
       " 'A2431.hea',\n",
       " 'A2175.hea',\n",
       " 'A2265.hea',\n",
       " 'A4142.hea',\n",
       " 'A1298.hea',\n",
       " 'A1378.hea',\n",
       " 'A1262.hea',\n",
       " 'A0675.hea',\n",
       " 'A2106.hea',\n",
       " 'A4518.hea',\n",
       " 'A2296.hea',\n",
       " 'A0431.hea',\n",
       " 'A5443.hea',\n",
       " 'A5263.hea',\n",
       " 'A5037.hea',\n",
       " 'A1852.hea',\n",
       " 'A4629.hea',\n",
       " 'A1860.hea',\n",
       " 'A4300.hea',\n",
       " 'A0412.hea',\n",
       " 'A3589.hea',\n",
       " 'A1539.hea',\n",
       " 'A2935.hea',\n",
       " 'A5127.hea',\n",
       " 'A4567.hea',\n",
       " 'A4231.hea',\n",
       " 'A2618.hea',\n",
       " 'A3371.hea',\n",
       " 'A1041.hea',\n",
       " 'A6087.hea',\n",
       " 'A5100.hea',\n",
       " 'A0661.hea',\n",
       " 'A1233.hea',\n",
       " 'A5215.hea',\n",
       " 'A5145.hea',\n",
       " 'A0322.hea',\n",
       " 'A0848.hea',\n",
       " 'A5129.hea',\n",
       " 'A6409.hea',\n",
       " 'A5874.hea',\n",
       " 'A6296.hea',\n",
       " 'A2732.hea',\n",
       " 'A1324.hea',\n",
       " 'A0834.hea',\n",
       " 'A3740.hea',\n",
       " 'A0804.hea',\n",
       " 'A0862.hea',\n",
       " 'A4499.hea',\n",
       " 'A4578.hea',\n",
       " 'A6469.hea',\n",
       " 'A5807.hea',\n",
       " 'A3072.hea',\n",
       " 'A3949.hea',\n",
       " 'A0251.hea',\n",
       " 'A2176.hea',\n",
       " 'A5095.hea',\n",
       " 'A6798.hea',\n",
       " 'A0407.hea',\n",
       " 'A5760.hea',\n",
       " 'A5392.hea',\n",
       " 'A3507.hea',\n",
       " 'A0478.hea',\n",
       " 'A4713.hea',\n",
       " 'A1029.hea',\n",
       " 'A4331.hea',\n",
       " 'A1621.hea',\n",
       " 'A6127.hea',\n",
       " 'A4339.hea',\n",
       " 'A6148.hea',\n",
       " 'A6760.hea',\n",
       " 'A4879.hea',\n",
       " 'A2013.hea',\n",
       " 'A1732.hea',\n",
       " 'A1505.hea',\n",
       " 'A4245.hea',\n",
       " 'A1389.hea',\n",
       " 'A3126.hea',\n",
       " 'A2872.hea',\n",
       " 'A5304.hea',\n",
       " 'A5085.hea',\n",
       " 'A1859.hea',\n",
       " 'A5517.hea',\n",
       " 'A0854.hea',\n",
       " 'A6218.hea',\n",
       " 'A3618.hea',\n",
       " 'A3224.hea',\n",
       " 'A0910.hea',\n",
       " 'A6113.hea',\n",
       " 'A5968.hea',\n",
       " 'A0112.hea',\n",
       " 'A5060.hea',\n",
       " 'A0277.hea',\n",
       " 'A3640.hea',\n",
       " 'A3299.hea',\n",
       " 'A6525.hea',\n",
       " 'A4898.hea',\n",
       " 'A3493.hea',\n",
       " 'A3294.hea',\n",
       " 'A1624.hea',\n",
       " 'A5843.hea',\n",
       " 'A5939.hea',\n",
       " 'A1774.hea',\n",
       " 'A6176.hea',\n",
       " 'A2069.hea',\n",
       " 'A5849.hea',\n",
       " 'A4990.hea',\n",
       " 'A6201.hea',\n",
       " 'A5231.hea',\n",
       " 'A2535.hea',\n",
       " 'A2158.hea']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
       " [0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       " [0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
       " [0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 1],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       " [0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 1],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       " [0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       " [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       " [0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       " [1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       " [0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       " [0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       " [0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       " [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0, 1, 0]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabels=[]\n",
    "for file in classes_multi:\n",
    "    label = get_labels(input_directory, file, class2index)\n",
    "    multilabels.append(label)\n",
    "multilabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "476"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(multilabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AF': 0,\n",
       " 'I-AVB': 1,\n",
       " 'LBBB': 2,\n",
       " 'Normal': 3,\n",
       " 'PAC': 4,\n",
       " 'PVC': 5,\n",
       " 'RBBB': 6,\n",
       " 'STD': 7,\n",
       " 'STE': 8}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A4459.hea\n",
      "[1, 0, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(classes_multi[0])\n",
    "print(multilabels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# val_acc = test_short(data_val, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention editting by CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_x = batch_mels[0]\n",
    "# curr_x = curr_x.reshape(1,minimum_len,n_channels)\n",
    "# print(curr_x.shape)\n",
    "# softmax_layer = 'dense_final'\n",
    "# softmax_out = p_model.get_layer(softmax_layer)\n",
    "# softmax_weights = softmax_out.weights\n",
    "\n",
    "# conv_layer = 'batch_normalization_12'\n",
    "# conv_out = p_model.get_layer(conv_layer).output\n",
    "# get_conv_out = K.function(p_model.input, [conv_out, softmax_weights[0]])\n",
    "# conv_out, softmax_weights = get_conv_out(curr_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Batch-wise to reduce time\n",
    "\n",
    "CAMdir = '/home/taejoon/PhysioNetChallenge/results_20200601_1_CAM_primitive_model'\n",
    "bestmodel = 'ECG_ABN_E87L0.33'\n",
    "p_model = tf.keras.models.load_model(os.path.join(CAMdir, bestmodel), custom_objects={'score_f1' : score_f1}) # primitive model\n",
    "conv_layer = 'batch_normalization_12'\n",
    "softmax_layer = 'dense_final'\n",
    "n_channels=12\n",
    "out_len=12#128\n",
    "softmax_weights = p_model.get_layer('dense_final').weights[0]\n",
    "# model.get_layer(softmax_layer).weights[0]\n",
    "def CAM_conv1D(model, conv_layer, softmax_layer, x_mean_final, x_std_final, minimum_len, \n",
    "                    n_channels, x, y, out_len):\n",
    "    \n",
    "    # x랑 y는 batch size만큼의 리스트 (32)\n",
    "    heatmaps=[]    \n",
    "    \n",
    "    curr_x = np.asarray(x)\n",
    "    curr_x = curr_x.reshape(len(x),minimum_len,n_channels)\n",
    "    get_conv_out = K.function(model.input, [model.get_layer(conv_layer).output, model.get_layer(softmax_layer).weights[0]])\n",
    "    conv_out, softmax_weights = get_conv_out(curr_x)\n",
    "\n",
    "    for i in range(len(x)):\n",
    "\n",
    "        curr_classes = y[i]\n",
    "        class_index=[]\n",
    "        [class_index.append(j) for j in range(len(curr_classes)) if curr_classes[j]==1]\n",
    "        heatmap=np.zeros((1,36)) # might need to fix this if GradCAM or primitive model changes\n",
    "\n",
    "        conv_out_ = conv_out[i] # (36, 128)\n",
    "\n",
    "\n",
    "        for label in class_index:  # multiclass일 경우 대비해서 for문\n",
    "            curr_weights = softmax_weights[:,label]\n",
    "            weighted_conv = conv_out_*curr_weights\n",
    "            \n",
    "            weighted_conv = weighted_conv.sum(axis=-1) # output = (1,36)\n",
    "            heatmap += weighted_conv\n",
    "            \n",
    "\n",
    "        heatmap %= len(class_index) # 단일 class일 경우 1로 나눠짐. 두개일 경우 더해진 heatmap들이 2로 나눠짐\n",
    "#         heatmap = np.resize(heatmap, (1,out_len))\n",
    "        heatmap = np.resize(heatmap, (out_len, 1))\n",
    "        heatmaps.append(heatmap)\n",
    "        \n",
    "    return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# heatmap = CAM_conv1D(p_model, conv_layer, softmax_layer,  x_mean_final, x_std_final, minimum_len, \n",
    "#                                   n_channels, batch_mels, batch_labels, out_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(heatmap[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(heatmap[0][0][0]) # 32, 1, 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_x, _ = randextract_mels(0,32, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "# curr_x = np.asarray(curr_x)\n",
    "# curr_x.shape\n",
    "# curr_x = curr_x.reshape(32,minimum_len,12)\n",
    "\n",
    "# # curr_x = curr_x[0]\n",
    "# # curr_x = curr_x.reshape(1,minimum_len,12)\n",
    "# CAMdir = '/home/taejoon/PhysioNetChallenge/results_20200601_1_CAM_primitive_model'\n",
    "# bestmodel = 'ECG_ABN_E87L0.33'\n",
    "# p_model = tf.keras.models.load_model(os.path.join(CAMdir, bestmodel), custom_objects={'score_f1' : score_f1}) # primitive model\n",
    "# conv_layer = 'batch_normalization_12'\n",
    "# softmax_layer = 'dense_final'\n",
    "# n_channels=12\n",
    "# out_len=128\n",
    "# softmax_weights = p_model.get_layer('dense_final').weights[0]\n",
    "\n",
    "# get_conv_out = K.function(p_model.input, p_model.get_layer(conv_layer).output)\n",
    "\n",
    "# # conv_out = get_conv_out(curr_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_out = get_conv_out(curr_x)\n",
    "# conv_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv_out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAMdir = '/home/taejoon/PhysioNetChallenge/results_20200601_1_CAM_primitive_model'\n",
    "# bestmodel = 'ECG_ABN_E87L0.33'\n",
    "# p_model = tf.keras.models.load_model(os.path.join(CAMdir, bestmodel), custom_objects={'score_f1' : score_f1}) # primitive model\n",
    "# conv_layer = 'batch_normalization_12'\n",
    "# softmax_layer = 'dense_final'\n",
    "# n_channels=12\n",
    "# out_len=128\n",
    "# def CAM_conv1D(model, conv_layer, softmax_layer, x_mean_final, x_std_final, minimum_len, \n",
    "#                     n_channels, x, y, out_len):\n",
    "    \n",
    "#     # x랑 y는 batch size만큼의 리스트 (32)\n",
    "       \n",
    "#     #레이어 이름에 해당되는 레이어 정보를 가져옴 \n",
    "# #     conv_out = model.get_layer(conv_layer).output\n",
    "# #     print(conv_out)\n",
    "# #     conv_output= conv_out.output\n",
    "# #     softmax_out = model.get_layer(softmax_layer)\n",
    "# #     softmax_weights = softmax_out.weights\n",
    "#     heatmaps=[]    \n",
    "#     for file in range(len(x)): # 굳이 이렇게 batch 따로돌리는 이유는 나중에 multilable일 때 heatmap 두개일 경우를 대비하기위해\n",
    "#         # 일단은 single label로 하기 \n",
    "#         class_index=[]\n",
    "#         curr_x  = x[file]\n",
    "#         curr_x = curr_x.reshape(1,minimum_len,n_channels)\n",
    "#         curr_classes = y[file]\n",
    "#         [class_index.append(i) for i in range(len(curr_classes)) if curr_classes[i]==1]\n",
    "#         heatmap=np.zeros((1,36)) # might need to fix this if GradCAM or primitive model changes\n",
    "# #         curr_x = tf.convert_to_tensor(curr_x)\n",
    "#         get_conv_out = K.function(model.input, [model.get_layer(conv_layer).output, model.get_layer(softmax_layer).weights[0]])\n",
    "#         conv_out, softmax_weights = get_conv_out(curr_x)\n",
    "#         print(file)\n",
    "#         for label in class_index:  # multiclass일 경우 대비해서 for문\n",
    "#             curr_weights = softmax_weights[:,label]\n",
    "#             weighted_conv = conv_out*curr_weights\n",
    "#             weighted_conv = weighted_conv.sum(axis=2) # output = (1,36)            \n",
    "#             heatmap += weighted_conv\n",
    "            \n",
    "#         heatmap %= len(class_index) # 단일 class일 경우 1로 나눠짐. 두개일 경우 더해진 heatmap들이 2로 나눠짐\n",
    "#         heatmap = np.resize(heatmap, (1,out_len))\n",
    "# #         heatmap = cv2.resize(heatmap, (1,out_len))\n",
    "#         heatmaps.append(heatmap)\n",
    "        \n",
    "#     return heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention editting by GradCAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradCAMdir = '/home/taejoon/PhysioNetChallenge/results_20200601_1_CAM_primitive_model'\n",
    "# bestmodel = 'ECG_ABN_E87L0.33'\n",
    "# p_model = tf.keras.models.load_model(os.path.join(GradCAMdir, bestmodel), custom_objects={'score_f1' : score_f1}) # primitive model\n",
    "# layer_nm = 'conv1d_12'\n",
    "# n_channels=12\n",
    "# out_len=128\n",
    "# def grad_cam_conv1D(model, layer_nm,x_mean_final, x_std_final, minimum_len, \n",
    "#                     n_channels, x, y, out_len,sample_weight=1,  keras_phase=0):\n",
    "    \n",
    "#     # x랑 y는 batch size만큼의 리스트 (32)\n",
    "       \n",
    "#     #레이어 이름에 해당되는 레이어 정보를 가져옴 \n",
    "#     layers_wt = model.get_layer(layer_nm).weights\n",
    "#     layers = model.get_layer(layer_nm)\n",
    "#     layers_weights = model.get_layer(layer_nm).get_weights()\n",
    "\n",
    "#     heatmaps=[]    \n",
    "#     for file in range(len(x)): # 굳이 이렇게 batch 따로돌리는 이유는 나중에 multilable일 때 heatmap 두개일 경우를 대비하기위해\n",
    "#         # 일단은 single label로 하기 \n",
    "#         print(file)\n",
    "#         class_index=[]\n",
    "#         curr_x  = x[file]\n",
    "# #         print(curr_x.shape)\n",
    "#         curr_x = curr_x.reshape(1,minimum_len,n_channels)\n",
    "#         curr_classes = y[file]\n",
    "# #         print(curr_classes)\n",
    "# #         print(curr_classes.shape)\n",
    "#         [class_index.append(i) for i in range(len(curr_classes)) if curr_classes[i]==1]\n",
    "#         heatmap=np.zeros(36) # might need to fix this if GradCAM or primitive model changes\n",
    "#         for label in class_index:  # multiclass일 경우 대비해서 for문\n",
    "\n",
    "#             #긍정 클래스를 설명할 수 있게 컨볼루션 필터 가중치의 gradient를 구함  \n",
    "#             grads = K.gradients(model.output[:,label], layers_wt)[0]\n",
    "\n",
    "#             #필터별로 가중치를 구함 \n",
    "#             pooled_grads = K.mean(grads, axis=(0,1))\n",
    "#             get_pooled_grads = K.function(model.input, \n",
    "#                                  [pooled_grads, layers.output[0]])\n",
    "\n",
    "#             pooled_grads_value, conv_layer_output_value = get_pooled_grads(curr_x)\n",
    "# #             print(conv_layer_output_value.shape) # (36,128)\n",
    "#             for i in range(conv_layer_output_value.shape[-1]):\n",
    "#                 conv_layer_output_value[:, i] *= pooled_grads_value[i]\n",
    "#                 for j in range(len(conv_layer_output_value[:,i])):\n",
    "#                     conv_layer_output_value[j,i] = max(0, conv_layer_output_value[j,i])\n",
    "#                 #YJS manually added RELU function on 2020-06-02\n",
    "# #                 conv_layer_output_value[:,i] = max(0, conv_layer_output_value[:,i])\n",
    "#             heatmap += np.mean(conv_layer_output_value, axis=-1)\n",
    "# #         print(class_index)\n",
    "#         heatmap %= len(class_index) # 단일 class일 경우 1로 나눠짐. 두개일 경우 더해진 heatmap들이 2로 나눠짐\n",
    "#         heatmap = cv2.resize(heatmap, (1,out_len))\n",
    "#         heatmaps.append(heatmap)\n",
    "# #     heatmaps = heatmaps.reshape(len(x),1,36)\n",
    "#     return heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, 12)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, None, 64)     2368        input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, 64)     256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 64)     12352       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 64)     256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, None, 64)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 128)    24704       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, 128)    512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 128)    49280       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, 128)    512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, None, 128)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 256)    98560       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, 256)    1024        conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 256)    196864      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, 256)    1024        conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 256)    196864      batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, 256)    1024        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, None, 256)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 512)    393728      max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, 512)    2048        conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, None, 512)    786944      batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, 512)    2048        conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, None, 512)    786944      batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, 512)    2048        conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, None, 512)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, None, 512)    786944      max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, 512)    2048        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, None, 256)    393472      batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, 256)    1024        conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, None, 128)    98432       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, 128)    512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, None, 128)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, None, 64)     8192        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, 64)     256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, None, 64)     12288       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, 64)     256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 64)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, None, 256)    16384       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, None, 256)    32768       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, None, 256)    1024        conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, 256)    1024        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, None, 256)    0           batch_normalization_16[0][0]     \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 256)    0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_bn_1 (BatchNor (None, None, 256)    1024        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_conv_1 (Conv1D (None, None, 9)      2304        attention_branch_bn_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_conv_1 (Co (None, None, 1)      9           attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_bn_1 (Batc (None, None, 1)      4           attention_branch_att_conv_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_sigmoid_1  (None, None, 1)      0           attention_branch_att_bn_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "heatmap_image (InputLayer)      [(None, None, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, None, 1)      0           attention_branch_att_sigmoid_1[0]\n",
      "                                                                 heatmap_image[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 128)    0           max_pooling1d_4[0][0]            \n",
      "                                                                 lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, None, 64)     8192        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, None, 64)     256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, None, 64)     12288       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, None, 64)     256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, 64)     0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, None, 256)    16384       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, None, 256)    32768       lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, None, 256)    1024        conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, None, 256)    1024        conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, 256)    0           batch_normalization_20[0][0]     \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, 256)    0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_avgpool_1 (Gl (None, 256)          0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_pred_conv_1 (C (None, None, 9)      81          attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_1 (Dens (None, 256)          65792       perception_branch_avgpool_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_gap_1 (GlobalA (None, 9)            0           attention_branch_pred_conv_1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_2 (Dens (None, 9)            2313        perception_branch_dense_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_output (Activa (None, 9)            0           attention_branch_gap_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_output (Activ (None, 9)            0           perception_branch_dense_2[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 4,057,703\n",
      "Trainable params: 4,047,461\n",
      "Non-trainable params: 10,242\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "from ABNmodules_multiclass import *\n",
    "\n",
    "# def get_custom_model(input_shape, n_classes, minimum_len, target_classes, out_ch=256, n=18):\n",
    "# model = get_custom_model((None, 12), 9, minimum_len, 1, out_ch=256, n=1)\n",
    "# model = get_model((None, 12), 9, n=7)\n",
    "\n",
    "# model = cam_model((None, 12), 9, minimum_len, out_ch=256, n=18)\n",
    "\n",
    "\n",
    "model = edit_model((None,12), len(unique_classes), minimum_len)\n",
    "# model = get_custom_model((None, 12), 9, minimum_len, 1, out_ch=256, n=1)\n",
    "model.summary()\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=optimizers.Adam(lr=1e-5),           \n",
    "              metrics=[score_f1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 중단된 training 이어돌리기위해 임시로 사용\n",
    "# results_directory = results_directory.replace(\"0608\", \"0604\")\n",
    "# results_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 중단된 training 이어돌리기 위해 임시로 사용\n",
    "# latest = tf.train.latest_checkpoint(results_directory)\n",
    "# latest\n",
    "# model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DISP DATETIME FOR CHECKING TIME\n",
    "start = time.time()\n",
    "val_acc_sum=[]\n",
    "train_loss_sum=[]\n",
    "train_acc_sum=[]\n",
    "val_loss_sum=[]\n",
    "val_acc_min = 0\n",
    "print(results_directory)\n",
    "\n",
    "for num_epoch in range(epochs):\n",
    "#     num_epoch += 32 # 중단된 코드 돌리기 위해 임의로 사용\n",
    "    random.shuffle(data_train)\n",
    "    train_loss, train_f1 = train_edit(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "    print('\\nEpoch',num_epoch+1,'train_loss:',f'{train_loss:.3f}','train_f1:',f'{train_f1:.3f}',\"\\t\")\n",
    "    model_output = \"ECG_ABN_E%02dL%.2f\" % (num_epoch, train_loss)\n",
    "    save_name = os.path.join(results_directory, model_output)\n",
    "    \n",
    "    val_acc = test_edit(data_val, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)\n",
    "\n",
    "    if val_acc > val_acc_min:\n",
    "        val_acc_min = val_acc\n",
    "        model.save_weights(save_name.format(epoch=0))\n",
    "#         model.save(save_name)\n",
    "#         cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_name,\n",
    "#                                                          save_weights_only=True,\n",
    "#                                                          verbose=1)\n",
    "    print('\\nValidation', num_epoch+1, 'valid_acc:',f'{val_acc:.3f}', 'best_acc:',f'{val_acc_min:.3f}', \"\\t\")\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_mels =np.asarray(batch_mels)\n",
    "# batch_mels = batch_mels.reshape(32,2880,12)\n",
    "# heatmap = np.asarray(heatmap)\n",
    "# heatmap = heatmap.reshape(batch_size, 1, out_len)\n",
    "# a = model2.predict([batch_mels, heatmap])\n",
    "# a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "617.813px",
    "left": "1544.27px",
    "right": "20px",
    "top": "79.75px",
    "width": "296.719px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
