{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numba/errors.py:137: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import keras\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from ABNmodules_multiclass import *\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "tf.set_random_seed(1004)\n",
    "random.seed(1004)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5225,) (276,) (1376,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def score_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "def get_unique_classes(input_directory,files):\n",
    "\n",
    "    unique_classes=set()\n",
    "    for f in files:\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    for c in tmp:\n",
    "                        unique_classes.add(c.strip())\n",
    "    return sorted(unique_classes)\n",
    "\n",
    "def one_hot_encoding(one_hot_vector,y, class2index):\n",
    "    ind=class2index[y]\n",
    "    one_hot_vector[ind]=1\n",
    "    return one_hot_vector\n",
    "\n",
    "# Search for multi-label subjects\n",
    "def searching_overlap(input_directory,class2index, input_file_names):\n",
    "    multiclasses=[]\n",
    "    multisubjects=[]\n",
    "    number = []\n",
    "    for file in input_file_names:\n",
    "        f=file\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    if len(tmp)>1:\n",
    "                        one_hot_vector = [0]*(len(class2index))\n",
    "                        for c in tmp:\n",
    "                            one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                        multiclasses.append(one_hot_vector)\n",
    "                        multisubjects.append(g)\n",
    "                        number.append(len(tmp))\n",
    "    return multisubjects, multiclasses, number\n",
    "\n",
    "def block_feature(sequence_en, minimum_len): \n",
    "    new_en = []\n",
    "    if len(sequence_en) > minimum_len:  # 길이가 minimum보다 긴 경우\n",
    "        start = random.randint(0,len(sequence_en)-minimum_len)\n",
    "        #print(start)\n",
    "        new_en = sequence_en[start:start+minimum_len]\n",
    "    elif len(sequence_en) == minimum_len: # 길이가 minimum\n",
    "        new_en = sequence_en\n",
    "    else: \n",
    "        assert len(sequence_en) <= minimum_len\n",
    "    return new_en\n",
    "\n",
    "def exploratory_look(input_directory,file, class2index):\n",
    "    classes = []\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                print(tmp, len(tmp))\n",
    "    return tmp     \n",
    "\n",
    "# Get classes of sorted file names\n",
    "def get_labels(input_directory,file, class2index):\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                one_hot_vector = [0]*(len(class2index))\n",
    "                for c in tmp:\n",
    "                    one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                \n",
    "    return one_hot_vector\n",
    "\n",
    "def randextract_mels(curr_step, batch_size, data, mel_directory, class2index, minimum_len, x_mean_final, x_std_final):\n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    start = batch_size*curr_step\n",
    "    end = batch_size*(curr_step+1)\n",
    "    curr_file_indices = data[start:end]\n",
    "    for file in curr_file_indices:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        clip_file = block_feature(tmp_file, minimum_len)\n",
    "        clip_file -= x_mean_final\n",
    "        clip_file /= x_std_final\n",
    "        mel_files.append(clip_file)\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        classes.append(label)\n",
    "    \n",
    "    concat = list(zip(mel_files, classes))\n",
    "    random.shuffle(concat)\n",
    "    mel_files, classes = zip(*concat)\n",
    "    return mel_files, classes\n",
    "\n",
    "def zeropadding_mels(curr_step, batch_size, data, mel_directory, class2index, minimum_len, x_mean_final, x_std_final):\n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    start = batch_size*curr_step\n",
    "    end = batch_size*(curr_step+1)\n",
    "    curr_file_indices = data[start:end]    \n",
    "    for file in curr_file_indices:\n",
    "        \n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        tmp_file -= x_mean_final\n",
    "        tmp_file /= x_std_final        \n",
    "#         print(tmp_file.shape)\n",
    "        zero_padding = np.zeros((minimum_len-len(tmp_file), 12))\n",
    "#         print(zero_padding.shape)\n",
    "        clip_file = np.concatenate((zero_padding, tmp_file), axis=0)\n",
    "#         print(clip_file)\n",
    "#         clip_file = zero_padding + tmp_file\n",
    "#         print(clip_file.shape)\n",
    "#         clip_file = block_feature(tmp_file, minimum_len)\n",
    "        #print(clip_file.shape)\n",
    "        #clip_file = tmp_file[:minimum_len]\n",
    "\n",
    "        mel_files.append(clip_file)\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        classes.append(label)\n",
    "    concat = list(zip(mel_files, classes))\n",
    "    random.shuffle(concat)\n",
    "    mel_files, classes = zip(*concat)\n",
    "    return mel_files, classes  \n",
    "\n",
    "def train(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final, model): \n",
    "    loss=[]\n",
    "#     acc = []\n",
    "    f1 = []\n",
    "\n",
    "    total_steps = int(np.ceil(len(data_train)/batch_size))\n",
    "    for curr_step in range(total_steps): # loops over batches\n",
    "#         batch_mels, batch_labels = randextract_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "        batch_mels, batch_labels = zeropadding_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "        batch_mels = np.asarray(batch_mels)\n",
    "        batch_labels = np.asarray(np.squeeze(batch_labels))\n",
    "        \n",
    "        batch_labels = [batch_labels, batch_labels]\n",
    "\n",
    "\n",
    "# return of train\n",
    "# 0 = total loss (attention branch + perception branch)\n",
    "# 1 = loss of attention pred\n",
    "# 2 = loss of perception pred\n",
    "# 3 = f1 of attention pred\n",
    "# 4 = f1 of perception pred     \n",
    "\n",
    "\n",
    "        train_tmp = model.train_on_batch(batch_mels, batch_labels)\n",
    "        loss_ = train_tmp[0]/2\n",
    "        f1_ = np.mean((train_tmp[3], train_tmp[4]))\n",
    "        loss.append(loss_)\n",
    "        f1.append(f1_)\n",
    "\n",
    "    loss = np.mean(np.array(loss))\n",
    "    f1 = np.mean(np.array(f1))\n",
    "    return loss, f1\n",
    "\n",
    "def train_edit(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final, model): \n",
    "    loss=[]\n",
    "#     acc = []\n",
    "    f1 = []\n",
    "\n",
    "    total_steps = int(np.ceil(len(data_train)/batch_size))\n",
    "    for curr_step in range(total_steps): # loops over batches\n",
    "        batch_mels, batch_labels = randextract_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "\n",
    "        batch_mels = np.asarray(batch_mels)\n",
    "        batch_labels = np.asarray(np.squeeze(batch_labels))\n",
    "\n",
    "\n",
    "        heatmap = CAM_conv1D(minimum_len, n_channels, batch_mels, batch_labels, out_len, get_conv_out)        \n",
    "        heatmap = np.asarray(heatmap)\n",
    "        \n",
    "        batch_labels = [batch_labels, batch_labels]\n",
    "\n",
    "\n",
    "# return of train\n",
    "# 0 = total loss (attention branch + perception branch)\n",
    "# 1 = loss of attention pred\n",
    "# 2 = loss of perception pred\n",
    "# 3 = f1 of attention pred\n",
    "# 4 = f1 of perception pred     \n",
    "\n",
    "        train_tmp = model.train_on_batch([batch_mels, heatmap], batch_labels)\n",
    "        loss_ = train_tmp[0]/2\n",
    "        f1_ = np.mean((train_tmp[3], train_tmp[4]))\n",
    "        loss.append(loss_)\n",
    "        f1.append(f1_)\n",
    "\n",
    "    loss = np.mean(np.array(loss))\n",
    "    f1 = np.mean(np.array(f1))\n",
    "    return loss, f1\n",
    "\n",
    "def test(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred=[]\n",
    "    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    \n",
    "    multi_files, _, _ = searching_overlap(input_directory,class2index, data)\n",
    "    multi_acc = 0\n",
    "    multi_y_true=[]\n",
    "    multi_y_pred=[]    \n",
    "    single_acc = 0\n",
    "    single_y_true=[]\n",
    "    single_y_pred=[]     \n",
    "    \n",
    "    for file in data:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "        heatmap_files=[]\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        label = [label]\n",
    "        for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "            clip_file = clip_file.reshape(1,minimum_len,n_channels)\n",
    "            mel_files.append(clip_file)    \n",
    "        mel_files = np.asarray(mel_files)\n",
    "        mel_files = mel_files.reshape(steps,minimum_len,n_channels)\n",
    "        logit = model.predict(mel_files)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        pred = np.zeros(len(logit))\n",
    "        for ii, label in enumerate(logit):\n",
    "            if label >= 0.5:\n",
    "                pred[ii] = 1\n",
    "            else:\n",
    "                pred[ii] = 0\n",
    "        pred = pred.tolist()\n",
    "        y_pred.append(pred)\n",
    "        label = get_labels(input_directory,file,class2index)\n",
    "        y_true.append(label)\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        if file.replace('.mat', '.hea') in multi_files:\n",
    "            multi_y_true.append(label)            \n",
    "            multi_y_pred.append(pred)\n",
    "            multi_acc += acc\n",
    "        else: # for calculating single_label accuracy\n",
    "            single_y_true.append(label)            \n",
    "            single_y_pred.append(pred)\n",
    "            single_acc += acc            \n",
    "        \n",
    "    final_acc = total_acc / len(data)\n",
    "    f1_classes = f1_score(y_true, y_pred, average=None)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    \n",
    "    multi_final_acc = multi_acc / len(multi_files)\n",
    "    multi_f1_classes = f1_score(multi_y_true, multi_y_pred, average=None)\n",
    "    multi_f1_micro = f1_score(multi_y_true, multi_y_pred, average='micro')\n",
    "\n",
    "    single_final_acc = single_acc / (len (data) - len(multi_files))\n",
    "    single_f1_classes = f1_score(single_y_true, single_y_pred, average=None)\n",
    "    single_f1_micro = f1_score(single_y_true, single_y_pred, average='micro')    \n",
    "    \n",
    "    return final_acc, f1_classes, f1_micro, multi_final_acc, multi_f1_classes, multi_f1_micro, single_final_acc, single_f1_classes, single_f1_micro\n",
    "\n",
    "def test_zero(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    y_true = []\n",
    "    y_pred=[]    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    \n",
    "    \n",
    "    multi_files, _, _ = searching_overlap(input_directory,class2index, data)\n",
    "    multi_acc = 0\n",
    "    multi_y_true=[]\n",
    "    multi_y_pred=[]    \n",
    "    single_acc = 0\n",
    "    single_y_true=[]\n",
    "    single_y_pred=[]  \n",
    "    for file in data:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        tmp_file -= x_mean_final\n",
    "        tmp_file /= x_std_final\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "#         for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            \n",
    "        zero_padding = np.zeros((minimum_len-len(tmp_file), 12))\n",
    "        clip_file = np.concatenate((zero_padding, tmp_file), axis=0)\n",
    "#         print(clip_file.shape)\n",
    "        \n",
    "\n",
    "        mel_files.append(clip_file)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "        logit = model.predict(mel_files)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        logit = np.mean(logit, axis=0)        \n",
    "        \n",
    "        \n",
    "        pred = np.zeros(len(logit))\n",
    "        for ii, label in enumerate(logit):\n",
    "            if label >= 0.5:\n",
    "                pred[ii] = 1\n",
    "            else:\n",
    "                pred[ii] = 0\n",
    "        pred = pred.tolist()\n",
    "        y_pred.append(pred)        \n",
    "        label = get_labels(input_directory,file,class2index)\n",
    "        y_true.append(label)\n",
    "        \n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        if file.replace('.mat', '.hea') in multi_files:\n",
    "            multi_y_true.append(label)            \n",
    "            multi_y_pred.append(pred)\n",
    "            multi_acc += acc         \n",
    "        else: # for calculating single_label accuracy\n",
    "            single_y_true.append(label)            \n",
    "            single_y_pred.append(pred)\n",
    "            single_acc += acc         \n",
    "        #total_f1 += f1\n",
    "    final_acc = total_acc / len(data)\n",
    "    f1_classes = f1_score(y_true, y_pred, average=None)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    \n",
    "    multi_final_acc = multi_acc / len(multi_files)\n",
    "    multi_f1_classes = f1_score(multi_y_true, multi_y_pred, average=None)\n",
    "    multi_f1_micro = f1_score(multi_y_true, multi_y_pred, average='micro')\n",
    "    \n",
    "    single_final_acc = single_acc / (len (data) - len(multi_files))\n",
    "    single_f1_classes = f1_score(single_y_true, single_y_pred, average=None)\n",
    "    single_f1_micro = f1_score(single_y_true, single_y_pred, average='micro')    \n",
    "    \n",
    "    return final_acc, f1_classes, f1_micro, multi_final_acc, multi_f1_classes, multi_f1_micro, single_final_acc, single_f1_classes, single_f1_micro\n",
    "\n",
    "\n",
    "def test_edit(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    \n",
    "    multi_files, _, _ = searching_overlap(input_directory,class2index, data)\n",
    "    multi_acc = 0\n",
    "    multi_y_true=[]\n",
    "    multi_y_pred=[]    \n",
    "    \n",
    "    for file in data:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "        heatmap_files=[]\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        label = [label]\n",
    "        for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "            clip_file = clip_file.reshape(1,minimum_len,n_channels)\n",
    "            heatmap = CAM_conv1D(minimum_len, n_channels, clip_file, label, out_len, get_conv_out)\n",
    "            mel_files.append(clip_file)    \n",
    "            heatmap_files.append(heatmap)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "        heatmap_files = np.asarray(heatmap_files)\n",
    "        heatmap_files = heatmap_files.reshape(steps, out_len,1)\n",
    "        mel_files = mel_files.reshape(steps,minimum_len,n_channels)\n",
    "        logit = model.predict([mel_files, heatmap_files])\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        pred = np.zeros(len(logit))\n",
    "        for ii, label in enumerate(logit):\n",
    "            if label >= 0.5:\n",
    "                pred[ii] = 1\n",
    "            else:\n",
    "                pred[ii] = 0\n",
    "        pred = pred.tolist()\n",
    "        y_pred.append(pred)\n",
    "        label = get_labels(input_directory,file,class2index)\n",
    "        y_true.append(label)\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        if file.replace('.mat', '.hea') in multi_files:\n",
    "            multi_y_true.append(label)\n",
    "            multi_y_pred.append(pred)\n",
    "            multi_acc += acc               \n",
    "    final_acc = total_acc / len(data)\n",
    "    f1_classes = f1_score(y_true, y_pred, average=None)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    \n",
    "    multi_final_acc = multi_acc / len(multi_files)\n",
    "    multi_f1_classes = f1_score(multi_y_true, multi_y_pred, average=None)\n",
    "    multi_f1_micro = f1_score(multi_y_true, multi_y_pred, average='micro')\n",
    "    \n",
    "    return final_acc, f1_classes, f1_micro, multi_final_acc, multi_f1_classes, multi_f1_micro\n",
    "\n",
    "def test_edit_final(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final, p_model):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred=[]\n",
    "    \n",
    "    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    \n",
    "    multi_files, _, _ = searching_overlap(input_directory,class2index, data)\n",
    "    multi_acc = 0\n",
    "    multi_y_true=[]\n",
    "    multi_y_pred=[]\n",
    "    \n",
    "    for file in data:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "        mel_files_logit = []\n",
    "        heatmap_files=[]\n",
    "\n",
    "        for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "            clip_file = clip_file.reshape(1,minimum_len,n_channels)\n",
    "            \n",
    "            # 여기서 logit으로 y구함#####################\n",
    "            logit_ = p_model.predict(clip_file)\n",
    "            logit_ = np.mean(logit_, axis=0)\n",
    "            logit_ = np.mean(logit_, axis=0)\n",
    "            pred_ = np.zeros(len(logit_))\n",
    "            for ii, label in enumerate(logit_):\n",
    "                if label >= 0.5: \n",
    "                    pred_[ii] = 1\n",
    "                else:\n",
    "                    pred_[ii] = 0\n",
    "            pred_ = pred_.tolist()\n",
    "            ##########################################\n",
    "            \n",
    "            heatmap = CAM_conv1D(minimum_len, n_channels, clip_file, [pred_], out_len, get_conv_out)\n",
    "            mel_files.append(clip_file)    \n",
    "            heatmap_files.append(heatmap)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "        heatmap_files = np.asarray(heatmap_files)\n",
    "        heatmap_files = heatmap_files.reshape(steps, out_len,1)\n",
    "        mel_files = mel_files.reshape(steps,minimum_len,n_channels)\n",
    "        logit = model.predict([mel_files, heatmap_files])\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        pred = np.zeros(len(logit))\n",
    "        for ii, label in enumerate(logit):\n",
    "            if label >= 0.5:\n",
    "                pred[ii] = 1\n",
    "            else:\n",
    "                pred[ii] = 0\n",
    "        pred = pred.tolist()\n",
    "        y_pred.append(pred)\n",
    "        label = get_labels(input_directory,file,class2index)\n",
    "        y_true.append(label)\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        if file.replace('.mat', '.hea') in multi_files:\n",
    "            multi_y_true.append(label)          \n",
    "            multi_y_pred.append(pred)\n",
    "            multi_acc += acc       \n",
    "\n",
    "    final_acc = total_acc / len(data)\n",
    "    f1_classes = f1_score(y_true, y_pred, average=None)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    \n",
    "    multi_final_acc = multi_acc / len(multi_files)\n",
    "    multi_f1_classes = f1_score(multi_y_true, multi_y_pred, average=None)\n",
    "    multi_f1_micro = f1_score(multi_y_true, multi_y_pred, average='micro')\n",
    "    \n",
    "    return final_acc, f1_classes, f1_micro, multi_final_acc, multi_f1_classes, multi_f1_micro\n",
    "\n",
    "def test_edit_final2(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final, p_model):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred=[]    \n",
    "\n",
    "    multi_files, _, _ = searching_overlap(input_directory,class2index, data)\n",
    "    multi_acc = 0\n",
    "    multi_y_true=[]\n",
    "    multi_y_pred=[]    \n",
    "    \n",
    "    for file in data:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "        mel_files_logit = []\n",
    "        heatmap_files=[]\n",
    "\n",
    "        for block in range(steps):\n",
    "            # 여기서 logit으로 y구함\n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "            clip_file = clip_file.reshape(1,minimum_len,n_channels)\n",
    "            mel_files_logit.append(clip_file)\n",
    "            \n",
    "        mel_files_logit = np.asarray(mel_files_logit)\n",
    "        mel_files_logit = mel_files_logit.reshape(steps,minimum_len,n_channels)\n",
    "        \n",
    "        \n",
    "        \n",
    "        logit_ = p_model.predict(mel_files_logit)\n",
    "        \n",
    "        logit_ = np.mean(logit_, axis=0)\n",
    "        logit_ = np.mean(logit_, axis=0)\n",
    "#         print(logit_)\n",
    "        pred_ = np.zeros(len(logit_))\n",
    "        for ii, label in enumerate(logit_):\n",
    "            if label >= 0.5: \n",
    "                pred_[ii] = 1\n",
    "            else:\n",
    "                pred_[ii] = 0\n",
    "        \n",
    "        pred_ = pred_.tolist()\n",
    "\n",
    "#         print(pred_)\n",
    "            #######\n",
    "        \n",
    "        \n",
    "        for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "            clip_file = clip_file.reshape(1,minimum_len,n_channels)\n",
    "\n",
    "            heatmap = CAM_conv1D(minimum_len, n_channels, clip_file, [pred_], out_len, get_conv_out)\n",
    "#             print(heatmap)\n",
    "            mel_files.append(clip_file)    \n",
    "            heatmap_files.append(heatmap)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "        heatmap_files = np.asarray(heatmap_files)\n",
    "        heatmap_files = heatmap_files.reshape(steps, out_len,1)\n",
    "        mel_files = mel_files.reshape(steps,minimum_len,n_channels)\n",
    "        logit = model.predict([mel_files, heatmap_files])\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        pred = np.zeros(len(logit))\n",
    "       \n",
    "\n",
    "        for ii, label in enumerate(logit):\n",
    "            if label >= 0.5:\n",
    "                pred[ii] = 1\n",
    "            else:\n",
    "                pred[ii] = 0\n",
    "        pred = pred.tolist()\n",
    "        y_pred.append(pred)\n",
    "#         print(pred)\n",
    "        label = get_labels(input_directory,file,class2index)\n",
    "        y_true.append(label)\n",
    "#         print(label)\n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        if file.replace('.mat', '.hea') in multi_files:\n",
    "            multi_y_true.append(label)\n",
    "            multi_y_pred.append(pred)\n",
    "            multi_acc += acc       \n",
    "\n",
    "    final_acc = total_acc / len(data)\n",
    "    f1_classes = f1_score(y_true, y_pred, average=None)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    \n",
    "    multi_final_acc = multi_acc / len(multi_files)\n",
    "    multi_f1_classes = f1_score(multi_y_true, multi_y_pred, average=None)\n",
    "    multi_f1_micro = f1_score(multi_y_true, multi_y_pred, average='micro')\n",
    "    \n",
    "    return final_acc, f1_classes, f1_micro, multi_final_acc, multi_f1_classes, multi_f1_micro\n",
    "\n",
    "batch_size = 32\n",
    "minimum_len = 72000 #2880\n",
    "epochs = 1000#300#1000\n",
    "n_channels=12\n",
    "loss_function = 'binary_crossentropy' \n",
    "rootdir = '../'\n",
    "date = dt.datetime.today().strftime(\"%Y%m%d\")\n",
    "input_directory = os.path.join(rootdir, 'Training_WFDB')\n",
    "mel_name = 'Raw_data_20200424' \n",
    "mel_directory = os.path.join(rootdir, mel_name)\n",
    "# results_directory = os.path.join(rootdir, 'results_'+date+'_0_IEEE_n=1')\n",
    "# results_directory = os.path.join(rootdir, 'results_'+date+'_editting_CAM_multiclass')\n",
    "\n",
    "if not os.path.isdir(input_directory):\n",
    "    os.mkdir(input_directory)\n",
    "if not os.path.isdir(mel_directory):\n",
    "    os.mkdir(mel_directory)\n",
    "\n",
    "        \n",
    "input_files = []\n",
    "for f in os.listdir(input_directory):\n",
    "    if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('mat'):\n",
    "        input_files.append(f)\n",
    "input_file_names = sorted(input_files)\n",
    "\n",
    "unique_classes = get_unique_classes(input_directory, input_files)\n",
    "class2index = {}\n",
    "for a, b in enumerate(unique_classes):\n",
    "    class2index[b] = a\n",
    "\n",
    "np.shape(input_file_names)\n",
    "\n",
    "x_mean_all = []\n",
    "x_std_all = []\n",
    "for file in input_file_names:\n",
    "    x = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "    x_mean = [np.mean(x[:,0]), np.mean(x[:,1]), np.mean(x[:,2]), np.mean(x[:,3]), np.mean(x[:,4]), np.mean(x[:,5]),\n",
    "             np.mean(x[:,6]), np.mean(x[:,7]), np.mean(x[:,8]), np.mean(x[:,9]), np.mean(x[:,10]), np.mean(x[:,11])]\n",
    "    \n",
    "    x_std = [np.std(x[:,0]), np.std(x[:,1]), np.std(x[:,2]), np.std(x[:,3]), np.std(x[:,4]), np.std(x[:,5]),\n",
    "             np.std(x[:,6]), np.std(x[:,7]), np.std(x[:,8]), np.std(x[:,9]), np.std(x[:,10]), np.std(x[:,11])]\n",
    "    x_mean_all.append(x_mean)\n",
    "    x_std_all.append(x_std) # yjs corrected on 2020-05-25\n",
    "x_mean_final = np.mean(x_mean_all, axis=0)\n",
    "x_std_final = np.mean(x_std_all, axis=0)\n",
    "\n",
    "\n",
    "data, data_test = train_test_split(input_file_names, test_size = 0.2, train_size = 0.8, shuffle=True, random_state=1004)\n",
    "data_train, data_val = train_test_split(data, test_size = 0.05, train_size = 0.95, shuffle=True, random_state=1004)\n",
    "print(np.shape(data_train), np.shape(data_val), np.shape(data_test))\n",
    "\n",
    "main_input = Input(shape=(minimum_len,12), dtype='float32', name='main_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A2355.mat',\n",
       " 'A3446.mat',\n",
       " 'A1814.mat',\n",
       " 'A3575.mat',\n",
       " 'A6197.mat',\n",
       " 'A6727.mat',\n",
       " 'A3974.mat',\n",
       " 'A3380.mat',\n",
       " 'A1978.mat',\n",
       " 'A2265.mat',\n",
       " 'A3514.mat',\n",
       " 'A4147.mat',\n",
       " 'A3852.mat',\n",
       " 'A0245.mat',\n",
       " 'A1846.mat',\n",
       " 'A5979.mat',\n",
       " 'A1696.mat',\n",
       " 'A3454.mat',\n",
       " 'A5164.mat',\n",
       " 'A6827.mat',\n",
       " 'A6778.mat',\n",
       " 'A5403.mat',\n",
       " 'A5567.mat',\n",
       " 'A5923.mat',\n",
       " 'A0254.mat',\n",
       " 'A4950.mat',\n",
       " 'A3926.mat',\n",
       " 'A4083.mat',\n",
       " 'A0223.mat',\n",
       " 'A6569.mat',\n",
       " 'A5393.mat',\n",
       " 'A6343.mat',\n",
       " 'A0300.mat',\n",
       " 'A3324.mat',\n",
       " 'A5812.mat',\n",
       " 'A6865.mat',\n",
       " 'A6600.mat',\n",
       " 'A6153.mat',\n",
       " 'A5947.mat',\n",
       " 'A0999.mat',\n",
       " 'A0613.mat',\n",
       " 'A5423.mat',\n",
       " 'A5316.mat',\n",
       " 'A2158.mat',\n",
       " 'A3799.mat',\n",
       " 'A0598.mat',\n",
       " 'A2677.mat',\n",
       " 'A5973.mat',\n",
       " 'A2911.mat',\n",
       " 'A5053.mat',\n",
       " 'A1400.mat',\n",
       " 'A1283.mat',\n",
       " 'A3298.mat',\n",
       " 'A2962.mat',\n",
       " 'A3459.mat',\n",
       " 'A5462.mat',\n",
       " 'A5589.mat',\n",
       " 'A1946.mat',\n",
       " 'A3583.mat',\n",
       " 'A3842.mat',\n",
       " 'A5419.mat',\n",
       " 'A1267.mat',\n",
       " 'A2669.mat',\n",
       " 'A6353.mat',\n",
       " 'A4129.mat',\n",
       " 'A1017.mat',\n",
       " 'A6503.mat',\n",
       " 'A2249.mat',\n",
       " 'A0345.mat',\n",
       " 'A0445.mat',\n",
       " 'A6595.mat',\n",
       " 'A3295.mat',\n",
       " 'A2297.mat',\n",
       " 'A2339.mat',\n",
       " 'A4796.mat',\n",
       " 'A2592.mat',\n",
       " 'A5975.mat',\n",
       " 'A6603.mat',\n",
       " 'A4948.mat',\n",
       " 'A2484.mat',\n",
       " 'A0970.mat',\n",
       " 'A1486.mat',\n",
       " 'A6150.mat',\n",
       " 'A4953.mat',\n",
       " 'A0401.mat',\n",
       " 'A1884.mat',\n",
       " 'A0142.mat',\n",
       " 'A4253.mat',\n",
       " 'A0091.mat',\n",
       " 'A5929.mat',\n",
       " 'A2437.mat',\n",
       " 'A2848.mat',\n",
       " 'A6081.mat',\n",
       " 'A3128.mat',\n",
       " 'A3368.mat',\n",
       " 'A5696.mat',\n",
       " 'A3561.mat',\n",
       " 'A2113.mat',\n",
       " 'A6672.mat',\n",
       " 'A0642.mat',\n",
       " 'A0601.mat',\n",
       " 'A2875.mat',\n",
       " 'A5877.mat',\n",
       " 'A0161.mat',\n",
       " 'A2241.mat',\n",
       " 'A2224.mat',\n",
       " 'A0278.mat',\n",
       " 'A5825.mat',\n",
       " 'A5221.mat',\n",
       " 'A5564.mat',\n",
       " 'A3909.mat',\n",
       " 'A0436.mat',\n",
       " 'A1418.mat',\n",
       " 'A2874.mat',\n",
       " 'A2630.mat',\n",
       " 'A3948.mat',\n",
       " 'A2637.mat',\n",
       " 'A0603.mat',\n",
       " 'A6694.mat',\n",
       " 'A6105.mat',\n",
       " 'A0654.mat',\n",
       " 'A3328.mat',\n",
       " 'A6777.mat',\n",
       " 'A4679.mat',\n",
       " 'A2877.mat',\n",
       " 'A1102.mat',\n",
       " 'A0850.mat',\n",
       " 'A1131.mat',\n",
       " 'A4874.mat',\n",
       " 'A1434.mat',\n",
       " 'A1305.mat',\n",
       " 'A5644.mat',\n",
       " 'A1749.mat',\n",
       " 'A0283.mat',\n",
       " 'A4110.mat',\n",
       " 'A6104.mat',\n",
       " 'A5254.mat',\n",
       " 'A0207.mat',\n",
       " 'A4018.mat',\n",
       " 'A3463.mat',\n",
       " 'A6661.mat',\n",
       " 'A1912.mat',\n",
       " 'A3615.mat',\n",
       " 'A2600.mat',\n",
       " 'A4786.mat',\n",
       " 'A4181.mat',\n",
       " 'A0890.mat',\n",
       " 'A4371.mat',\n",
       " 'A2697.mat',\n",
       " 'A1950.mat',\n",
       " 'A6241.mat',\n",
       " 'A2606.mat',\n",
       " 'A2882.mat',\n",
       " 'A4592.mat',\n",
       " 'A4781.mat',\n",
       " 'A6766.mat',\n",
       " 'A4746.mat',\n",
       " 'A3157.mat',\n",
       " 'A2432.mat',\n",
       " 'A5832.mat',\n",
       " 'A0937.mat',\n",
       " 'A6829.mat',\n",
       " 'A5914.mat',\n",
       " 'A2639.mat',\n",
       " 'A0509.mat',\n",
       " 'A0697.mat',\n",
       " 'A0251.mat',\n",
       " 'A1586.mat',\n",
       " 'A2259.mat',\n",
       " 'A4245.mat',\n",
       " 'A4234.mat',\n",
       " 'A1251.mat',\n",
       " 'A5470.mat',\n",
       " 'A3447.mat',\n",
       " 'A4630.mat',\n",
       " 'A4743.mat',\n",
       " 'A1954.mat',\n",
       " 'A4112.mat',\n",
       " 'A4837.mat',\n",
       " 'A2998.mat',\n",
       " 'A6774.mat',\n",
       " 'A3471.mat',\n",
       " 'A0902.mat',\n",
       " 'A5927.mat',\n",
       " 'A5933.mat',\n",
       " 'A5450.mat',\n",
       " 'A1123.mat',\n",
       " 'A4251.mat',\n",
       " 'A6068.mat',\n",
       " 'A3008.mat',\n",
       " 'A6152.mat',\n",
       " 'A5355.mat',\n",
       " 'A2787.mat',\n",
       " 'A6362.mat',\n",
       " 'A2352.mat',\n",
       " 'A0956.mat',\n",
       " 'A3420.mat',\n",
       " 'A1147.mat',\n",
       " 'A5597.mat',\n",
       " 'A3294.mat',\n",
       " 'A0318.mat',\n",
       " 'A6359.mat',\n",
       " 'A5444.mat',\n",
       " 'A0012.mat',\n",
       " 'A1032.mat',\n",
       " 'A2603.mat',\n",
       " 'A0391.mat',\n",
       " 'A4011.mat',\n",
       " 'A0581.mat',\n",
       " 'A0316.mat',\n",
       " 'A6372.mat',\n",
       " 'A3935.mat',\n",
       " 'A1545.mat',\n",
       " 'A2508.mat',\n",
       " 'A1204.mat',\n",
       " 'A2138.mat',\n",
       " 'A4313.mat',\n",
       " 'A1895.mat',\n",
       " 'A3818.mat',\n",
       " 'A6537.mat',\n",
       " 'A2231.mat',\n",
       " 'A2080.mat',\n",
       " 'A3719.mat',\n",
       " 'A3150.mat',\n",
       " 'A0468.mat',\n",
       " 'A2497.mat',\n",
       " 'A1137.mat',\n",
       " 'A6828.mat',\n",
       " 'A5422.mat',\n",
       " 'A0998.mat',\n",
       " 'A4667.mat',\n",
       " 'A6527.mat',\n",
       " 'A5514.mat',\n",
       " 'A3770.mat',\n",
       " 'A3418.mat',\n",
       " 'A3423.mat',\n",
       " 'A6709.mat',\n",
       " 'A3080.mat',\n",
       " 'A2515.mat',\n",
       " 'A2192.mat',\n",
       " 'A0048.mat',\n",
       " 'A3144.mat',\n",
       " 'A1709.mat',\n",
       " 'A6763.mat',\n",
       " 'A6858.mat',\n",
       " 'A1787.mat',\n",
       " 'A4214.mat',\n",
       " 'A0446.mat',\n",
       " 'A2178.mat',\n",
       " 'A5659.mat',\n",
       " 'A2262.mat',\n",
       " 'A0096.mat',\n",
       " 'A0809.mat',\n",
       " 'A2146.mat',\n",
       " 'A2499.mat',\n",
       " 'A6062.mat',\n",
       " 'A1661.mat',\n",
       " 'A1651.mat',\n",
       " 'A5370.mat',\n",
       " 'A2755.mat',\n",
       " 'A3924.mat',\n",
       " 'A6458.mat',\n",
       " 'A3325.mat',\n",
       " 'A5239.mat',\n",
       " 'A0420.mat',\n",
       " 'A0504.mat',\n",
       " 'A6374.mat',\n",
       " 'A2201.mat',\n",
       " 'A2520.mat',\n",
       " 'A2452.mat',\n",
       " 'A5103.mat',\n",
       " 'A4291.mat',\n",
       " 'A2460.mat',\n",
       " 'A5888.mat',\n",
       " 'A2021.mat',\n",
       " 'A1629.mat',\n",
       " 'A3831.mat',\n",
       " 'A1410.mat',\n",
       " 'A4493.mat',\n",
       " 'A6701.mat',\n",
       " 'A6842.mat',\n",
       " 'A1300.mat',\n",
       " 'A5683.mat',\n",
       " 'A1908.mat',\n",
       " 'A5785.mat',\n",
       " 'A6298.mat',\n",
       " 'A0060.mat',\n",
       " 'A6313.mat',\n",
       " 'A0101.mat',\n",
       " 'A6312.mat',\n",
       " 'A3748.mat',\n",
       " 'A2028.mat',\n",
       " 'A6728.mat',\n",
       " 'A2434.mat',\n",
       " 'A6818.mat',\n",
       " 'A2416.mat',\n",
       " 'A0747.mat',\n",
       " 'A3355.mat',\n",
       " 'A0441.mat',\n",
       " 'A5375.mat',\n",
       " 'A1795.mat',\n",
       " 'A2154.mat',\n",
       " 'A1216.mat',\n",
       " 'A5051.mat',\n",
       " 'A3200.mat',\n",
       " 'A3792.mat',\n",
       " 'A0024.mat',\n",
       " 'A0986.mat',\n",
       " 'A6645.mat',\n",
       " 'A3010.mat',\n",
       " 'A3114.mat',\n",
       " 'A2312.mat',\n",
       " 'A6182.mat',\n",
       " 'A0175.mat',\n",
       " 'A3392.mat',\n",
       " 'A3929.mat',\n",
       " 'A6349.mat',\n",
       " 'A2597.mat',\n",
       " 'A0985.mat',\n",
       " 'A2015.mat',\n",
       " 'A6756.mat',\n",
       " 'A3410.mat',\n",
       " 'A3549.mat',\n",
       " 'A4647.mat',\n",
       " 'A6252.mat',\n",
       " 'A2844.mat',\n",
       " 'A4977.mat',\n",
       " 'A4488.mat',\n",
       " 'A2128.mat',\n",
       " 'A4640.mat',\n",
       " 'A3302.mat',\n",
       " 'A4613.mat',\n",
       " 'A6649.mat',\n",
       " 'A1163.mat',\n",
       " 'A0506.mat',\n",
       " 'A1700.mat',\n",
       " 'A1311.mat',\n",
       " 'A0553.mat',\n",
       " 'A4834.mat',\n",
       " 'A5783.mat',\n",
       " 'A6083.mat',\n",
       " 'A2120.mat',\n",
       " 'A1277.mat',\n",
       " 'A2984.mat',\n",
       " 'A0880.mat',\n",
       " 'A6439.mat',\n",
       " 'A6032.mat',\n",
       " 'A3254.mat',\n",
       " 'A2301.mat',\n",
       " 'A2834.mat',\n",
       " 'A4737.mat',\n",
       " 'A1303.mat',\n",
       " 'A0105.mat',\n",
       " 'A6311.mat',\n",
       " 'A3580.mat',\n",
       " 'A0390.mat',\n",
       " 'A6748.mat',\n",
       " 'A0064.mat',\n",
       " 'A4275.mat',\n",
       " 'A1887.mat',\n",
       " 'A0957.mat',\n",
       " 'A0851.mat',\n",
       " 'A4793.mat',\n",
       " 'A6307.mat',\n",
       " 'A1213.mat',\n",
       " 'A4367.mat',\n",
       " 'A1502.mat',\n",
       " 'A4775.mat',\n",
       " 'A0148.mat',\n",
       " 'A6294.mat',\n",
       " 'A0021.mat',\n",
       " 'A3151.mat',\n",
       " 'A0683.mat',\n",
       " 'A2833.mat',\n",
       " 'A5744.mat',\n",
       " 'A5096.mat',\n",
       " 'A5739.mat',\n",
       " 'A4506.mat',\n",
       " 'A5141.mat',\n",
       " 'A6544.mat',\n",
       " 'A4383.mat',\n",
       " 'A1298.mat',\n",
       " 'A4272.mat',\n",
       " 'A5209.mat',\n",
       " 'A2044.mat',\n",
       " 'A3914.mat',\n",
       " 'A5106.mat',\n",
       " 'A6604.mat',\n",
       " 'A6725.mat',\n",
       " 'A3365.mat',\n",
       " 'A3658.mat',\n",
       " 'A2808.mat',\n",
       " 'A3116.mat',\n",
       " 'A1681.mat',\n",
       " 'A0723.mat',\n",
       " 'A0017.mat',\n",
       " 'A5229.mat',\n",
       " 'A4125.mat',\n",
       " 'A4091.mat',\n",
       " 'A0293.mat',\n",
       " 'A0800.mat',\n",
       " 'A4114.mat',\n",
       " 'A2653.mat',\n",
       " 'A3118.mat',\n",
       " 'A0384.mat',\n",
       " 'A5320.mat',\n",
       " 'A3768.mat',\n",
       " 'A4759.mat',\n",
       " 'A4441.mat',\n",
       " 'A3548.mat',\n",
       " 'A2721.mat',\n",
       " 'A6581.mat',\n",
       " 'A3394.mat',\n",
       " 'A4719.mat',\n",
       " 'A4675.mat',\n",
       " 'A2948.mat',\n",
       " 'A6033.mat',\n",
       " 'A2832.mat',\n",
       " 'A4560.mat',\n",
       " 'A2235.mat',\n",
       " 'A4887.mat',\n",
       " 'A2104.mat',\n",
       " 'A0486.mat',\n",
       " 'A3557.mat',\n",
       " 'A0310.mat',\n",
       " 'A0820.mat',\n",
       " 'A3236.mat',\n",
       " 'A4609.mat',\n",
       " 'A0812.mat',\n",
       " 'A5261.mat',\n",
       " 'A3000.mat',\n",
       " 'A1944.mat',\n",
       " 'A1731.mat',\n",
       " 'A0525.mat',\n",
       " 'A6580.mat',\n",
       " 'A6355.mat',\n",
       " 'A4701.mat',\n",
       " 'A0825.mat',\n",
       " 'A0428.mat',\n",
       " 'A6125.mat',\n",
       " 'A2293.mat',\n",
       " 'A5035.mat',\n",
       " 'A0832.mat',\n",
       " 'A1444.mat',\n",
       " 'A3969.mat',\n",
       " 'A3905.mat',\n",
       " 'A6744.mat',\n",
       " 'A4283.mat',\n",
       " 'A1479.mat',\n",
       " 'A6049.mat',\n",
       " 'A3367.mat',\n",
       " 'A6634.mat',\n",
       " 'A3017.mat',\n",
       " 'A0386.mat',\n",
       " 'A5770.mat',\n",
       " 'A0901.mat',\n",
       " 'A2285.mat',\n",
       " 'A3938.mat',\n",
       " 'A4599.mat',\n",
       " 'A1777.mat',\n",
       " 'A3229.mat',\n",
       " 'A2624.mat',\n",
       " 'A6830.mat',\n",
       " 'A0966.mat',\n",
       " 'A3143.mat',\n",
       " 'A2381.mat',\n",
       " 'A6493.mat',\n",
       " 'A5265.mat',\n",
       " 'A1960.mat',\n",
       " 'A1228.mat',\n",
       " 'A0897.mat',\n",
       " 'A6486.mat',\n",
       " 'A4908.mat',\n",
       " 'A4767.mat',\n",
       " 'A2916.mat',\n",
       " 'A2668.mat',\n",
       " 'A4690.mat',\n",
       " 'A4209.mat',\n",
       " 'A6700.mat',\n",
       " 'A1544.mat',\n",
       " 'A4979.mat',\n",
       " 'A2109.mat',\n",
       " 'A4897.mat',\n",
       " 'A1065.mat',\n",
       " 'A5344.mat',\n",
       " 'A0106.mat',\n",
       " 'A2732.mat',\n",
       " 'A6473.mat',\n",
       " 'A0426.mat',\n",
       " 'A4387.mat',\n",
       " 'A0340.mat',\n",
       " 'A3300.mat',\n",
       " 'A5038.mat',\n",
       " 'A2295.mat',\n",
       " 'A5796.mat',\n",
       " 'A4449.mat',\n",
       " 'A3625.mat',\n",
       " 'A3411.mat',\n",
       " 'A1326.mat',\n",
       " 'A5948.mat',\n",
       " 'A0146.mat',\n",
       " 'A2790.mat',\n",
       " 'A0782.mat',\n",
       " 'A3731.mat',\n",
       " 'A3221.mat',\n",
       " 'A0978.mat',\n",
       " 'A2946.mat',\n",
       " 'A5574.mat',\n",
       " 'A4554.mat',\n",
       " 'A4577.mat',\n",
       " 'A2446.mat',\n",
       " 'A0505.mat',\n",
       " 'A2282.mat',\n",
       " 'A2306.mat',\n",
       " 'A1023.mat',\n",
       " 'A4672.mat',\n",
       " 'A4430.mat',\n",
       " 'A4695.mat',\n",
       " 'A0763.mat',\n",
       " 'A4867.mat',\n",
       " 'A4602.mat',\n",
       " 'A3916.mat',\n",
       " 'A3796.mat',\n",
       " 'A1081.mat',\n",
       " 'A5191.mat',\n",
       " 'A4255.mat',\n",
       " 'A1618.mat',\n",
       " 'A5572.mat',\n",
       " 'A6199.mat',\n",
       " 'A2272.mat',\n",
       " 'A3981.mat',\n",
       " 'A0298.mat',\n",
       " 'A3963.mat',\n",
       " 'A5539.mat',\n",
       " 'A1699.mat',\n",
       " 'A4157.mat',\n",
       " 'A4991.mat',\n",
       " 'A4239.mat',\n",
       " 'A1224.mat',\n",
       " 'A4390.mat',\n",
       " 'A5446.mat',\n",
       " 'A2118.mat',\n",
       " 'A5487.mat',\n",
       " 'A2788.mat',\n",
       " 'A3452.mat',\n",
       " 'A2711.mat',\n",
       " 'A2335.mat',\n",
       " 'A3678.mat',\n",
       " 'A3642.mat',\n",
       " 'A1708.mat',\n",
       " 'A0975.mat',\n",
       " 'A2461.mat',\n",
       " 'A1642.mat',\n",
       " 'A3607.mat',\n",
       " 'A0121.mat',\n",
       " 'A3968.mat',\n",
       " 'A4104.mat',\n",
       " 'A4101.mat',\n",
       " 'A5833.mat',\n",
       " 'A3203.mat',\n",
       " 'A4631.mat',\n",
       " 'A4048.mat',\n",
       " 'A0704.mat',\n",
       " 'A3371.mat',\n",
       " 'A6041.mat',\n",
       " 'A6656.mat',\n",
       " 'A0241.mat',\n",
       " 'A2601.mat',\n",
       " 'A2699.mat',\n",
       " 'A3855.mat',\n",
       " 'A4366.mat',\n",
       " 'A5390.mat',\n",
       " 'A1001.mat',\n",
       " 'A1461.mat',\n",
       " 'A0789.mat',\n",
       " 'A1207.mat',\n",
       " 'A1832.mat',\n",
       " 'A1034.mat',\n",
       " 'A6176.mat',\n",
       " 'A1798.mat',\n",
       " 'A6419.mat',\n",
       " 'A6471.mat',\n",
       " 'A0080.mat',\n",
       " 'A2198.mat',\n",
       " 'A1543.mat',\n",
       " 'A6711.mat',\n",
       " 'A1262.mat',\n",
       " 'A3811.mat',\n",
       " 'A5612.mat',\n",
       " 'A3758.mat',\n",
       " 'A3528.mat',\n",
       " 'A2133.mat',\n",
       " 'A1132.mat',\n",
       " 'A1209.mat',\n",
       " 'A2982.mat',\n",
       " 'A5206.mat',\n",
       " 'A5988.mat',\n",
       " 'A0965.mat',\n",
       " 'A5357.mat',\n",
       " 'A0859.mat',\n",
       " 'A2401.mat',\n",
       " 'A4642.mat',\n",
       " 'A5426.mat',\n",
       " 'A6315.mat',\n",
       " 'A6453.mat',\n",
       " 'A1564.mat',\n",
       " 'A3687.mat',\n",
       " 'A5116.mat',\n",
       " 'A1897.mat',\n",
       " 'A5240.mat',\n",
       " 'A2267.mat',\n",
       " 'A5705.mat',\n",
       " 'A2664.mat',\n",
       " 'A3571.mat',\n",
       " 'A3611.mat',\n",
       " 'A0862.mat',\n",
       " 'A4135.mat',\n",
       " 'A6535.mat',\n",
       " 'A5241.mat',\n",
       " 'A1096.mat',\n",
       " 'A3895.mat',\n",
       " 'A1372.mat',\n",
       " 'A5474.mat',\n",
       " 'A6042.mat',\n",
       " 'A0365.mat',\n",
       " 'A5858.mat',\n",
       " 'A1280.mat',\n",
       " 'A4415.mat',\n",
       " 'A2687.mat',\n",
       " 'A1595.mat',\n",
       " 'A0423.mat',\n",
       " 'A5956.mat',\n",
       " 'A4940.mat',\n",
       " 'A6031.mat',\n",
       " 'A4736.mat',\n",
       " 'A6279.mat',\n",
       " 'A4784.mat',\n",
       " 'A1141.mat',\n",
       " 'A4650.mat',\n",
       " 'A2445.mat',\n",
       " 'A2715.mat',\n",
       " 'A6029.mat',\n",
       " 'A5966.mat',\n",
       " 'A6554.mat',\n",
       " 'A1785.mat',\n",
       " 'A3467.mat',\n",
       " 'A5764.mat',\n",
       " 'A4334.mat',\n",
       " 'A3790.mat',\n",
       " 'A2411.mat',\n",
       " 'A6631.mat',\n",
       " 'A1334.mat',\n",
       " 'A4247.mat',\n",
       " 'A6435.mat',\n",
       " 'A1625.mat',\n",
       " 'A5920.mat',\n",
       " 'A5025.mat',\n",
       " 'A0584.mat',\n",
       " 'A1951.mat',\n",
       " 'A1597.mat',\n",
       " 'A6536.mat',\n",
       " 'A0465.mat',\n",
       " 'A3822.mat',\n",
       " 'A6630.mat',\n",
       " 'A5088.mat',\n",
       " 'A1214.mat',\n",
       " 'A5384.mat',\n",
       " 'A5618.mat',\n",
       " 'A0039.mat',\n",
       " 'A2358.mat',\n",
       " 'A5545.mat',\n",
       " 'A3891.mat',\n",
       " 'A1052.mat',\n",
       " 'A4713.mat',\n",
       " 'A3554.mat',\n",
       " 'A3751.mat',\n",
       " 'A3646.mat',\n",
       " 'A0707.mat',\n",
       " 'A2869.mat',\n",
       " 'A2179.mat',\n",
       " 'A3305.mat',\n",
       " 'A3140.mat',\n",
       " 'A3269.mat',\n",
       " 'A0193.mat',\n",
       " 'A1521.mat',\n",
       " 'A0035.mat',\n",
       " 'A2400.mat',\n",
       " 'A5295.mat',\n",
       " 'A3640.mat',\n",
       " 'A3595.mat',\n",
       " 'A3263.mat',\n",
       " 'A3540.mat',\n",
       " 'A5836.mat',\n",
       " 'A1982.mat',\n",
       " 'A4220.mat',\n",
       " 'A3172.mat',\n",
       " 'A6871.mat',\n",
       " 'A1149.mat',\n",
       " 'A5131.mat',\n",
       " 'A6225.mat',\n",
       " 'A0089.mat',\n",
       " 'A1767.mat',\n",
       " 'A3717.mat',\n",
       " 'A2364.mat',\n",
       " 'A3918.mat',\n",
       " 'A6717.mat',\n",
       " 'A1304.mat',\n",
       " 'A5600.mat',\n",
       " 'A0565.mat',\n",
       " 'A3992.mat',\n",
       " 'A3987.mat',\n",
       " 'A6280.mat',\n",
       " 'A3705.mat',\n",
       " 'A3477.mat',\n",
       " 'A6149.mat',\n",
       " 'A3210.mat',\n",
       " 'A3741.mat',\n",
       " 'A4294.mat',\n",
       " 'A0608.mat',\n",
       " 'A0215.mat',\n",
       " 'A6578.mat',\n",
       " 'A5995.mat',\n",
       " 'A0713.mat',\n",
       " 'A1934.mat',\n",
       " 'A2300.mat',\n",
       " 'A4725.mat',\n",
       " 'A6057.mat',\n",
       " 'A5073.mat',\n",
       " 'A4142.mat',\n",
       " 'A1161.mat',\n",
       " 'A0355.mat',\n",
       " 'A2213.mat',\n",
       " 'A6811.mat',\n",
       " 'A5168.mat',\n",
       " 'A2689.mat',\n",
       " 'A0094.mat',\n",
       " 'A5654.mat',\n",
       " 'A1952.mat',\n",
       " 'A0619.mat',\n",
       " 'A6075.mat',\n",
       " 'A0433.mat',\n",
       " 'A2719.mat',\n",
       " 'A1217.mat',\n",
       " 'A0576.mat',\n",
       " 'A1855.mat',\n",
       " 'A2857.mat',\n",
       " 'A5454.mat',\n",
       " 'A1074.mat',\n",
       " 'A2292.mat',\n",
       " 'A3293.mat',\n",
       " 'A0261.mat',\n",
       " 'A3767.mat',\n",
       " 'A3427.mat',\n",
       " 'A4144.mat',\n",
       " 'A1331.mat',\n",
       " 'A6760.mat',\n",
       " 'A5752.mat',\n",
       " 'A0294.mat',\n",
       " 'A1747.mat',\n",
       " 'A6017.mat',\n",
       " 'A6507.mat',\n",
       " 'A3127.mat',\n",
       " 'A1589.mat',\n",
       " 'A4943.mat',\n",
       " 'A2894.mat',\n",
       " 'A4100.mat',\n",
       " 'A0167.mat',\n",
       " 'A1316.mat',\n",
       " 'A4819.mat',\n",
       " 'A1002.mat',\n",
       " 'A1019.mat',\n",
       " 'A5666.mat',\n",
       " 'A2219.mat',\n",
       " 'A1669.mat',\n",
       " 'A0836.mat',\n",
       " 'A2761.mat',\n",
       " 'A2593.mat',\n",
       " 'A2043.mat',\n",
       " 'A6335.mat',\n",
       " 'A3435.mat',\n",
       " 'A3650.mat',\n",
       " 'A1739.mat',\n",
       " 'A4833.mat',\n",
       " 'A2408.mat',\n",
       " 'A3598.mat',\n",
       " 'A6652.mat',\n",
       " 'A4857.mat',\n",
       " 'A5352.mat',\n",
       " 'A3026.mat',\n",
       " 'A4282.mat',\n",
       " 'A3893.mat',\n",
       " 'A5456.mat',\n",
       " 'A0481.mat',\n",
       " 'A4911.mat',\n",
       " 'A2558.mat',\n",
       " 'A1626.mat',\n",
       " 'A3412.mat',\n",
       " 'A4705.mat',\n",
       " 'A5354.mat',\n",
       " 'A1670.mat',\n",
       " 'A4025.mat',\n",
       " 'A6201.mat',\n",
       " 'A3359.mat',\n",
       " 'A5378.mat',\n",
       " 'A1018.mat',\n",
       " 'A2359.mat',\n",
       " 'A4782.mat',\n",
       " 'A1901.mat',\n",
       " 'A2332.mat',\n",
       " 'A4434.mat',\n",
       " 'A1202.mat',\n",
       " 'A4433.mat',\n",
       " 'A0860.mat',\n",
       " 'A0415.mat',\n",
       " 'A0918.mat',\n",
       " 'A6178.mat',\n",
       " 'A3482.mat',\n",
       " 'A4890.mat',\n",
       " 'A0996.mat',\n",
       " 'A4102.mat',\n",
       " 'A5158.mat',\n",
       " 'A2093.mat',\n",
       " 'A4703.mat',\n",
       " 'A2867.mat',\n",
       " 'A1591.mat',\n",
       " 'A5582.mat',\n",
       " 'A3314.mat',\n",
       " 'A4182.mat',\n",
       " 'A1133.mat',\n",
       " 'A5533.mat',\n",
       " 'A2794.mat',\n",
       " 'A4028.mat',\n",
       " 'A5273.mat',\n",
       " 'A4794.mat',\n",
       " 'A2588.mat',\n",
       " 'A3958.mat',\n",
       " 'A5516.mat',\n",
       " 'A2048.mat',\n",
       " 'A2681.mat',\n",
       " 'A2925.mat',\n",
       " 'A6035.mat',\n",
       " 'A1484.mat',\n",
       " 'A1652.mat',\n",
       " 'A2553.mat',\n",
       " 'A2634.mat',\n",
       " 'A0771.mat',\n",
       " 'A1433.mat',\n",
       " 'A1694.mat',\n",
       " 'A1839.mat',\n",
       " 'A5182.mat',\n",
       " 'A2011.mat',\n",
       " 'A0469.mat',\n",
       " 'A1116.mat',\n",
       " 'A0700.mat',\n",
       " 'A3783.mat',\n",
       " 'A5055.mat',\n",
       " 'A2745.mat',\n",
       " 'A6214.mat',\n",
       " 'A5112.mat',\n",
       " 'A4902.mat',\n",
       " 'A6707.mat',\n",
       " 'A3402.mat',\n",
       " 'A5363.mat',\n",
       " 'A1229.mat',\n",
       " 'A0238.mat',\n",
       " 'A3695.mat',\n",
       " 'A2145.mat',\n",
       " 'A4154.mat',\n",
       " 'A6785.mat',\n",
       " 'A1294.mat',\n",
       " 'A0677.mat',\n",
       " 'A3689.mat',\n",
       " 'A2150.mat',\n",
       " 'A3564.mat',\n",
       " 'A0537.mat',\n",
       " 'A0405.mat',\n",
       " 'A4364.mat',\n",
       " 'A4511.mat',\n",
       " 'A6338.mat',\n",
       " 'A4192.mat',\n",
       " 'A1883.mat',\n",
       " 'A6278.mat',\n",
       " 'A0561.mat',\n",
       " 'A4861.mat',\n",
       " 'A0811.mat',\n",
       " 'A2091.mat',\n",
       " 'A2388.mat',\n",
       " 'A1126.mat',\n",
       " 'A5757.mat',\n",
       " 'A5248.mat',\n",
       " 'A3509.mat',\n",
       " 'A0451.mat',\n",
       " 'A1919.mat',\n",
       " 'A0815.mat',\n",
       " 'A4836.mat',\n",
       " 'A1054.mat',\n",
       " 'A1559.mat',\n",
       " 'A3130.mat',\n",
       " 'A4208.mat',\n",
       " 'A2251.mat',\n",
       " 'A0233.mat',\n",
       " 'A5089.mat',\n",
       " 'A4698.mat',\n",
       " 'A1611.mat',\n",
       " 'A6637.mat',\n",
       " 'A1220.mat',\n",
       " 'A4831.mat',\n",
       " 'A0794.mat',\n",
       " 'A5268.mat',\n",
       " 'A5367.mat',\n",
       " 'A3071.mat',\n",
       " 'A0003.mat',\n",
       " 'A3348.mat',\n",
       " 'A2622.mat',\n",
       " 'A0801.mat',\n",
       " 'A4976.mat',\n",
       " 'A1605.mat',\n",
       " 'A5417.mat',\n",
       " 'A1440.mat',\n",
       " 'A3618.mat',\n",
       " 'A5536.mat',\n",
       " 'A4183.mat',\n",
       " 'A1975.mat',\n",
       " 'A2491.mat',\n",
       " 'A6084.mat',\n",
       " 'A6001.mat',\n",
       " 'A5800.mat',\n",
       " 'A5198.mat',\n",
       " 'A2972.mat',\n",
       " 'A2544.mat',\n",
       " 'A1971.mat',\n",
       " 'A5445.mat',\n",
       " 'A4809.mat',\n",
       " 'A2727.mat',\n",
       " 'A1906.mat',\n",
       " 'A1249.mat',\n",
       " 'A6191.mat',\n",
       " 'A0295.mat',\n",
       " 'A5161.mat',\n",
       " 'A4305.mat',\n",
       " 'A4885.mat',\n",
       " 'A1234.mat',\n",
       " 'A4376.mat',\n",
       " 'A2280.mat',\n",
       " 'A3769.mat',\n",
       " 'A1653.mat',\n",
       " 'A1875.mat',\n",
       " 'A0979.mat',\n",
       " 'A3998.mat',\n",
       " 'A1222.mat',\n",
       " 'A5353.mat',\n",
       " 'A5515.mat',\n",
       " 'A2029.mat',\n",
       " 'A3231.mat',\n",
       " 'A2023.mat',\n",
       " 'A6614.mat',\n",
       " 'A6761.mat',\n",
       " 'A6500.mat',\n",
       " 'A3261.mat',\n",
       " 'A2052.mat',\n",
       " 'A0411.mat',\n",
       " 'A3864.mat',\n",
       " 'A0935.mat',\n",
       " 'A5980.mat',\n",
       " 'A2578.mat',\n",
       " 'A4278.mat',\n",
       " 'A1471.mat',\n",
       " 'A6239.mat',\n",
       " 'A1170.mat',\n",
       " 'A0196.mat',\n",
       " 'A5755.mat',\n",
       " 'A0130.mat',\n",
       " 'A5639.mat',\n",
       " 'A2529.mat',\n",
       " 'A5603.mat',\n",
       " 'A0435.mat',\n",
       " 'A0950.mat',\n",
       " 'A4012.mat',\n",
       " 'A6408.mat',\n",
       " 'A1880.mat',\n",
       " 'A0013.mat',\n",
       " 'A0513.mat',\n",
       " 'A5728.mat',\n",
       " 'A1257.mat',\n",
       " 'A2505.mat',\n",
       " 'A5427.mat',\n",
       " 'A6047.mat',\n",
       " 'A3462.mat',\n",
       " 'A6658.mat',\n",
       " 'A6749.mat',\n",
       " 'A0040.mat',\n",
       " 'A1957.mat',\n",
       " 'A4655.mat',\n",
       " 'A5024.mat',\n",
       " 'A3844.mat',\n",
       " 'A3174.mat',\n",
       " 'A1053.mat',\n",
       " 'A5551.mat',\n",
       " 'A5892.mat',\n",
       " 'A2783.mat',\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train # starts with A2355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A5949.mat',\n",
       " 'A0268.mat',\n",
       " 'A0938.mat',\n",
       " 'A0379.mat',\n",
       " 'A6235.mat',\n",
       " 'A5085.mat',\n",
       " 'A0368.mat',\n",
       " 'A4547.mat',\n",
       " 'A6477.mat',\n",
       " 'A3075.mat',\n",
       " 'A1011.mat',\n",
       " 'A0821.mat',\n",
       " 'A6566.mat',\n",
       " 'A0688.mat',\n",
       " 'A3682.mat',\n",
       " 'A5270.mat',\n",
       " 'A6875.mat',\n",
       " 'A0698.mat',\n",
       " 'A1369.mat',\n",
       " 'A1633.mat',\n",
       " 'A3438.mat',\n",
       " 'A6648.mat',\n",
       " 'A0988.mat',\n",
       " 'A2271.mat',\n",
       " 'A4479.mat',\n",
       " 'A0499.mat',\n",
       " 'A5788.mat',\n",
       " 'A5377.mat',\n",
       " 'A4074.mat',\n",
       " 'A3279.mat',\n",
       " 'A0944.mat',\n",
       " 'A1244.mat',\n",
       " 'A4372.mat',\n",
       " 'A3159.mat',\n",
       " 'A5350.mat',\n",
       " 'A1703.mat',\n",
       " 'A5599.mat',\n",
       " 'A5319.mat',\n",
       " 'A3609.mat',\n",
       " 'A0018.mat',\n",
       " 'A4472.mat',\n",
       " 'A0262.mat',\n",
       " 'A3497.mat',\n",
       " 'A4132.mat',\n",
       " 'A2589.mat',\n",
       " 'A0033.mat',\n",
       " 'A6427.mat',\n",
       " 'A3516.mat',\n",
       " 'A3639.mat',\n",
       " 'A5699.mat',\n",
       " 'A6117.mat',\n",
       " 'A2881.mat',\n",
       " 'A1386.mat',\n",
       " 'A0592.mat',\n",
       " 'A5711.mat',\n",
       " 'A4696.mat',\n",
       " 'A3192.mat',\n",
       " 'A2427.mat',\n",
       " 'A5336.mat',\n",
       " 'A1233.mat',\n",
       " 'A3805.mat',\n",
       " 'A1748.mat',\n",
       " 'A0407.mat',\n",
       " 'A4002.mat',\n",
       " 'A1318.mat',\n",
       " 'A3105.mat',\n",
       " 'A3764.mat',\n",
       " 'A4894.mat',\n",
       " 'A5889.mat',\n",
       " 'A0225.mat',\n",
       " 'A3407.mat',\n",
       " 'A2347.mat',\n",
       " 'A5465.mat',\n",
       " 'A3875.mat',\n",
       " 'A5167.mat',\n",
       " 'A1366.mat',\n",
       " 'A0049.mat',\n",
       " 'A0113.mat',\n",
       " 'A6123.mat',\n",
       " 'A3566.mat',\n",
       " 'A0701.mat',\n",
       " 'A6303.mat',\n",
       " 'A3115.mat',\n",
       " 'A5031.mat',\n",
       " 'A2490.mat',\n",
       " 'A4606.mat',\n",
       " 'A0081.mat',\n",
       " 'A2835.mat',\n",
       " 'A2607.mat',\n",
       " 'A6738.mat',\n",
       " 'A0974.mat',\n",
       " 'A6260.mat',\n",
       " 'A0302.mat',\n",
       " 'A2296.mat',\n",
       " 'A5725.mat',\n",
       " 'A5072.mat',\n",
       " 'A1073.mat',\n",
       " 'A1976.mat',\n",
       " 'A0107.mat',\n",
       " 'A0464.mat',\n",
       " 'A4285.mat',\n",
       " 'A6208.mat',\n",
       " 'A2968.mat',\n",
       " 'A2649.mat',\n",
       " 'A5461.mat',\n",
       " 'A6360.mat',\n",
       " 'A6387.mat',\n",
       " 'A6320.mat',\n",
       " 'A4558.mat',\n",
       " 'A6367.mat',\n",
       " 'A2159.mat',\n",
       " 'A2873.mat',\n",
       " 'A4109.mat',\n",
       " 'A2846.mat',\n",
       " 'A5883.mat',\n",
       " 'A2360.mat',\n",
       " 'A4541.mat',\n",
       " 'A5606.mat',\n",
       " 'A6437.mat',\n",
       " 'A1273.mat',\n",
       " 'A5234.mat',\n",
       " 'A1254.mat',\n",
       " 'A6109.mat',\n",
       " 'A5438.mat',\n",
       " 'A4540.mat',\n",
       " 'A0848.mat',\n",
       " 'A1024.mat',\n",
       " 'A0346.mat',\n",
       " 'A1637.mat',\n",
       " 'A0398.mat',\n",
       " 'A0640.mat',\n",
       " 'A5425.mat',\n",
       " 'A0503.mat',\n",
       " 'A5601.mat',\n",
       " 'A1891.mat',\n",
       " 'A2433.mat',\n",
       " 'A6454.mat',\n",
       " 'A5348.mat',\n",
       " 'A5713.mat',\n",
       " 'A5550.mat',\n",
       " 'A4206.mat',\n",
       " 'A1134.mat',\n",
       " 'A4301.mat',\n",
       " 'A3984.mat',\n",
       " 'A3536.mat',\n",
       " 'A0780.mat',\n",
       " 'A6851.mat',\n",
       " 'A3042.mat',\n",
       " 'A4769.mat',\n",
       " 'A1084.mat',\n",
       " 'A4092.mat',\n",
       " 'A2399.mat',\n",
       " 'A5156.mat',\n",
       " 'A6015.mat',\n",
       " 'A6291.mat',\n",
       " 'A6618.mat',\n",
       " 'A2064.mat',\n",
       " 'A0847.mat',\n",
       " 'A5022.mat',\n",
       " 'A1497.mat',\n",
       " 'A2559.mat',\n",
       " 'A5186.mat',\n",
       " 'A5478.mat',\n",
       " 'A5884.mat',\n",
       " 'A4942.mat',\n",
       " 'A0129.mat',\n",
       " 'A0647.mat',\n",
       " 'A1928.mat',\n",
       " 'A2442.mat',\n",
       " 'A5615.mat',\n",
       " 'A0114.mat',\n",
       " 'A4934.mat',\n",
       " 'A3713.mat',\n",
       " 'A6009.mat',\n",
       " 'A3532.mat',\n",
       " 'A4067.mat',\n",
       " 'A3246.mat',\n",
       " 'A1501.mat',\n",
       " 'A3943.mat',\n",
       " 'A1973.mat',\n",
       " 'A4965.mat',\n",
       " 'A6193.mat',\n",
       " 'A5685.mat',\n",
       " 'A3207.mat',\n",
       " 'A1357.mat',\n",
       " 'A2577.mat',\n",
       " 'A2234.mat',\n",
       " 'A2365.mat',\n",
       " 'A5017.mat',\n",
       " 'A4250.mat',\n",
       " 'A1757.mat',\n",
       " 'A5271.mat',\n",
       " 'A1660.mat',\n",
       " 'A5392.mat',\n",
       " 'A3651.mat',\n",
       " 'A4330.mat',\n",
       " 'A4663.mat',\n",
       " 'A4674.mat',\n",
       " 'A4556.mat',\n",
       " 'A3480.mat',\n",
       " 'A1258.mat',\n",
       " 'A2800.mat',\n",
       " 'A1719.mat',\n",
       " 'A5899.mat',\n",
       " 'A1111.mat',\n",
       " 'A1033.mat',\n",
       " 'A1451.mat',\n",
       " 'A5867.mat',\n",
       " 'A3659.mat',\n",
       " 'A1862.mat',\n",
       " 'A1803.mat',\n",
       " 'A1707.mat',\n",
       " 'A5279.mat',\n",
       " 'A2707.mat',\n",
       " 'A2927.mat',\n",
       " 'A6386.mat',\n",
       " 'A0272.mat',\n",
       " 'A2923.mat',\n",
       " 'A3744.mat',\n",
       " 'A3199.mat',\n",
       " 'A3660.mat',\n",
       " 'A5464.mat',\n",
       " 'A3515.mat',\n",
       " 'A3738.mat',\n",
       " 'A3031.mat',\n",
       " 'A4218.mat',\n",
       " 'A4174.mat',\n",
       " 'A0255.mat',\n",
       " 'A6491.mat',\n",
       " 'A4361.mat',\n",
       " 'A5522.mat',\n",
       " 'A6565.mat',\n",
       " 'A4010.mat',\n",
       " 'A0984.mat',\n",
       " 'A3377.mat',\n",
       " 'A0633.mat',\n",
       " 'A5758.mat',\n",
       " 'A1587.mat',\n",
       " 'A5990.mat',\n",
       " 'A0531.mat',\n",
       " 'A0155.mat',\n",
       " 'A1154.mat',\n",
       " 'A2368.mat',\n",
       " 'A0973.mat',\n",
       " 'A4353.mat',\n",
       " 'A5771.mat',\n",
       " 'A2214.mat',\n",
       " 'A2831.mat',\n",
       " 'A6053.mat',\n",
       " 'A5631.mat',\n",
       " 'A2468.mat',\n",
       " 'A2014.mat',\n",
       " 'A1317.mat',\n",
       " 'A6139.mat',\n",
       " 'A5569.mat',\n",
       " 'A5517.mat',\n",
       " 'A1942.mat',\n",
       " 'A1512.mat',\n",
       " 'A0471.mat',\n",
       " 'A4394.mat',\n",
       " 'A3578.mat',\n",
       " 'A6157.mat',\n",
       " 'A6090.mat',\n",
       " 'A6571.mat',\n",
       " 'A6282.mat',\n",
       " 'A3152.mat',\n",
       " 'A3400.mat',\n",
       " 'A5964.mat',\n",
       " 'A2130.mat',\n",
       " 'A5300.mat',\n",
       " 'A5505.mat',\n",
       " 'A2315.mat',\n",
       " 'A6679.mat',\n",
       " 'A6852.mat',\n",
       " 'A5701.mat',\n",
       " 'A2985.mat']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_val # starts with A5949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A0349.mat',\n",
       " 'A3947.mat',\n",
       " 'A3900.mat',\n",
       " 'A0817.mat',\n",
       " 'A2140.mat',\n",
       " 'A6793.mat',\n",
       " 'A1879.mat',\n",
       " 'A0900.mat',\n",
       " 'A0454.mat',\n",
       " 'A6855.mat',\n",
       " 'A1142.mat',\n",
       " 'A2316.mat',\n",
       " 'A1701.mat',\n",
       " 'A0892.mat',\n",
       " 'A4156.mat',\n",
       " 'A6654.mat',\n",
       " 'A1902.mat',\n",
       " 'A3887.mat',\n",
       " 'A4328.mat',\n",
       " 'A6705.mat',\n",
       " 'A2393.mat',\n",
       " 'A1654.mat',\n",
       " 'A4534.mat',\n",
       " 'A4223.mat',\n",
       " 'A6232.mat',\n",
       " 'A5215.mat',\n",
       " 'A6361.mat',\n",
       " 'A2060.mat',\n",
       " 'A5460.mat',\n",
       " 'A4804.mat',\n",
       " 'A5066.mat',\n",
       " 'A6283.mat',\n",
       " 'A3889.mat',\n",
       " 'A0621.mat',\n",
       " 'A5856.mat',\n",
       " 'A0943.mat',\n",
       " 'A1936.mat',\n",
       " 'A3808.mat',\n",
       " 'A5473.mat',\n",
       " 'A6224.mat',\n",
       " 'A6848.mat',\n",
       " 'A1420.mat',\n",
       " 'A3335.mat',\n",
       " 'A3258.mat',\n",
       " 'A1623.mat',\n",
       " 'A4062.mat',\n",
       " 'A5721.mat',\n",
       " 'A5733.mat',\n",
       " 'A1238.mat',\n",
       " 'A4654.mat',\n",
       " 'A4504.mat',\n",
       " 'A0987.mat',\n",
       " 'A2839.mat',\n",
       " 'A4243.mat',\n",
       " 'A5778.mat',\n",
       " 'A6173.mat',\n",
       " 'A1782.mat',\n",
       " 'A5404.mat',\n",
       " 'A1443.mat',\n",
       " 'A3470.mat',\n",
       " 'A6876.mat',\n",
       " 'A1230.mat',\n",
       " 'A1870.mat',\n",
       " 'A1186.mat',\n",
       " 'A0736.mat',\n",
       " 'A4685.mat',\n",
       " 'A4562.mat',\n",
       " 'A4444.mat',\n",
       " 'A1568.mat',\n",
       " 'A3403.mat',\n",
       " 'A6465.mat',\n",
       " 'A0394.mat',\n",
       " 'A5871.mat',\n",
       " 'A4337.mat',\n",
       " 'A1373.mat',\n",
       " 'A0070.mat',\n",
       " 'A6119.mat',\n",
       " 'A0357.mat',\n",
       " 'A5125.mat',\n",
       " 'A4207.mat',\n",
       " 'A0589.mat',\n",
       " 'A6206.mat',\n",
       " 'A2661.mat',\n",
       " 'A0067.mat',\n",
       " 'A1720.mat',\n",
       " 'A0099.mat',\n",
       " 'A6102.mat',\n",
       " 'A4491.mat',\n",
       " 'A6297.mat',\n",
       " 'A1995.mat',\n",
       " 'A6764.mat',\n",
       " 'A5468.mat',\n",
       " 'A1848.mat',\n",
       " 'A5475.mat',\n",
       " 'A2865.mat',\n",
       " 'A1152.mat',\n",
       " 'A4520.mat',\n",
       " 'A5146.mat',\n",
       " 'A6845.mat',\n",
       " 'A6650.mat',\n",
       " 'A1263.mat',\n",
       " 'A5900.mat',\n",
       " 'A2369.mat',\n",
       " 'A4121.mat',\n",
       " 'A6720.mat',\n",
       " 'A5841.mat',\n",
       " 'A3953.mat',\n",
       " 'A0661.mat',\n",
       " 'A1674.mat',\n",
       " 'A4106.mat',\n",
       " 'A0351.mat',\n",
       " 'A2701.mat',\n",
       " 'A2383.mat',\n",
       " 'A6135.mat',\n",
       " 'A5118.mat',\n",
       " 'A0772.mat',\n",
       " 'A4303.mat',\n",
       " 'A3241.mat',\n",
       " 'A0297.mat',\n",
       " 'A4777.mat',\n",
       " 'A2963.mat',\n",
       " 'A1395.mat',\n",
       " 'A2960.mat',\n",
       " 'A1620.mat',\n",
       " 'A5459.mat',\n",
       " 'A0041.mat',\n",
       " 'A0292.mat',\n",
       " 'A5895.mat',\n",
       " 'A6290.mat',\n",
       " 'A4226.mat',\n",
       " 'A5278.mat',\n",
       " 'A1513.mat',\n",
       " 'A4723.mat',\n",
       " 'A4308.mat',\n",
       " 'A1051.mat',\n",
       " 'A3344.mat',\n",
       " 'A6174.mat',\n",
       " 'A5595.mat',\n",
       " 'A5959.mat',\n",
       " 'A1038.mat',\n",
       " 'A0628.mat',\n",
       " 'A3222.mat',\n",
       " 'A1506.mat',\n",
       " 'A5700.mat',\n",
       " 'A2585.mat',\n",
       " 'A1662.mat',\n",
       " 'A1449.mat',\n",
       " 'A5808.mat',\n",
       " 'A4501.mat',\n",
       " 'A6671.mat',\n",
       " 'A5095.mat',\n",
       " 'A3620.mat',\n",
       " 'A6805.mat',\n",
       " 'A6194.mat',\n",
       " 'A1397.mat',\n",
       " 'A6808.mat',\n",
       " 'A1474.mat',\n",
       " 'A6840.mat',\n",
       " 'A2194.mat',\n",
       " 'A2612.mat',\n",
       " 'A4429.mat',\n",
       " 'A1120.mat',\n",
       " 'A0947.mat',\n",
       " 'A1245.mat',\n",
       " 'A1000.mat',\n",
       " 'A3849.mat',\n",
       " 'A4846.mat',\n",
       " 'A0679.mat',\n",
       " 'A6340.mat',\n",
       " 'A6467.mat',\n",
       " 'A0805.mat',\n",
       " 'A5857.mat',\n",
       " 'A4733.mat',\n",
       " 'A2379.mat',\n",
       " 'A4918.mat',\n",
       " 'A2663.mat',\n",
       " 'A5734.mat',\n",
       " 'A2330.mat',\n",
       " 'A4634.mat',\n",
       " 'A0197.mat',\n",
       " 'A2415.mat',\n",
       " 'A0699.mat',\n",
       " 'A3259.mat',\n",
       " 'A3737.mat',\n",
       " 'A3772.mat',\n",
       " 'A5301.mat',\n",
       " 'A5208.mat',\n",
       " 'A1526.mat',\n",
       " 'A6160.mat',\n",
       " 'A0248.mat',\n",
       " 'A6011.mat',\n",
       " 'A4397.mat',\n",
       " 'A3599.mat',\n",
       " 'A3399.mat',\n",
       " 'A0891.mat',\n",
       " 'A3385.mat',\n",
       " 'A0438.mat',\n",
       " 'A2658.mat',\n",
       " 'A2749.mat',\n",
       " 'A5149.mat',\n",
       " 'A2425.mat',\n",
       " 'A2004.mat',\n",
       " 'A3248.mat',\n",
       " 'A2704.mat',\n",
       " 'A6551.mat',\n",
       " 'A4019.mat',\n",
       " 'A1920.mat',\n",
       " 'A3327.mat',\n",
       " 'A5730.mat',\n",
       " 'A5549.mat',\n",
       " 'A0595.mat',\n",
       " 'A4898.mat',\n",
       " 'A5019.mat',\n",
       " 'A1472.mat',\n",
       " 'A2900.mat',\n",
       " 'A1738.mat',\n",
       " 'A0488.mat',\n",
       " 'A3519.mat',\n",
       " 'A3038.mat',\n",
       " 'A6567.mat',\n",
       " 'A4173.mat',\n",
       " 'A5294.mat',\n",
       " 'A0906.mat',\n",
       " 'A0941.mat',\n",
       " 'A3356.mat',\n",
       " 'A5496.mat',\n",
       " 'A0133.mat',\n",
       " 'A6697.mat',\n",
       " 'A5586.mat',\n",
       " 'A2519.mat',\n",
       " 'A4785.mat',\n",
       " 'A5996.mat',\n",
       " 'A1106.mat',\n",
       " 'A0844.mat',\n",
       " 'A4964.mat',\n",
       " 'A4000.mat',\n",
       " 'A5048.mat',\n",
       " 'A1826.mat',\n",
       " 'A5593.mat',\n",
       " 'A3573.mat',\n",
       " 'A4123.mat',\n",
       " 'A5166.mat',\n",
       " 'A2318.mat',\n",
       " 'A3752.mat',\n",
       " 'A3311.mat',\n",
       " 'A5926.mat',\n",
       " 'A3113.mat',\n",
       " 'A3313.mat',\n",
       " 'A1272.mat',\n",
       " 'A5660.mat',\n",
       " 'A0396.mat',\n",
       " 'A3951.mat',\n",
       " 'A5637.mat',\n",
       " 'A0634.mat',\n",
       " 'A3533.mat',\n",
       " 'A4168.mat',\n",
       " 'A5129.mat',\n",
       " 'A2815.mat',\n",
       " 'A1915.mat',\n",
       " 'A2374.mat',\n",
       " 'A6184.mat',\n",
       " 'A6598.mat',\n",
       " 'A2511.mat',\n",
       " 'A1752.mat',\n",
       " 'A0123.mat',\n",
       " 'A4298.mat',\n",
       " 'A1755.mat',\n",
       " 'A1963.mat',\n",
       " 'A6432.mat',\n",
       " 'A5668.mat',\n",
       " 'A3101.mat',\n",
       " 'A1310.mat',\n",
       " 'A6706.mat',\n",
       " 'A0779.mat',\n",
       " 'A5057.mat',\n",
       " 'A3569.mat',\n",
       " 'A2933.mat',\n",
       " 'A0770.mat',\n",
       " 'A4704.mat',\n",
       " 'A2501.mat',\n",
       " 'A5636.mat',\n",
       " 'A1781.mat',\n",
       " 'A3434.mat',\n",
       " 'A5640.mat',\n",
       " 'A0452.mat',\n",
       " 'A5801.mat',\n",
       " 'A2184.mat',\n",
       " 'A4164.mat',\n",
       " 'A4633.mat',\n",
       " 'A5184.mat',\n",
       " 'A3338.mat',\n",
       " 'A2868.mat',\n",
       " 'A6719.mat',\n",
       " 'A2402.mat',\n",
       " 'A0946.mat',\n",
       " 'A3816.mat',\n",
       " 'A2667.mat',\n",
       " 'A2290.mat',\n",
       " 'A5011.mat',\n",
       " 'A2568.mat',\n",
       " 'A3894.mat',\n",
       " 'A5063.mat',\n",
       " 'A4751.mat',\n",
       " 'A0284.mat',\n",
       " 'A4055.mat',\n",
       " 'A6156.mat',\n",
       " 'A1551.mat',\n",
       " 'A6478.mat',\n",
       " 'A5414.mat',\n",
       " 'A2860.mat',\n",
       " 'A0425.mat',\n",
       " 'A2682.mat',\n",
       " 'A0716.mat',\n",
       " 'A3262.mat',\n",
       " 'A1507.mat',\n",
       " 'A2069.mat',\n",
       " 'A4380.mat',\n",
       " 'A4636.mat',\n",
       " 'A0287.mat',\n",
       " 'A0266.mat',\n",
       " 'A0338.mat',\n",
       " 'A2090.mat',\n",
       " 'A3027.mat',\n",
       " 'A4280.mat',\n",
       " 'A0735.mat',\n",
       " 'A1500.mat',\n",
       " 'A0308.mat',\n",
       " 'A0373.mat',\n",
       " 'A5525.mat',\n",
       " 'A4322.mat',\n",
       " 'A0807.mat',\n",
       " 'A5084.mat',\n",
       " 'A4315.mat',\n",
       " 'A2772.mat',\n",
       " 'A3456.mat',\n",
       " 'A2991.mat',\n",
       " 'A1276.mat',\n",
       " 'A2542.mat',\n",
       " 'A0676.mat',\n",
       " 'A3415.mat',\n",
       " 'A4779.mat',\n",
       " 'A2885.mat',\n",
       " 'A5821.mat',\n",
       " 'A6069.mat',\n",
       " 'A5818.mat',\n",
       " 'A1144.mat',\n",
       " 'A0362.mat',\n",
       " 'A1560.mat',\n",
       " 'A5682.mat',\n",
       " 'A5643.mat',\n",
       " 'A1156.mat',\n",
       " 'A5226.mat',\n",
       " 'A3908.mat',\n",
       " 'A5484.mat',\n",
       " 'A5781.mat',\n",
       " 'A6129.mat',\n",
       " 'A3641.mat',\n",
       " 'A5050.mat',\n",
       " 'A4900.mat',\n",
       " 'A0174.mat',\n",
       " 'A0910.mat',\n",
       " 'A4715.mat',\n",
       " 'A4113.mat',\n",
       " 'A5976.mat',\n",
       " 'A0228.mat',\n",
       " 'A1864.mat',\n",
       " 'A6051.mat',\n",
       " 'A2361.mat',\n",
       " 'A1010.mat',\n",
       " 'A0863.mat',\n",
       " 'A5457.mat',\n",
       " 'A1109.mat',\n",
       " 'A3173.mat',\n",
       " 'A0258.mat',\n",
       " 'A3624.mat',\n",
       " 'A1894.mat',\n",
       " 'A0869.mat',\n",
       " 'A5006.mat',\n",
       " 'A3050.mat',\n",
       " 'A0458.mat',\n",
       " 'A2089.mat',\n",
       " 'A1578.mat',\n",
       " 'A2975.mat',\n",
       " 'A5671.mat',\n",
       " 'A1340.mat',\n",
       " 'A5571.mat',\n",
       " 'A0210.mat',\n",
       " 'A5299.mat',\n",
       " 'A1007.mat',\n",
       " 'A0563.mat',\n",
       " 'A4462.mat',\n",
       " 'A1095.mat',\n",
       " 'A1241.mat',\n",
       " 'A3315.mat',\n",
       " 'A5558.mat',\n",
       " 'A3334.mat',\n",
       " 'A4702.mat',\n",
       " 'A3727.mat',\n",
       " 'A1047.mat',\n",
       " 'A3994.mat',\n",
       " 'A3906.mat',\n",
       " 'A1692.mat',\n",
       " 'A1379.mat',\n",
       " 'A0662.mat',\n",
       " 'A4821.mat',\n",
       " 'A0702.mat',\n",
       " 'A3019.mat',\n",
       " 'A3352.mat',\n",
       " 'A1364.mat',\n",
       " 'A4571.mat',\n",
       " 'A4797.mat',\n",
       " 'A0911.mat',\n",
       " 'A3676.mat',\n",
       " 'A2540.mat',\n",
       " 'A1143.mat',\n",
       " 'A5180.mat',\n",
       " 'A1572.mat',\n",
       " 'A3374.mat',\n",
       " 'A5611.mat',\n",
       " 'A3100.mat',\n",
       " 'A0731.mat',\n",
       " 'A3859.mat',\n",
       " 'A6008.mat',\n",
       " 'A1050.mat',\n",
       " 'A2122.mat',\n",
       " 'A4414.mat',\n",
       " 'A3066.mat',\n",
       " 'A2961.mat',\n",
       " 'A0404.mat',\n",
       " 'A5814.mat',\n",
       " 'A4791.mat',\n",
       " 'A3520.mat',\n",
       " 'A4665.mat',\n",
       " 'A2049.mat',\n",
       " 'A3791.mat',\n",
       " 'A2819.mat',\n",
       " 'A0397.mat',\n",
       " 'A6060.mat',\n",
       " 'A4670.mat',\n",
       " 'A1196.mat',\n",
       " 'A1510.mat',\n",
       " 'A6244.mat',\n",
       " 'A3917.mat',\n",
       " 'A6620.mat',\n",
       " 'A2027.mat',\n",
       " 'A5009.mat',\n",
       " 'A1505.mat',\n",
       " 'A5406.mat',\n",
       " 'A6093.mat',\n",
       " 'A5054.mat',\n",
       " 'A3301.mat',\n",
       " 'A3724.mat',\n",
       " 'A6266.mat',\n",
       " 'A6134.mat',\n",
       " 'A3185.mat',\n",
       " 'A4176.mat',\n",
       " 'A4649.mat',\n",
       " 'A4269.mat',\n",
       " 'A0750.mat',\n",
       " 'A6181.mat',\n",
       " 'A3819.mat',\n",
       " 'A3287.mat',\n",
       " 'A2182.mat',\n",
       " 'A1408.mat',\n",
       " 'A3213.mat',\n",
       " 'A3490.mat',\n",
       " 'A2920.mat',\n",
       " 'A4627.mat',\n",
       " 'A2343.mat',\n",
       " 'A3647.mat',\n",
       " 'A4917.mat',\n",
       " 'A1066.mat',\n",
       " 'A0494.mat',\n",
       " 'A4266.mat',\n",
       " 'A5283.mat',\n",
       " 'A3912.mat',\n",
       " 'A4763.mat',\n",
       " 'A1523.mat',\n",
       " 'A4768.mat',\n",
       " 'A4302.mat',\n",
       " 'A0882.mat',\n",
       " 'A2764.mat',\n",
       " 'A4475.mat',\n",
       " 'A3696.mat',\n",
       " 'A4059.mat',\n",
       " 'A4063.mat',\n",
       " 'A0795.mat',\n",
       " 'A3638.mat',\n",
       " 'A1118.mat',\n",
       " 'A0188.mat',\n",
       " 'A5622.mat',\n",
       " 'A3862.mat',\n",
       " 'A6103.mat',\n",
       " 'A3260.mat',\n",
       " 'A0818.mat',\n",
       " 'A5588.mat',\n",
       " 'A5791.mat',\n",
       " 'A0968.mat',\n",
       " 'A6568.mat',\n",
       " 'A3396.mat',\n",
       " 'A5287.mat',\n",
       " 'A5835.mat',\n",
       " 'A2189.mat',\n",
       " 'A5389.mat',\n",
       " 'A4455.mat',\n",
       " 'A4673.mat',\n",
       " 'A5532.mat',\n",
       " 'A5542.mat',\n",
       " 'A3296.mat',\n",
       " 'A3129.mat',\n",
       " 'A2136.mat',\n",
       " 'A4684.mat',\n",
       " 'A4146.mat',\n",
       " 'A5961.mat',\n",
       " 'A0669.mat',\n",
       " 'A1093.mat',\n",
       " 'A1431.mat',\n",
       " 'A6680.mat',\n",
       " 'A1164.mat',\n",
       " 'A3388.mat',\n",
       " 'A5720.mat',\n",
       " 'A2470.mat',\n",
       " 'A4866.mat',\n",
       " 'A3059.mat',\n",
       " 'A5953.mat',\n",
       " 'A6416.mat',\n",
       " 'A4148.mat',\n",
       " 'A5753.mat',\n",
       " 'A3634.mat',\n",
       " 'A1704.mat',\n",
       " 'A1834.mat',\n",
       " 'A5411.mat',\n",
       " 'A1496.mat',\n",
       " 'A6330.mat',\n",
       " 'A6553.mat',\n",
       " 'A0095.mat',\n",
       " 'A1260.mat',\n",
       " 'A2774.mat',\n",
       " 'A3637.mat',\n",
       " 'A3586.mat',\n",
       " 'A0036.mat',\n",
       " 'A0037.mat',\n",
       " 'A1436.mat',\n",
       " 'A4815.mat',\n",
       " 'A0709.mat',\n",
       " 'A3068.mat',\n",
       " 'A5817.mat',\n",
       " 'A6038.mat',\n",
       " 'A4093.mat',\n",
       " 'A3453.mat',\n",
       " 'A4899.mat',\n",
       " 'A2586.mat',\n",
       " 'A6775.mat',\n",
       " 'A4581.mat',\n",
       " 'A3775.mat',\n",
       " 'A2152.mat',\n",
       " 'A5590.mat',\n",
       " 'A2039.mat',\n",
       " 'A0263.mat',\n",
       " 'A4069.mat',\n",
       " 'A0665.mat',\n",
       " 'A4722.mat',\n",
       " 'A1562.mat',\n",
       " 'A1969.mat',\n",
       " 'A0742.mat',\n",
       " 'A1964.mat',\n",
       " 'A2313.mat',\n",
       " 'A0319.mat',\n",
       " 'A5592.mat',\n",
       " 'A2238.mat',\n",
       " 'A1917.mat',\n",
       " 'A6607.mat',\n",
       " 'A1401.mat',\n",
       " 'A1259.mat',\n",
       " 'A5628.mat',\n",
       " 'A0516.mat',\n",
       " 'A1989.mat',\n",
       " 'A4742.mat',\n",
       " 'A4893.mat',\n",
       " 'A4188.mat',\n",
       " 'A1121.mat',\n",
       " 'A6094.mat',\n",
       " 'A5060.mat',\n",
       " 'A1370.mat',\n",
       " 'A0710.mat',\n",
       " 'A5001.mat',\n",
       " 'A2602.mat',\n",
       " 'A5396.mat',\n",
       " 'A0721.mat',\n",
       " 'A4601.mat',\n",
       " 'A5272.mat',\n",
       " 'A3506.mat',\n",
       " 'A2765.mat',\n",
       " 'A1248.mat',\n",
       " 'A5183.mat',\n",
       " 'A3089.mat',\n",
       " 'A3576.mat',\n",
       " 'A3343.mat',\n",
       " 'A0218.mat',\n",
       " 'A3962.mat',\n",
       " 'A2740.mat',\n",
       " 'A1668.mat',\n",
       " 'A4314.mat',\n",
       " 'A3134.mat',\n",
       " 'A5860.mat',\n",
       " 'A1636.mat',\n",
       " 'A5969.mat',\n",
       " 'A1197.mat',\n",
       " 'A3501.mat',\n",
       " 'A6441.mat',\n",
       " 'A2650.mat',\n",
       " 'A0073.mat',\n",
       " 'A0749.mat',\n",
       " 'A0031.mat',\n",
       " 'A2286.mat',\n",
       " 'A3341.mat',\n",
       " 'A0442.mat',\n",
       " 'A6678.mat',\n",
       " 'A0724.mat',\n",
       " 'A6695.mat',\n",
       " 'A6059.mat',\n",
       " 'A4586.mat',\n",
       " 'A3587.mat',\n",
       " 'A0942.mat',\n",
       " 'A2453.mat',\n",
       " 'A2756.mat',\n",
       " 'A0748.mat',\n",
       " 'A4242.mat',\n",
       " 'A1923.mat',\n",
       " 'A6555.mat',\n",
       " 'A5985.mat',\n",
       " 'A5893.mat',\n",
       " 'A5751.mat',\n",
       " 'A5930.mat',\n",
       " 'A3591.mat',\n",
       " 'A2796.mat',\n",
       " 'A2419.mat',\n",
       " 'A4133.mat',\n",
       " 'A6107.mat',\n",
       " 'A2308.mat',\n",
       " 'A4470.mat',\n",
       " 'A0523.mat',\n",
       " 'A3723.mat',\n",
       " 'A3666.mat',\n",
       " 'A1522.mat',\n",
       " 'A4978.mat',\n",
       " 'A2518.mat',\n",
       " 'A6505.mat',\n",
       " 'A2898.mat',\n",
       " 'A4519.mat',\n",
       " 'A0803.mat',\n",
       " 'A3266.mat',\n",
       " 'A3036.mat',\n",
       " 'A1338.mat',\n",
       " 'A2970.mat',\n",
       " 'A4587.mat',\n",
       " 'A2716.mat',\n",
       " 'A5553.mat',\n",
       " 'A6126.mat',\n",
       " 'A5333.mat',\n",
       " 'A3856.mat',\n",
       " 'A0650.mat',\n",
       " 'A0597.mat',\n",
       " 'A2278.mat',\n",
       " 'A4800.mat',\n",
       " 'A5052.mat',\n",
       " 'A4761.mat',\n",
       " 'A2331.mat',\n",
       " 'A6385.mat',\n",
       " 'A2031.mat',\n",
       " 'A3526.mat',\n",
       " 'A2304.mat',\n",
       " 'A5376.mat',\n",
       " 'A1882.mat',\n",
       " 'A0846.mat',\n",
       " 'A1405.mat',\n",
       " 'A1784.mat',\n",
       " 'A3555.mat',\n",
       " 'A4338.mat',\n",
       " 'A4078.mat',\n",
       " 'A1588.mat',\n",
       " 'A2207.mat',\n",
       " 'A2022.mat',\n",
       " 'A3148.mat',\n",
       " 'A0948.mat',\n",
       " 'A5745.mat',\n",
       " 'A6815.mat',\n",
       " 'A1037.mat',\n",
       " 'A4200.mat',\n",
       " 'A5508.mat',\n",
       " 'A1983.mat',\n",
       " 'A2147.mat',\n",
       " 'A0152.mat',\n",
       " 'A1999.mat',\n",
       " 'A3983.mat',\n",
       " 'A6530.mat',\n",
       " 'A4808.mat',\n",
       " 'A5960.mat',\n",
       " 'A3169.mat',\n",
       " 'A2778.mat',\n",
       " 'A4089.mat',\n",
       " 'A6132.mat',\n",
       " 'A4210.mat',\n",
       " 'A6745.mat',\n",
       " 'A4369.mat',\n",
       " 'A2795.mat',\n",
       " 'A6170.mat',\n",
       " 'A4351.mat',\n",
       " 'A6019.mat',\n",
       " 'A5173.mat',\n",
       " 'A0858.mat',\n",
       " 'A4841.mat',\n",
       " 'A2440.mat',\n",
       " 'A6615.mat',\n",
       " 'A4603.mat',\n",
       " 'A4915.mat',\n",
       " 'A6296.mat',\n",
       " 'A6210.mat',\n",
       " 'A4267.mat',\n",
       " 'A4983.mat',\n",
       " 'A6790.mat',\n",
       " 'A1312.mat',\n",
       " 'A1293.mat',\n",
       " 'A2852.mat',\n",
       " 'A1837.mat',\n",
       " 'A5453.mat',\n",
       " 'A0865.mat',\n",
       " 'A6302.mat',\n",
       " 'A3347.mat',\n",
       " 'A0109.mat',\n",
       " 'A2781.mat',\n",
       " 'A3577.mat',\n",
       " 'A6850.mat',\n",
       " 'A0050.mat',\n",
       " 'A1993.mat',\n",
       " 'A3478.mat',\n",
       " 'A1896.mat',\n",
       " 'A4937.mat',\n",
       " 'A0205.mat',\n",
       " 'A2107.mat',\n",
       " 'A6483.mat',\n",
       " 'A4061.mat',\n",
       " 'A5469.mat',\n",
       " 'A4395.mat',\n",
       " 'A4967.mat',\n",
       " 'A4329.mat',\n",
       " 'A4993.mat',\n",
       " 'A4432.mat',\n",
       " 'A6407.mat',\n",
       " 'A5954.mat',\n",
       " 'A5811.mat',\n",
       " 'A2742.mat',\n",
       " 'A1307.mat',\n",
       " 'A1535.mat',\n",
       " 'A1925.mat',\n",
       " 'A3556.mat',\n",
       " 'A2252.mat',\n",
       " 'A5596.mat',\n",
       " 'A1178.mat',\n",
       " 'A4094.mat',\n",
       " 'A1959.mat',\n",
       " 'A3542.mat',\n",
       " 'A6495.mat',\n",
       " 'A5163.mat',\n",
       " 'A0727.mat',\n",
       " 'A0010.mat',\n",
       " 'A5192.mat',\n",
       " 'A3282.mat',\n",
       " 'A2684.mat',\n",
       " 'A3602.mat',\n",
       " 'A2200.mat',\n",
       " 'A4246.mat',\n",
       " 'A2850.mat',\n",
       " 'A5607.mat',\n",
       " 'A6663.mat',\n",
       " 'A4862.mat',\n",
       " 'A0612.mat',\n",
       " 'A0939.mat',\n",
       " 'A5133.mat',\n",
       " 'A1596.mat',\n",
       " 'A3122.mat',\n",
       " 'A0899.mat',\n",
       " 'A0551.mat',\n",
       " 'A3781.mat',\n",
       " 'A3032.mat',\n",
       " 'A1655.mat',\n",
       " 'A5360.mat',\n",
       " 'A4826.mat',\n",
       " 'A5724.mat',\n",
       " 'A1665.mat',\n",
       " 'A6734.mat',\n",
       " 'A1243.mat',\n",
       " 'A4335.mat',\n",
       " 'A2871.mat',\n",
       " 'A6365.mat',\n",
       " 'A6494.mat',\n",
       " 'A3443.mat',\n",
       " 'A1858.mat',\n",
       " 'A1961.mat',\n",
       " 'A3568.mat',\n",
       " 'A0931.mat',\n",
       " 'A3605.mat',\n",
       " 'A3381.mat',\n",
       " 'A2640.mat',\n",
       " 'A1680.mat',\n",
       " 'A6341.mat',\n",
       " 'A0953.mat',\n",
       " 'A3228.mat',\n",
       " 'A5197.mat',\n",
       " 'A4765.mat',\n",
       " 'A1977.mat',\n",
       " 'A2436.mat',\n",
       " 'A5865.mat',\n",
       " 'A5576.mat',\n",
       " 'A6538.mat',\n",
       " 'A6043.mat',\n",
       " 'A5676.mat',\n",
       " 'A2403.mat',\n",
       " 'A5987.mat',\n",
       " 'A3774.mat',\n",
       " 'A4111.mat',\n",
       " 'A5511.mat',\n",
       " 'A2167.mat',\n",
       " 'A4316.mat',\n",
       " 'A4617.mat',\n",
       " 'A6532.mat',\n",
       " 'A1877.mat',\n",
       " 'A1927.mat',\n",
       " 'A0624.mat',\n",
       " 'A1250.mat',\n",
       " 'A4853.mat',\n",
       " 'A4359.mat',\n",
       " 'A6022.mat',\n",
       " 'A4293.mat',\n",
       " 'A0777.mat',\n",
       " 'A6782.mat',\n",
       " 'A5692.mat',\n",
       " 'A5429.mat',\n",
       " 'A0924.mat',\n",
       " 'A2805.mat',\n",
       " 'A1406.mat',\n",
       " 'A5451.mat',\n",
       " 'A1323.mat',\n",
       " 'A3537.mat',\n",
       " 'A2986.mat',\n",
       " 'A6635.mat',\n",
       " 'A6331.mat',\n",
       " 'A2956.mat',\n",
       " 'A6601.mat',\n",
       " 'A3617.mat',\n",
       " 'A0981.mat',\n",
       " 'A2605.mat',\n",
       " 'A6187.mat',\n",
       " 'A0967.mat',\n",
       " 'A1253.mat',\n",
       " 'A0566.mat',\n",
       " 'A5235.mat',\n",
       " 'A6522.mat',\n",
       " 'A1676.mat',\n",
       " 'A2245.mat',\n",
       " 'A5971.mat',\n",
       " 'A6388.mat',\n",
       " 'A4227.mat',\n",
       " 'A3393.mat',\n",
       " 'A4265.mat',\n",
       " 'A3880.mat',\n",
       " 'A4170.mat',\n",
       " 'A6529.mat',\n",
       " 'A1158.mat',\n",
       " 'A2703.mat',\n",
       " 'A0989.mat',\n",
       " 'A1008.mat',\n",
       " 'A4454.mat',\n",
       " 'A2116.mat',\n",
       " 'A4409.mat',\n",
       " 'A1615.mat',\n",
       " 'A0632.mat',\n",
       " 'A6817.mat',\n",
       " 'A2935.mat',\n",
       " 'A2543.mat',\n",
       " 'A4075.mat',\n",
       " 'A3227.mat',\n",
       " 'A0781.mat',\n",
       " 'A0993.mat',\n",
       " 'A2001.mat',\n",
       " 'A4500.mat',\n",
       " 'A1765.mat',\n",
       " 'A5366.mat',\n",
       " 'A2931.mat',\n",
       " 'A2311.mat',\n",
       " 'A4254.mat',\n",
       " 'A3215.mat',\n",
       " 'A5868.mat',\n",
       " 'A4594.mat',\n",
       " 'A6436.mat',\n",
       " 'A1528.mat',\n",
       " 'A5430.mat',\n",
       " 'A3202.mat',\n",
       " 'A6196.mat',\n",
       " 'A5718.mat',\n",
       " 'A2889.mat',\n",
       " 'A3766.mat',\n",
       " 'A2212.mat',\n",
       " 'A2102.mat',\n",
       " 'A2980.mat',\n",
       " 'A3349.mat',\n",
       " 'A3812.mat',\n",
       " 'A1456.mat',\n",
       " 'A3707.mat',\n",
       " 'A3588.mat',\n",
       " 'A6834.mat',\n",
       " 'A4098.mat',\n",
       " 'A0352.mat',\n",
       " 'A2752.mat',\n",
       " 'A6231.mat',\n",
       " 'A0136.mat',\n",
       " 'A4016.mat',\n",
       " 'A6547.mat',\n",
       " 'A1644.mat',\n",
       " 'A0871.mat',\n",
       " 'A3255.mat',\n",
       " 'A0447.mat',\n",
       " 'A0315.mat',\n",
       " 'A3220.mat',\n",
       " 'A5799.mat',\n",
       " 'A6039.mat',\n",
       " 'A1865.mat',\n",
       " 'A5760.mat',\n",
       " 'A4236.mat',\n",
       " 'A1138.mat',\n",
       " 'A1080.mat',\n",
       " 'A2071.mat',\n",
       " 'A5621.mat',\n",
       " 'A2573.mat',\n",
       " 'A6262.mat',\n",
       " 'A2053.mat',\n",
       " 'A1172.mat',\n",
       " 'A4482.mat',\n",
       " 'A2009.mat',\n",
       " 'A6792.mat',\n",
       " 'A5936.mat',\n",
       " 'A6628.mat',\n",
       " 'A3424.mat',\n",
       " 'A5217.mat',\n",
       " 'A1064.mat',\n",
       " 'A3662.mat',\n",
       " 'A5703.mat',\n",
       " 'A1737.mat',\n",
       " 'A1072.mat',\n",
       " 'A0560.mat',\n",
       " 'A0185.mat',\n",
       " 'A3982.mat',\n",
       " 'A6023.mat',\n",
       " 'A2012.mat',\n",
       " 'A2801.mat',\n",
       " 'A2108.mat',\n",
       " 'A2070.mat',\n",
       " 'A4124.mat',\n",
       " 'A4332.mat',\n",
       " 'A2863.mat',\n",
       " 'A4465.mat',\n",
       " 'A6405.mat',\n",
       " 'A0176.mat',\n",
       " 'A6729.mat',\n",
       " 'A5816.mat',\n",
       " 'A2467.mat',\n",
       " 'A0726.mat',\n",
       " 'A4318.mat',\n",
       " 'A0194.mat',\n",
       " 'A4014.mat',\n",
       " 'A6523.mat',\n",
       " 'A3339.mat',\n",
       " 'A5432.mat',\n",
       " 'A2753.mat',\n",
       " 'A4034.mat',\n",
       " 'A5247.mat',\n",
       " 'A4652.mat',\n",
       " 'A6018.mat',\n",
       " 'A2575.mat',\n",
       " 'A0323.mat',\n",
       " 'A6383.mat',\n",
       " 'A5614.mat',\n",
       " 'A2616.mat',\n",
       " 'A4452.mat',\n",
       " 'A0714.mat',\n",
       " 'A2720.mat',\n",
       " 'A1705.mat',\n",
       " 'A2611.mat',\n",
       " 'A3033.mat',\n",
       " 'A2705.mat',\n",
       " 'A6236.mat',\n",
       " 'A3821.mat',\n",
       " 'A3552.mat',\n",
       " 'A3226.mat',\n",
       " 'A4309.mat',\n",
       " 'A2404.mat',\n",
       " 'A6682.mat',\n",
       " 'A4141.mat',\n",
       " 'A3332.mat',\n",
       " 'A3201.mat',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test # starts with A0349"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking how many multilabel in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366\n",
      "19\n",
      "91\n"
     ]
    }
   ],
   "source": [
    "multi_train, _, _ = searching_overlap(input_directory, class2index, data_train)\n",
    "multi_val, _, _ = searching_overlap(input_directory, class2index, data_val)\n",
    "multi_test, _, _ = searching_overlap(input_directory, class2index, data_test)\n",
    "print(len(multi_train))\n",
    "print(len(multi_val))\n",
    "print(len(multi_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For attention editting using CAM extracted from primitive ABN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, 12)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, None, 64)     2368        input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, 64)     256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 64)     12352       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 64)     256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, None, 64)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 128)    24704       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, 128)    512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 128)    49280       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, 128)    512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, None, 128)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 256)    98560       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, 256)    1024        conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 256)    196864      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, 256)    1024        conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 256)    196864      batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, 256)    1024        conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, None, 256)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 512)    393728      max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, 512)    2048        conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, None, 512)    786944      batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, 512)    2048        conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, None, 512)    786944      batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, 512)    2048        conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, None, 512)    0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, None, 512)    786944      max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, 512)    2048        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, None, 256)    393472      batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, 256)    1024        conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, None, 128)    98432       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, 128)    512         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, None, 128)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, None, 64)     8192        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, 64)     256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 64)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, None, 64)     12288       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, 64)     256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 64)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, None, 256)    16384       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, None, 256)    32768       max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, None, 256)    1024        conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, 256)    1024        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, None, 256)    0           batch_normalization_16[0][0]     \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, 256)    0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_bn_1 (BatchNor (None, None, 256)    1024        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_conv_1 (Conv1D (None, None, 9)      2304        attention_branch_bn_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_conv_1 (Co (None, None, 1)      9           attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_bn_1 (Batc (None, None, 1)      4           attention_branch_att_conv_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_sigmoid_1  (None, None, 1)      0           attention_branch_att_bn_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, None, 128)    0           max_pooling1d_4[0][0]            \n",
      "                                                                 attention_branch_att_sigmoid_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, None, 64)     8192        lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, None, 64)     256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, 64)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, None, 64)     12288       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, None, 64)     256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, 64)     0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, None, 256)    16384       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, None, 256)    32768       lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, None, 256)    1024        conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, None, 256)    1024        conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, 256)    0           batch_normalization_20[0][0]     \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, 256)    0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_pred_conv_1 (C (None, None, 9)      81          attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_avgpool_1 (Gl (None, 256)          0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_gap_1 (GlobalA (None, 9)            0           attention_branch_pred_conv_1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_2 (Dens (None, 9)            2313        perception_branch_avgpool_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_output (Activa (None, 9)            0           attention_branch_gap_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_output (Activ (None, 9)            0           perception_branch_dense_2[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 3,991,911\n",
      "Trainable params: 3,981,669\n",
      "Non-trainable params: 10,242\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# CAMdir = '/home/taejoon/PhysioNetChallenge/results_20200622_ABN_multiclass_V4_primitiveABN'\n",
    "CAMdir = '/home/taejoon/PhysioNetChallenge/results_20200625_V4_ABN_primitive_n=1'\n",
    "p_model = primitive_ABN((None, 12), 9, minimum_len, out_ch=256, n=1)\n",
    "latest = tf.train.latest_checkpoint(CAMdir)\n",
    "p_model.load_weights(latest)\n",
    "\n",
    "conv_layer = 'activation_5'\n",
    "softmax_layer = 'perception_branch_dense_2'\n",
    "\n",
    "\n",
    "out_len=12\n",
    "get_conv_out = K.function(p_model.input, [p_model.get_layer(conv_layer).output, p_model.get_layer(softmax_layer).weights[0]])\n",
    "\n",
    "def CAM_conv1D(minimum_len, n_channels, x, y, out_len, get_conv_out):\n",
    "    \n",
    "    # x랑 y는 batch size만큼의 리스트 (32)\n",
    "    heatmaps=[]    \n",
    "    \n",
    "    curr_x = np.asarray(x)\n",
    "    curr_x = curr_x.reshape(len(x),minimum_len,n_channels)\n",
    "    \n",
    "    conv_out, softmax_weights = get_conv_out(curr_x)\n",
    "\n",
    "    for i in range(len(x)):\n",
    "\n",
    "        curr_classes = y[i]\n",
    "        class_index=[]\n",
    "        [class_index.append(j) for j in range(len(curr_classes)) if curr_classes[j]==1]\n",
    "        heatmap = np.zeros((1,12)) #heatmap=np.zeros((1,36)) # might need to fix this if GradCAM or primitive model changes\n",
    "\n",
    "        conv_out_ = conv_out[i] # (36, 128) / (32, 12, 256)\n",
    "#         print(conv_out.shape)\n",
    "\n",
    "        for label in class_index:  # multiclass일 경우 대비해서 for문\n",
    "            curr_weights = softmax_weights[:,label]\n",
    "            weighted_conv = conv_out_*curr_weights\n",
    "            \n",
    "            weighted_conv = weighted_conv.sum(axis=-1) # output = (1,36)\n",
    "            heatmap += weighted_conv\n",
    "            \n",
    "        heatmap %= len(class_index) # 단일 class일 경우 1로 나눠짐. 두개일 경우 더해진 heatmap들이 2로 나눠짐\n",
    "#         heatmap = np.resize(heatmap, (1,out_len))\n",
    "        heatmap = np.resize(heatmap, (out_len, 1))\n",
    "        heatmaps.append(heatmap)\n",
    "        \n",
    "    return heatmaps\n",
    "p_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# results_directory = os.path.join(rootdir, 'results_'+date+'_V5_ABN_primitive_leakyRELU_dropout_ver4')\n",
    "\n",
    "# gamma = 0.0001 # 0.001\n",
    "\n",
    "\n",
    "# model = ABN_model((None,12), len(unique_classes), minimum_len, n=1)\n",
    "# model = primitive_ABN((None,12), len(unique_classes), minimum_len, n=1)\n",
    "# model = edit_ABN_model((None,12), len(unique_classes), minimum_len,n=1)\n",
    "# model, edit_loss = edit_ABN_model_loss((None,12), len(unique_classes), minimum_len,n=1, gamma = gamma)\n",
    "\n",
    "\n",
    "\n",
    "# model.compile(loss=edit_loss,\n",
    "#               optimizer=optimizers.Adam(lr=1e-5),           \n",
    "#               metrics=[score_f1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 중단된 training 이어돌리기위해 임시로 사용\n",
    "# results_directory = results_directory.replace(\"0608\", \"0604\") # 날짜 달라졌을때\n",
    "# latest = tf.train.latest_checkpoint(results_directory)\n",
    "# latest\n",
    "# model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-24 13:57:29.421946\n",
      "../results_20200724_e2e_ABN_zero_gamma0.01\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, 12)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, None, 64)     2368        input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, None, 64)     256         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, None, 64)     12352       batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, None, 64)     256         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, None, 64)     0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, None, 128)    24704       max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, None, 128)    512         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, None, 128)    49280       batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, None, 128)    512         conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, None, 128)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, None, 256)    98560       max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, None, 256)    1024        conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, None, 256)    196864      batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, None, 256)    1024        conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, None, 256)    196864      batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, None, 256)    1024        conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, None, 256)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, None, 512)    393728      max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, None, 512)    2048        conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, None, 512)    786944      batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, None, 512)    2048        conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, None, 512)    786944      batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, None, 512)    2048        conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, None, 512)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, None, 512)    786944      max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, None, 512)    2048        conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, None, 256)    393472      batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, None, 256)    1024        conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, None, 128)    98432       batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, None, 128)    512         conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, None, 128)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, None, 64)     8192        max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, None, 64)     256         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, None, 64)     0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, None, 64)     12288       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, None, 64)     256         conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, 64)     0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, None, 256)    16384       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, None, 256)    32768       max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, None, 256)    1024        conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, None, 256)    1024        conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, None, 256)    0           batch_normalization_37[0][0]     \n",
      "                                                                 batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, 256)    0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_bn_1 (BatchNor (None, None, 256)    1024        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_conv_1 (Conv1D (None, None, 9)      2304        attention_branch_bn_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_conv_1 (Co (None, None, 1)      9           attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_bn_1 (Batc (None, None, 1)      4           attention_branch_att_conv_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_sigmoid_1  (None, None, 1)      0           attention_branch_att_bn_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 128)    0           max_pooling1d_9[0][0]            \n",
      "                                                                 attention_branch_att_sigmoid_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, None, 64)     8192        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, None, 64)     256         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, 64)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, None, 64)     12288       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, None, 64)     256         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, 64)     0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, None, 256)    16384       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, None, 256)    32768       lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, None, 256)    1024        conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, None, 256)    1024        conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, None, 256)    0           batch_normalization_41[0][0]     \n",
      "                                                                 batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, None, 256)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_avgpool_1 (Gl (None, 256)          0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_1 (Dens (None, 512)          131584      perception_branch_avgpool_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_gap_1 (GlobalA (None, 9)            0           attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_2 (Dens (None, 9)            4617        perception_branch_dense_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_output (Activa (None, 9)            0           attention_branch_gap_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_output (Activ (None, 9)            0           perception_branch_dense_2[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 4,125,718\n",
      "Trainable params: 4,115,476\n",
      "Non-trainable params: 10,242\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 train_loss: 0.602 train_f1: 0.118 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1511: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 0 valid_f1: 0.033 best_f1: 0.033 mean accuracy:0.007 \t\n",
      "\n",
      "Epoch 1 train_loss: 0.536 train_f1: 0.151 \t\n",
      "\n",
      "Validation 1 valid_f1: 0.222 best_f1: 0.222 mean accuracy:0.105 \t\n",
      "\n",
      "Epoch 2 train_loss: 0.514 train_f1: 0.225 \t\n",
      "\n",
      "Validation 2 valid_f1: 0.267 best_f1: 0.267 mean accuracy:0.130 \t\n",
      "\n",
      "Epoch 3 train_loss: 0.500 train_f1: 0.248 \t\n",
      "\n",
      "Validation 3 valid_f1: 0.287 best_f1: 0.287 mean accuracy:0.149 \t\n",
      "\n",
      "Epoch 4 train_loss: 0.490 train_f1: 0.265 \t\n",
      "\n",
      "Validation 4 valid_f1: 0.301 best_f1: 0.301 mean accuracy:0.159 \t\n",
      "\n",
      "Epoch 5 train_loss: 0.482 train_f1: 0.278 \t\n",
      "\n",
      "Validation 5 valid_f1: 0.354 best_f1: 0.354 mean accuracy:0.203 \t\n",
      "\n",
      "Epoch 6 train_loss: 0.475 train_f1: 0.296 \t\n",
      "\n",
      "Validation 6 valid_f1: 0.366 best_f1: 0.366 mean accuracy:0.203 \t\n",
      "\n",
      "Epoch 7 train_loss: 0.469 train_f1: 0.310 \t\n",
      "\n",
      "Validation 7 valid_f1: 0.368 best_f1: 0.368 mean accuracy:0.210 \t\n",
      "\n",
      "Epoch 8 train_loss: 0.463 train_f1: 0.325 \t\n",
      "\n",
      "Validation 8 valid_f1: 0.423 best_f1: 0.423 mean accuracy:0.246 \t\n",
      "\n",
      "Epoch 9 train_loss: 0.459 train_f1: 0.340 \t\n",
      "\n",
      "Validation 9 valid_f1: 0.427 best_f1: 0.427 mean accuracy:0.250 \t\n",
      "\n",
      "Epoch 10 train_loss: 0.454 train_f1: 0.351 \t\n",
      "\n",
      "Validation 10 valid_f1: 0.474 best_f1: 0.474 mean accuracy:0.283 \t\n",
      "\n",
      "Epoch 11 train_loss: 0.450 train_f1: 0.365 \t\n",
      "\n",
      "Validation 11 valid_f1: 0.462 best_f1: 0.474 mean accuracy:0.290 \t\n",
      "\n",
      "Epoch 12 train_loss: 0.446 train_f1: 0.375 \t\n",
      "\n",
      "Validation 12 valid_f1: 0.505 best_f1: 0.505 mean accuracy:0.312 \t\n",
      "\n",
      "Epoch 13 train_loss: 0.443 train_f1: 0.382 \t\n",
      "\n",
      "Validation 13 valid_f1: 0.525 best_f1: 0.525 mean accuracy:0.333 \t\n",
      "\n",
      "Epoch 14 train_loss: 0.439 train_f1: 0.394 \t\n",
      "\n",
      "Validation 14 valid_f1: 0.553 best_f1: 0.553 mean accuracy:0.366 \t\n",
      "\n",
      "Epoch 15 train_loss: 0.435 train_f1: 0.406 \t\n",
      "\n",
      "Validation 15 valid_f1: 0.547 best_f1: 0.553 mean accuracy:0.355 \t\n",
      "\n",
      "Epoch 16 train_loss: 0.433 train_f1: 0.417 \t\n",
      "\n",
      "Validation 16 valid_f1: 0.517 best_f1: 0.553 mean accuracy:0.348 \t\n",
      "\n",
      "Epoch 17 train_loss: 0.430 train_f1: 0.426 \t\n",
      "\n",
      "Validation 17 valid_f1: 0.566 best_f1: 0.566 mean accuracy:0.384 \t\n",
      "\n",
      "Epoch 18 train_loss: 0.428 train_f1: 0.437 \t\n",
      "\n",
      "Validation 18 valid_f1: 0.584 best_f1: 0.584 mean accuracy:0.406 \t\n",
      "\n",
      "Epoch 19 train_loss: 0.425 train_f1: 0.447 \t\n",
      "\n",
      "Validation 19 valid_f1: 0.612 best_f1: 0.612 mean accuracy:0.449 \t\n",
      "\n",
      "Epoch 20 train_loss: 0.422 train_f1: 0.457 \t\n",
      "\n",
      "Validation 20 valid_f1: 0.639 best_f1: 0.639 mean accuracy:0.464 \t\n",
      "\n",
      "Epoch 21 train_loss: 0.420 train_f1: 0.464 \t\n",
      "\n",
      "Validation 21 valid_f1: 0.623 best_f1: 0.639 mean accuracy:0.453 \t\n",
      "\n",
      "Epoch 22 train_loss: 0.417 train_f1: 0.469 \t\n",
      "\n",
      "Validation 22 valid_f1: 0.605 best_f1: 0.639 mean accuracy:0.446 \t\n",
      "\n",
      "Epoch 23 train_loss: 0.416 train_f1: 0.477 \t\n",
      "\n",
      "Validation 23 valid_f1: 0.641 best_f1: 0.641 mean accuracy:0.482 \t\n",
      "\n",
      "Epoch 24 train_loss: 0.414 train_f1: 0.483 \t\n",
      "\n",
      "Validation 24 valid_f1: 0.646 best_f1: 0.646 mean accuracy:0.486 \t\n",
      "\n",
      "Epoch 25 train_loss: 0.412 train_f1: 0.485 \t\n",
      "\n",
      "Validation 25 valid_f1: 0.658 best_f1: 0.658 mean accuracy:0.496 \t\n",
      "\n",
      "Epoch 26 train_loss: 0.411 train_f1: 0.490 \t\n",
      "\n",
      "Validation 26 valid_f1: 0.660 best_f1: 0.660 mean accuracy:0.493 \t\n",
      "\n",
      "Epoch 27 train_loss: 0.408 train_f1: 0.499 \t\n",
      "\n",
      "Validation 27 valid_f1: 0.679 best_f1: 0.679 mean accuracy:0.529 \t\n",
      "\n",
      "Epoch 28 train_loss: 0.407 train_f1: 0.500 \t\n",
      "\n",
      "Validation 28 valid_f1: 0.685 best_f1: 0.685 mean accuracy:0.536 \t\n",
      "\n",
      "Epoch 29 train_loss: 0.405 train_f1: 0.506 \t\n",
      "\n",
      "Validation 29 valid_f1: 0.680 best_f1: 0.685 mean accuracy:0.547 \t\n",
      "\n",
      "Epoch 30 train_loss: 0.404 train_f1: 0.507 \t\n",
      "\n",
      "Validation 30 valid_f1: 0.683 best_f1: 0.685 mean accuracy:0.533 \t\n",
      "\n",
      "Epoch 31 train_loss: 0.402 train_f1: 0.511 \t\n",
      "\n",
      "Validation 31 valid_f1: 0.687 best_f1: 0.687 mean accuracy:0.551 \t\n",
      "\n",
      "Epoch 32 train_loss: 0.401 train_f1: 0.518 \t\n",
      "\n",
      "Validation 32 valid_f1: 0.702 best_f1: 0.702 mean accuracy:0.554 \t\n",
      "\n",
      "Epoch 33 train_loss: 0.399 train_f1: 0.520 \t\n",
      "\n",
      "Validation 33 valid_f1: 0.702 best_f1: 0.702 mean accuracy:0.554 \t\n",
      "\n",
      "Epoch 34 train_loss: 0.398 train_f1: 0.526 \t\n",
      "\n",
      "Validation 34 valid_f1: 0.712 best_f1: 0.712 mean accuracy:0.576 \t\n",
      "\n",
      "Epoch 35 train_loss: 0.396 train_f1: 0.527 \t\n",
      "\n",
      "Validation 35 valid_f1: 0.715 best_f1: 0.715 mean accuracy:0.587 \t\n",
      "\n",
      "Epoch 36 train_loss: 0.395 train_f1: 0.530 \t\n",
      "\n",
      "Validation 36 valid_f1: 0.721 best_f1: 0.721 mean accuracy:0.580 \t\n",
      "\n",
      "Epoch 37 train_loss: 0.394 train_f1: 0.535 \t\n",
      "\n",
      "Validation 37 valid_f1: 0.707 best_f1: 0.721 mean accuracy:0.583 \t\n",
      "\n",
      "Epoch 38 train_loss: 0.393 train_f1: 0.537 \t\n",
      "\n",
      "Validation 38 valid_f1: 0.723 best_f1: 0.723 mean accuracy:0.594 \t\n",
      "\n",
      "Epoch 39 train_loss: 0.392 train_f1: 0.539 \t\n",
      "\n",
      "Validation 39 valid_f1: 0.734 best_f1: 0.734 mean accuracy:0.601 \t\n",
      "\n",
      "Epoch 40 train_loss: 0.391 train_f1: 0.543 \t\n",
      "\n",
      "Validation 40 valid_f1: 0.714 best_f1: 0.734 mean accuracy:0.587 \t\n",
      "\n",
      "Epoch 41 train_loss: 0.389 train_f1: 0.545 \t\n",
      "\n",
      "Validation 41 valid_f1: 0.688 best_f1: 0.734 mean accuracy:0.529 \t\n",
      "\n",
      "Epoch 42 train_loss: 0.388 train_f1: 0.550 \t\n",
      "\n",
      "Validation 42 valid_f1: 0.722 best_f1: 0.734 mean accuracy:0.594 \t\n",
      "\n",
      "Epoch 43 train_loss: 0.386 train_f1: 0.551 \t\n",
      "\n",
      "Validation 43 valid_f1: 0.728 best_f1: 0.734 mean accuracy:0.609 \t\n",
      "\n",
      "Epoch 44 train_loss: 0.386 train_f1: 0.558 \t\n",
      "\n",
      "Validation 44 valid_f1: 0.738 best_f1: 0.738 mean accuracy:0.616 \t\n",
      "\n",
      "Epoch 45 train_loss: 0.384 train_f1: 0.559 \t\n",
      "\n",
      "Validation 45 valid_f1: 0.718 best_f1: 0.738 mean accuracy:0.583 \t\n",
      "\n",
      "Epoch 46 train_loss: 0.383 train_f1: 0.562 \t\n",
      "\n",
      "Validation 46 valid_f1: 0.713 best_f1: 0.738 mean accuracy:0.594 \t\n",
      "\n",
      "Epoch 47 train_loss: 0.382 train_f1: 0.567 \t\n",
      "\n",
      "Validation 47 valid_f1: 0.723 best_f1: 0.738 mean accuracy:0.601 \t\n",
      "\n",
      "Epoch 48 train_loss: 0.382 train_f1: 0.564 \t\n",
      "\n",
      "Validation 48 valid_f1: 0.722 best_f1: 0.738 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 49 train_loss: 0.379 train_f1: 0.573 \t\n",
      "\n",
      "Validation 49 valid_f1: 0.738 best_f1: 0.738 mean accuracy:0.605 \t\n",
      "\n",
      "Epoch 50 train_loss: 0.379 train_f1: 0.575 \t\n",
      "\n",
      "Validation 50 valid_f1: 0.735 best_f1: 0.738 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 51 train_loss: 0.379 train_f1: 0.571 \t\n",
      "\n",
      "Validation 51 valid_f1: 0.743 best_f1: 0.743 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 52 train_loss: 0.377 train_f1: 0.578 \t\n",
      "\n",
      "Validation 52 valid_f1: 0.749 best_f1: 0.749 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 53 train_loss: 0.376 train_f1: 0.579 \t\n",
      "\n",
      "Validation 53 valid_f1: 0.740 best_f1: 0.749 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 54 train_loss: 0.375 train_f1: 0.582 \t\n",
      "\n",
      "Validation 54 valid_f1: 0.751 best_f1: 0.751 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 55 train_loss: 0.373 train_f1: 0.588 \t\n",
      "\n",
      "Validation 55 valid_f1: 0.724 best_f1: 0.751 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 56 train_loss: 0.372 train_f1: 0.588 \t\n",
      "\n",
      "Validation 56 valid_f1: 0.758 best_f1: 0.758 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 57 train_loss: 0.371 train_f1: 0.592 \t\n",
      "\n",
      "Validation 57 valid_f1: 0.726 best_f1: 0.758 mean accuracy:0.609 \t\n",
      "\n",
      "Epoch 58 train_loss: 0.370 train_f1: 0.597 \t\n",
      "\n",
      "Validation 58 valid_f1: 0.751 best_f1: 0.758 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 59 train_loss: 0.370 train_f1: 0.594 \t\n",
      "\n",
      "Validation 59 valid_f1: 0.730 best_f1: 0.758 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 60 train_loss: 0.368 train_f1: 0.596 \t\n",
      "\n",
      "Validation 60 valid_f1: 0.754 best_f1: 0.758 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 61 train_loss: 0.367 train_f1: 0.603 \t\n",
      "\n",
      "Validation 61 valid_f1: 0.724 best_f1: 0.758 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 62 train_loss: 0.367 train_f1: 0.603 \t\n",
      "\n",
      "Validation 62 valid_f1: 0.761 best_f1: 0.761 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 63 train_loss: 0.367 train_f1: 0.606 \t\n",
      "\n",
      "Validation 63 valid_f1: 0.731 best_f1: 0.761 mean accuracy:0.609 \t\n",
      "\n",
      "Epoch 64 train_loss: 0.366 train_f1: 0.607 \t\n",
      "\n",
      "Validation 64 valid_f1: 0.740 best_f1: 0.761 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 65 train_loss: 0.364 train_f1: 0.610 \t\n",
      "\n",
      "Validation 65 valid_f1: 0.754 best_f1: 0.761 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 66 train_loss: 0.363 train_f1: 0.611 \t\n",
      "\n",
      "Validation 66 valid_f1: 0.771 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 67 train_loss: 0.361 train_f1: 0.618 \t\n",
      "\n",
      "Validation 67 valid_f1: 0.707 best_f1: 0.771 mean accuracy:0.572 \t\n",
      "\n",
      "Epoch 68 train_loss: 0.363 train_f1: 0.614 \t\n",
      "\n",
      "Validation 68 valid_f1: 0.756 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 69 train_loss: 0.361 train_f1: 0.621 \t\n",
      "\n",
      "Validation 69 valid_f1: 0.757 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 70 train_loss: 0.360 train_f1: 0.620 \t\n",
      "\n",
      "Validation 70 valid_f1: 0.730 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 71 train_loss: 0.360 train_f1: 0.623 \t\n",
      "\n",
      "Validation 71 valid_f1: 0.769 best_f1: 0.771 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 72 train_loss: 0.358 train_f1: 0.627 \t\n",
      "\n",
      "Validation 72 valid_f1: 0.761 best_f1: 0.771 mean accuracy:0.663 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73 train_loss: 0.357 train_f1: 0.633 \t\n",
      "\n",
      "Validation 73 valid_f1: 0.717 best_f1: 0.771 mean accuracy:0.587 \t\n",
      "\n",
      "Epoch 74 train_loss: 0.357 train_f1: 0.630 \t\n",
      "\n",
      "Validation 74 valid_f1: 0.750 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 75 train_loss: 0.355 train_f1: 0.636 \t\n",
      "\n",
      "Validation 75 valid_f1: 0.764 best_f1: 0.771 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 76 train_loss: 0.354 train_f1: 0.638 \t\n",
      "\n",
      "Validation 76 valid_f1: 0.759 best_f1: 0.771 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 77 train_loss: 0.355 train_f1: 0.639 \t\n",
      "\n",
      "Validation 77 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 78 train_loss: 0.354 train_f1: 0.641 \t\n",
      "\n",
      "Validation 78 valid_f1: 0.735 best_f1: 0.771 mean accuracy:0.601 \t\n",
      "\n",
      "Epoch 79 train_loss: 0.353 train_f1: 0.643 \t\n",
      "\n",
      "Validation 79 valid_f1: 0.737 best_f1: 0.771 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 80 train_loss: 0.352 train_f1: 0.644 \t\n",
      "\n",
      "Validation 80 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 81 train_loss: 0.351 train_f1: 0.649 \t\n",
      "\n",
      "Validation 81 valid_f1: 0.727 best_f1: 0.771 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 82 train_loss: 0.350 train_f1: 0.651 \t\n",
      "\n",
      "Validation 82 valid_f1: 0.731 best_f1: 0.771 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 83 train_loss: 0.350 train_f1: 0.655 \t\n",
      "\n",
      "Validation 83 valid_f1: 0.712 best_f1: 0.771 mean accuracy:0.583 \t\n",
      "\n",
      "Epoch 84 train_loss: 0.350 train_f1: 0.653 \t\n",
      "\n",
      "Validation 84 valid_f1: 0.748 best_f1: 0.771 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 85 train_loss: 0.348 train_f1: 0.657 \t\n",
      "\n",
      "Validation 85 valid_f1: 0.762 best_f1: 0.771 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 86 train_loss: 0.347 train_f1: 0.660 \t\n",
      "\n",
      "Validation 86 valid_f1: 0.781 best_f1: 0.781 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 87 train_loss: 0.347 train_f1: 0.662 \t\n",
      "\n",
      "Validation 87 valid_f1: 0.744 best_f1: 0.781 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 88 train_loss: 0.347 train_f1: 0.663 \t\n",
      "\n",
      "Validation 88 valid_f1: 0.760 best_f1: 0.781 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 89 train_loss: 0.346 train_f1: 0.664 \t\n",
      "\n",
      "Validation 89 valid_f1: 0.758 best_f1: 0.781 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 90 train_loss: 0.345 train_f1: 0.669 \t\n",
      "\n",
      "Validation 90 valid_f1: 0.754 best_f1: 0.781 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 91 train_loss: 0.345 train_f1: 0.670 \t\n",
      "\n",
      "Validation 91 valid_f1: 0.739 best_f1: 0.781 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 92 train_loss: 0.344 train_f1: 0.670 \t\n",
      "\n",
      "Validation 92 valid_f1: 0.753 best_f1: 0.781 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 93 train_loss: 0.343 train_f1: 0.671 \t\n",
      "\n",
      "Validation 93 valid_f1: 0.700 best_f1: 0.781 mean accuracy:0.601 \t\n",
      "\n",
      "Epoch 94 train_loss: 0.343 train_f1: 0.675 \t\n",
      "\n",
      "Validation 94 valid_f1: 0.746 best_f1: 0.781 mean accuracy:0.616 \t\n",
      "\n",
      "Epoch 95 train_loss: 0.343 train_f1: 0.671 \t\n",
      "\n",
      "Validation 95 valid_f1: 0.762 best_f1: 0.781 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 96 train_loss: 0.344 train_f1: 0.670 \t\n",
      "\n",
      "Validation 96 valid_f1: 0.732 best_f1: 0.781 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 97 train_loss: 0.341 train_f1: 0.681 \t\n",
      "\n",
      "Validation 97 valid_f1: 0.781 best_f1: 0.781 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 98 train_loss: 0.340 train_f1: 0.684 \t\n",
      "\n",
      "Validation 98 valid_f1: 0.682 best_f1: 0.781 mean accuracy:0.558 \t\n",
      "\n",
      "Epoch 99 train_loss: 0.340 train_f1: 0.683 \t\n",
      "\n",
      "Validation 99 valid_f1: 0.726 best_f1: 0.781 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 100 train_loss: 0.341 train_f1: 0.682 \t\n",
      "\n",
      "Validation 100 valid_f1: 0.745 best_f1: 0.781 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 101 train_loss: 0.340 train_f1: 0.684 \t\n",
      "\n",
      "Validation 101 valid_f1: 0.775 best_f1: 0.781 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 102 train_loss: 0.339 train_f1: 0.686 \t\n",
      "\n",
      "Validation 102 valid_f1: 0.764 best_f1: 0.781 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 103 train_loss: 0.339 train_f1: 0.686 \t\n",
      "\n",
      "Validation 103 valid_f1: 0.786 best_f1: 0.786 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 104 train_loss: 0.338 train_f1: 0.685 \t\n",
      "\n",
      "Validation 104 valid_f1: 0.771 best_f1: 0.786 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 105 train_loss: 0.338 train_f1: 0.691 \t\n",
      "\n",
      "Validation 105 valid_f1: 0.757 best_f1: 0.786 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 106 train_loss: 0.338 train_f1: 0.685 \t\n",
      "\n",
      "Validation 106 valid_f1: 0.760 best_f1: 0.786 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 107 train_loss: 0.338 train_f1: 0.689 \t\n",
      "\n",
      "Validation 107 valid_f1: 0.734 best_f1: 0.786 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 108 train_loss: 0.337 train_f1: 0.694 \t\n",
      "\n",
      "Validation 108 valid_f1: 0.765 best_f1: 0.786 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 109 train_loss: 0.336 train_f1: 0.696 \t\n",
      "\n",
      "Validation 109 valid_f1: 0.732 best_f1: 0.786 mean accuracy:0.616 \t\n",
      "\n",
      "Epoch 110 train_loss: 0.337 train_f1: 0.692 \t\n",
      "\n",
      "Validation 110 valid_f1: 0.768 best_f1: 0.786 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 111 train_loss: 0.336 train_f1: 0.698 \t\n",
      "\n",
      "Validation 111 valid_f1: 0.762 best_f1: 0.786 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 112 train_loss: 0.335 train_f1: 0.699 \t\n",
      "\n",
      "Validation 112 valid_f1: 0.768 best_f1: 0.786 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 113 train_loss: 0.335 train_f1: 0.700 \t\n",
      "\n",
      "Validation 113 valid_f1: 0.769 best_f1: 0.786 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 114 train_loss: 0.334 train_f1: 0.702 \t\n",
      "\n",
      "Validation 114 valid_f1: 0.785 best_f1: 0.786 mean accuracy:0.707 \t\n",
      "\n",
      "Epoch 115 train_loss: 0.334 train_f1: 0.702 \t\n",
      "\n",
      "Validation 115 valid_f1: 0.774 best_f1: 0.786 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 116 train_loss: 0.333 train_f1: 0.704 \t\n",
      "\n",
      "Validation 116 valid_f1: 0.725 best_f1: 0.786 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 117 train_loss: 0.333 train_f1: 0.706 \t\n",
      "\n",
      "Validation 117 valid_f1: 0.775 best_f1: 0.786 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 118 train_loss: 0.333 train_f1: 0.705 \t\n",
      "\n",
      "Validation 118 valid_f1: 0.728 best_f1: 0.786 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 119 train_loss: 0.333 train_f1: 0.705 \t\n",
      "\n",
      "Validation 119 valid_f1: 0.783 best_f1: 0.786 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 120 train_loss: 0.331 train_f1: 0.713 \t\n",
      "\n",
      "Validation 120 valid_f1: 0.783 best_f1: 0.786 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 121 train_loss: 0.332 train_f1: 0.709 \t\n",
      "\n",
      "Validation 121 valid_f1: 0.755 best_f1: 0.786 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 122 train_loss: 0.332 train_f1: 0.712 \t\n",
      "\n",
      "Validation 122 valid_f1: 0.762 best_f1: 0.786 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 123 train_loss: 0.332 train_f1: 0.711 \t\n",
      "\n",
      "Validation 123 valid_f1: 0.743 best_f1: 0.786 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 124 train_loss: 0.333 train_f1: 0.708 \t\n",
      "\n",
      "Validation 124 valid_f1: 0.783 best_f1: 0.786 mean accuracy:0.696 \t\n",
      "\n",
      "Epoch 125 train_loss: 0.332 train_f1: 0.714 \t\n",
      "\n",
      "Validation 125 valid_f1: 0.794 best_f1: 0.794 mean accuracy:0.707 \t\n",
      "\n",
      "Epoch 126 train_loss: 0.331 train_f1: 0.715 \t\n",
      "\n",
      "Validation 126 valid_f1: 0.789 best_f1: 0.794 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 127 train_loss: 0.331 train_f1: 0.713 \t\n",
      "\n",
      "Validation 127 valid_f1: 0.769 best_f1: 0.794 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 128 train_loss: 0.329 train_f1: 0.721 \t\n",
      "\n",
      "Validation 128 valid_f1: 0.751 best_f1: 0.794 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 129 train_loss: 0.329 train_f1: 0.722 \t\n",
      "\n",
      "Validation 129 valid_f1: 0.759 best_f1: 0.794 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 130 train_loss: 0.330 train_f1: 0.717 \t\n",
      "\n",
      "Validation 130 valid_f1: 0.772 best_f1: 0.794 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 131 train_loss: 0.330 train_f1: 0.719 \t\n",
      "\n",
      "Validation 131 valid_f1: 0.766 best_f1: 0.794 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 132 train_loss: 0.330 train_f1: 0.719 \t\n",
      "\n",
      "Validation 132 valid_f1: 0.775 best_f1: 0.794 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 133 train_loss: 0.328 train_f1: 0.725 \t\n",
      "\n",
      "Validation 133 valid_f1: 0.763 best_f1: 0.794 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 134 train_loss: 0.329 train_f1: 0.724 \t\n",
      "\n",
      "Validation 134 valid_f1: 0.751 best_f1: 0.794 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 135 train_loss: 0.329 train_f1: 0.724 \t\n",
      "\n",
      "Validation 135 valid_f1: 0.789 best_f1: 0.794 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 136 train_loss: 0.327 train_f1: 0.733 \t\n",
      "\n",
      "Validation 136 valid_f1: 0.753 best_f1: 0.794 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 137 train_loss: 0.327 train_f1: 0.732 \t\n",
      "\n",
      "Validation 137 valid_f1: 0.745 best_f1: 0.794 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 138 train_loss: 0.328 train_f1: 0.729 \t\n",
      "\n",
      "Validation 138 valid_f1: 0.787 best_f1: 0.794 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 139 train_loss: 0.327 train_f1: 0.729 \t\n",
      "\n",
      "Validation 139 valid_f1: 0.750 best_f1: 0.794 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 140 train_loss: 0.326 train_f1: 0.734 \t\n",
      "\n",
      "Validation 140 valid_f1: 0.776 best_f1: 0.794 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 141 train_loss: 0.327 train_f1: 0.731 \t\n",
      "\n",
      "Validation 141 valid_f1: 0.775 best_f1: 0.794 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 142 train_loss: 0.327 train_f1: 0.735 \t\n",
      "\n",
      "Validation 142 valid_f1: 0.777 best_f1: 0.794 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 143 train_loss: 0.327 train_f1: 0.733 \t\n",
      "\n",
      "Validation 143 valid_f1: 0.775 best_f1: 0.794 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 144 train_loss: 0.326 train_f1: 0.735 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 144 valid_f1: 0.774 best_f1: 0.794 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 145 train_loss: 0.327 train_f1: 0.730 \t\n",
      "\n",
      "Validation 145 valid_f1: 0.754 best_f1: 0.794 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 146 train_loss: 0.325 train_f1: 0.739 \t\n",
      "\n",
      "Validation 146 valid_f1: 0.767 best_f1: 0.794 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 147 train_loss: 0.326 train_f1: 0.735 \t\n",
      "\n",
      "Validation 147 valid_f1: 0.754 best_f1: 0.794 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 148 train_loss: 0.325 train_f1: 0.742 \t\n",
      "\n",
      "Validation 148 valid_f1: 0.776 best_f1: 0.794 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 149 train_loss: 0.325 train_f1: 0.742 \t\n",
      "\n",
      "Validation 149 valid_f1: 0.785 best_f1: 0.794 mean accuracy:0.696 \t\n",
      "\n",
      "Epoch 150 train_loss: 0.325 train_f1: 0.739 \t\n",
      "\n",
      "Validation 150 valid_f1: 0.796 best_f1: 0.796 mean accuracy:0.707 \t\n",
      "\n",
      "Epoch 151 train_loss: 0.325 train_f1: 0.741 \t\n",
      "\n",
      "Validation 151 valid_f1: 0.778 best_f1: 0.796 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 152 train_loss: 0.327 train_f1: 0.737 \t\n",
      "\n",
      "Validation 152 valid_f1: 0.770 best_f1: 0.796 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 153 train_loss: 0.327 train_f1: 0.740 \t\n",
      "\n",
      "Validation 153 valid_f1: 0.767 best_f1: 0.796 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 154 train_loss: 0.326 train_f1: 0.745 \t\n",
      "\n",
      "Validation 154 valid_f1: 0.765 best_f1: 0.796 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 155 train_loss: 0.326 train_f1: 0.738 \t\n",
      "\n",
      "Validation 155 valid_f1: 0.789 best_f1: 0.796 mean accuracy:0.703 \t\n",
      "\n",
      "Epoch 156 train_loss: 0.324 train_f1: 0.748 \t\n",
      "\n",
      "Validation 156 valid_f1: 0.783 best_f1: 0.796 mean accuracy:0.696 \t\n",
      "\n",
      "Epoch 157 train_loss: 0.323 train_f1: 0.747 \t\n",
      "\n",
      "Validation 157 valid_f1: 0.769 best_f1: 0.796 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 158 train_loss: 0.323 train_f1: 0.748 \t\n",
      "\n",
      "Validation 158 valid_f1: 0.772 best_f1: 0.796 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 159 train_loss: 0.324 train_f1: 0.748 \t\n",
      "\n",
      "Validation 159 valid_f1: 0.764 best_f1: 0.796 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 160 train_loss: 0.325 train_f1: 0.743 \t\n",
      "\n",
      "Validation 160 valid_f1: 0.761 best_f1: 0.796 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 161 train_loss: 0.324 train_f1: 0.749 \t\n",
      "\n",
      "Validation 161 valid_f1: 0.799 best_f1: 0.799 mean accuracy:0.707 \t\n",
      "\n",
      "Epoch 162 train_loss: 0.324 train_f1: 0.753 \t\n",
      "\n",
      "Validation 162 valid_f1: 0.770 best_f1: 0.799 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 163 train_loss: 0.327 train_f1: 0.741 \t\n",
      "\n",
      "Validation 163 valid_f1: 0.785 best_f1: 0.799 mean accuracy:0.696 \t\n",
      "\n",
      "Epoch 164 train_loss: 0.323 train_f1: 0.754 \t\n",
      "\n",
      "Validation 164 valid_f1: 0.769 best_f1: 0.799 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 165 train_loss: 0.323 train_f1: 0.754 \t\n",
      "\n",
      "Validation 165 valid_f1: 0.776 best_f1: 0.799 mean accuracy:0.696 \t\n",
      "\n",
      "Epoch 166 train_loss: 0.322 train_f1: 0.755 \t\n",
      "\n",
      "Validation 166 valid_f1: 0.778 best_f1: 0.799 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 167 train_loss: 0.323 train_f1: 0.756 \t\n",
      "\n",
      "Validation 167 valid_f1: 0.764 best_f1: 0.799 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 168 train_loss: 0.323 train_f1: 0.756 \t\n",
      "\n",
      "Validation 168 valid_f1: 0.769 best_f1: 0.799 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 169 train_loss: 0.322 train_f1: 0.753 \t\n",
      "\n",
      "Validation 169 valid_f1: 0.767 best_f1: 0.799 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 170 train_loss: 0.323 train_f1: 0.753 \t\n",
      "\n",
      "Validation 170 valid_f1: 0.785 best_f1: 0.799 mean accuracy:0.696 \t\n",
      "\n",
      "Epoch 171 train_loss: 0.324 train_f1: 0.758 \t\n",
      "\n",
      "Validation 171 valid_f1: 0.790 best_f1: 0.799 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 172 train_loss: 0.322 train_f1: 0.758 \t\n",
      "\n",
      "Validation 172 valid_f1: 0.766 best_f1: 0.799 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 173 train_loss: 0.323 train_f1: 0.759 \t\n",
      "\n",
      "Validation 173 valid_f1: 0.794 best_f1: 0.799 mean accuracy:0.707 \t\n",
      "\n",
      "Epoch 174 train_loss: 0.322 train_f1: 0.761 \t\n",
      "\n",
      "Validation 174 valid_f1: 0.790 best_f1: 0.799 mean accuracy:0.696 \t\n",
      "\n",
      "Epoch 175 train_loss: 0.323 train_f1: 0.759 \t\n",
      "\n",
      "Validation 175 valid_f1: 0.739 best_f1: 0.799 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 176 train_loss: 0.322 train_f1: 0.758 \t\n",
      "\n",
      "Validation 176 valid_f1: 0.788 best_f1: 0.799 mean accuracy:0.707 \t\n",
      "\n",
      "Epoch 177 train_loss: 0.321 train_f1: 0.762 \t\n",
      "\n",
      "Validation 177 valid_f1: 0.761 best_f1: 0.799 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 178 train_loss: 0.322 train_f1: 0.762 \t\n",
      "\n",
      "Validation 178 valid_f1: 0.787 best_f1: 0.799 mean accuracy:0.717 \t\n",
      "\n",
      "Epoch 179 train_loss: 0.321 train_f1: 0.765 \t\n",
      "\n",
      "Validation 179 valid_f1: 0.804 best_f1: 0.804 mean accuracy:0.717 \t\n",
      "\n",
      "Epoch 180 train_loss: 0.321 train_f1: 0.765 \t\n",
      "\n",
      "Validation 180 valid_f1: 0.769 best_f1: 0.804 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 181 train_loss: 0.320 train_f1: 0.767 \t\n",
      "\n",
      "Validation 181 valid_f1: 0.783 best_f1: 0.804 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 182 train_loss: 0.320 train_f1: 0.766 \t\n",
      "\n",
      "Validation 182 valid_f1: 0.775 best_f1: 0.804 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 183 train_loss: 0.320 train_f1: 0.769 \t\n",
      "\n",
      "Validation 183 valid_f1: 0.772 best_f1: 0.804 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 184 train_loss: 0.321 train_f1: 0.764 \t\n",
      "\n",
      "Validation 184 valid_f1: 0.786 best_f1: 0.804 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 185 train_loss: 0.320 train_f1: 0.768 \t\n",
      "\n",
      "Validation 185 valid_f1: 0.709 best_f1: 0.804 mean accuracy:0.587 \t\n",
      "\n",
      "Epoch 186 train_loss: 0.321 train_f1: 0.767 \t\n",
      "\n",
      "Validation 186 valid_f1: 0.741 best_f1: 0.804 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 187 train_loss: 0.320 train_f1: 0.770 \t\n",
      "\n",
      "Validation 187 valid_f1: 0.787 best_f1: 0.804 mean accuracy:0.699 \t\n",
      "\n",
      "Epoch 188 train_loss: 0.320 train_f1: 0.769 \t\n",
      "\n",
      "Validation 188 valid_f1: 0.778 best_f1: 0.804 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 189 train_loss: 0.319 train_f1: 0.775 \t\n",
      "\n",
      "Validation 189 valid_f1: 0.787 best_f1: 0.804 mean accuracy:0.703 \t\n",
      "\n",
      "Epoch 190 train_loss: 0.321 train_f1: 0.766 \t\n",
      "\n",
      "Validation 190 valid_f1: 0.788 best_f1: 0.804 mean accuracy:0.696 \t\n",
      "\n",
      "Epoch 191 train_loss: 0.321 train_f1: 0.772 \t\n",
      "\n",
      "Validation 191 valid_f1: 0.790 best_f1: 0.804 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 192 train_loss: 0.319 train_f1: 0.775 \t\n",
      "\n",
      "Validation 192 valid_f1: 0.783 best_f1: 0.804 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 193 train_loss: 0.320 train_f1: 0.776 \t\n",
      "\n",
      "Validation 193 valid_f1: 0.763 best_f1: 0.804 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 194 train_loss: 0.322 train_f1: 0.767 \t\n",
      "\n",
      "Validation 194 valid_f1: 0.737 best_f1: 0.804 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 195 train_loss: 0.320 train_f1: 0.776 \t\n",
      "\n",
      "Validation 195 valid_f1: 0.794 best_f1: 0.804 mean accuracy:0.703 \t\n",
      "\n",
      "Epoch 196 train_loss: 0.320 train_f1: 0.774 \t\n",
      "\n",
      "Validation 196 valid_f1: 0.767 best_f1: 0.804 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 197 train_loss: 0.321 train_f1: 0.773 \t\n",
      "\n",
      "Validation 197 valid_f1: 0.786 best_f1: 0.804 mean accuracy:0.699 \t\n",
      "\n",
      "Epoch 198 train_loss: 0.318 train_f1: 0.781 \t\n",
      "\n",
      "Validation 198 valid_f1: 0.779 best_f1: 0.804 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 199 train_loss: 0.319 train_f1: 0.778 \t\n",
      "\n",
      "Validation 199 valid_f1: 0.794 best_f1: 0.804 mean accuracy:0.710 \t\n",
      "\n",
      "Epoch 200 train_loss: 0.319 train_f1: 0.777 \t\n",
      "\n",
      "Validation 200 valid_f1: 0.796 best_f1: 0.804 mean accuracy:0.703 \t\n",
      "\n",
      "Epoch 201 train_loss: 0.321 train_f1: 0.774 \t\n",
      "\n",
      "Validation 201 valid_f1: 0.780 best_f1: 0.804 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 202 train_loss: 0.323 train_f1: 0.768 \t\n",
      "\n",
      "Validation 202 valid_f1: 0.787 best_f1: 0.804 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 203 train_loss: 0.319 train_f1: 0.786 \t\n",
      "\n",
      "Validation 203 valid_f1: 0.794 best_f1: 0.804 mean accuracy:0.707 \t\n",
      "\n",
      "Epoch 204 train_loss: 0.319 train_f1: 0.780 \t\n",
      "\n",
      "Validation 204 valid_f1: 0.790 best_f1: 0.804 mean accuracy:0.696 \t\n",
      "\n",
      "Epoch 205 train_loss: 0.320 train_f1: 0.782 \t\n",
      "\n",
      "Validation 205 valid_f1: 0.773 best_f1: 0.804 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 206 train_loss: 0.319 train_f1: 0.783 \t\n",
      "\n",
      "Validation 206 valid_f1: 0.785 best_f1: 0.804 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 207 train_loss: 0.319 train_f1: 0.787 \t\n",
      "\n",
      "Validation 207 valid_f1: 0.783 best_f1: 0.804 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 208 train_loss: 0.318 train_f1: 0.787 \t\n",
      "\n",
      "Validation 208 valid_f1: 0.783 best_f1: 0.804 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 209 train_loss: 0.318 train_f1: 0.785 \t\n",
      "\n",
      "Validation 209 valid_f1: 0.796 best_f1: 0.804 mean accuracy:0.699 \t\n",
      "\n",
      "Epoch 210 train_loss: 0.319 train_f1: 0.785 \t\n",
      "\n",
      "Validation 210 valid_f1: 0.809 best_f1: 0.809 mean accuracy:0.728 \t\n",
      "\n",
      "Epoch 211 train_loss: 0.319 train_f1: 0.785 \t\n",
      "\n",
      "Validation 211 valid_f1: 0.784 best_f1: 0.809 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 212 train_loss: 0.319 train_f1: 0.789 \t\n",
      "\n",
      "Validation 212 valid_f1: 0.764 best_f1: 0.809 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 213 train_loss: 0.320 train_f1: 0.785 \t\n",
      "\n",
      "Validation 213 valid_f1: 0.789 best_f1: 0.809 mean accuracy:0.699 \t\n",
      "\n",
      "Epoch 214 train_loss: 0.318 train_f1: 0.792 \t\n",
      "\n",
      "Validation 214 valid_f1: 0.754 best_f1: 0.809 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 215 train_loss: 0.317 train_f1: 0.794 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 215 valid_f1: 0.782 best_f1: 0.809 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 216 train_loss: 0.317 train_f1: 0.794 \t\n",
      "\n",
      "Validation 216 valid_f1: 0.786 best_f1: 0.809 mean accuracy:0.696 \t\n",
      "\n",
      "Epoch 217 train_loss: 0.317 train_f1: 0.792 \t\n",
      "\n",
      "Validation 217 valid_f1: 0.780 best_f1: 0.809 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 218 train_loss: 0.318 train_f1: 0.790 \t\n",
      "\n",
      "Validation 218 valid_f1: 0.767 best_f1: 0.809 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 219 train_loss: 0.318 train_f1: 0.796 \t\n",
      "\n",
      "Validation 219 valid_f1: 0.781 best_f1: 0.809 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 220 train_loss: 0.318 train_f1: 0.795 \t\n",
      "\n",
      "Validation 220 valid_f1: 0.767 best_f1: 0.809 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 221 train_loss: 0.319 train_f1: 0.792 \t\n",
      "\n",
      "Validation 221 valid_f1: 0.765 best_f1: 0.809 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 222 train_loss: 0.318 train_f1: 0.798 \t\n",
      "\n",
      "Validation 222 valid_f1: 0.754 best_f1: 0.809 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 223 train_loss: 0.320 train_f1: 0.792 \t\n",
      "\n",
      "Validation 223 valid_f1: 0.774 best_f1: 0.809 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 224 train_loss: 0.318 train_f1: 0.799 \t\n",
      "\n",
      "Validation 224 valid_f1: 0.777 best_f1: 0.809 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 225 train_loss: 0.318 train_f1: 0.794 \t\n",
      "\n",
      "Validation 225 valid_f1: 0.770 best_f1: 0.809 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 226 train_loss: 0.317 train_f1: 0.795 \t\n",
      "\n",
      "Validation 226 valid_f1: 0.765 best_f1: 0.809 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 227 train_loss: 0.318 train_f1: 0.799 \t\n",
      "\n",
      "Validation 227 valid_f1: 0.785 best_f1: 0.809 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 228 train_loss: 0.316 train_f1: 0.803 \t\n",
      "\n",
      "Validation 228 valid_f1: 0.768 best_f1: 0.809 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 229 train_loss: 0.317 train_f1: 0.802 \t\n",
      "\n",
      "Validation 229 valid_f1: 0.783 best_f1: 0.809 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 230 train_loss: 0.317 train_f1: 0.806 \t\n",
      "\n",
      "Validation 230 valid_f1: 0.809 best_f1: 0.809 mean accuracy:0.732 \t\n",
      "\n",
      "Epoch 231 train_loss: 0.316 train_f1: 0.805 \t\n",
      "\n",
      "Validation 231 valid_f1: 0.777 best_f1: 0.809 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 232 train_loss: 0.317 train_f1: 0.799 \t\n",
      "\n",
      "Validation 232 valid_f1: 0.787 best_f1: 0.809 mean accuracy:0.703 \t\n",
      "\n",
      "Epoch 233 train_loss: 0.316 train_f1: 0.807 \t\n",
      "\n",
      "Validation 233 valid_f1: 0.780 best_f1: 0.809 mean accuracy:0.699 \t\n",
      "\n",
      "Epoch 234 train_loss: 0.316 train_f1: 0.807 \t\n",
      "\n",
      "Validation 234 valid_f1: 0.778 best_f1: 0.809 mean accuracy:0.703 \t\n",
      "\n",
      "Epoch 235 train_loss: 0.316 train_f1: 0.808 \t\n",
      "\n",
      "Validation 235 valid_f1: 0.771 best_f1: 0.809 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 236 train_loss: 0.316 train_f1: 0.808 \t\n",
      "\n",
      "Validation 236 valid_f1: 0.761 best_f1: 0.809 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 237 train_loss: 0.316 train_f1: 0.803 \t\n",
      "\n",
      "Validation 237 valid_f1: 0.783 best_f1: 0.809 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 238 train_loss: 0.319 train_f1: 0.797 \t\n",
      "\n",
      "Validation 238 valid_f1: 0.785 best_f1: 0.809 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 239 train_loss: 0.317 train_f1: 0.802 \t\n",
      "\n",
      "Validation 239 valid_f1: 0.792 best_f1: 0.809 mean accuracy:0.728 \t\n",
      "\n",
      "Epoch 240 train_loss: 0.316 train_f1: 0.807 \t\n",
      "\n",
      "Validation 240 valid_f1: 0.776 best_f1: 0.809 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 241 train_loss: 0.316 train_f1: 0.810 \t\n",
      "\n",
      "Validation 241 valid_f1: 0.772 best_f1: 0.809 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 242 train_loss: 0.315 train_f1: 0.816 \t\n",
      "\n",
      "Validation 242 valid_f1: 0.761 best_f1: 0.809 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 243 train_loss: 0.316 train_f1: 0.814 \t\n",
      "\n",
      "Validation 243 valid_f1: 0.794 best_f1: 0.809 mean accuracy:0.703 \t\n",
      "\n",
      "Epoch 244 train_loss: 0.319 train_f1: 0.809 \t\n",
      "\n",
      "Validation 244 valid_f1: 0.774 best_f1: 0.809 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 245 train_loss: 0.315 train_f1: 0.819 \t\n",
      "\n",
      "Validation 245 valid_f1: 0.795 best_f1: 0.809 mean accuracy:0.717 \t\n",
      "\n",
      "Epoch 246 train_loss: 0.316 train_f1: 0.816 \t\n",
      "\n",
      "Validation 246 valid_f1: 0.784 best_f1: 0.809 mean accuracy:0.696 \t\n",
      "\n",
      "Epoch 247 train_loss: 0.315 train_f1: 0.816 \t\n",
      "\n",
      "Validation 247 valid_f1: 0.792 best_f1: 0.809 mean accuracy:0.717 \t\n",
      "\n",
      "Epoch 248 train_loss: 0.316 train_f1: 0.812 \t\n",
      "\n",
      "Validation 248 valid_f1: 0.712 best_f1: 0.809 mean accuracy:0.540 \t\n",
      "\n",
      "Epoch 249 train_loss: 0.318 train_f1: 0.809 \t\n",
      "\n",
      "Validation 249 valid_f1: 0.759 best_f1: 0.809 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 250 train_loss: 0.318 train_f1: 0.806 \t\n",
      "\n",
      "Validation 250 valid_f1: 0.765 best_f1: 0.809 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 251 train_loss: 0.319 train_f1: 0.805 \t\n",
      "\n",
      "Validation 251 valid_f1: 0.771 best_f1: 0.809 mean accuracy:0.696 \t\n",
      "\n",
      "Epoch 252 train_loss: 0.316 train_f1: 0.815 \t\n",
      "\n",
      "Validation 252 valid_f1: 0.784 best_f1: 0.809 mean accuracy:0.707 \t\n",
      "\n",
      "Epoch 253 train_loss: 0.315 train_f1: 0.821 \t\n",
      "\n",
      "Validation 253 valid_f1: 0.799 best_f1: 0.809 mean accuracy:0.717 \t\n",
      "\n",
      "Epoch 254 train_loss: 0.316 train_f1: 0.824 \t\n",
      "\n",
      "Validation 254 valid_f1: 0.772 best_f1: 0.809 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 255 train_loss: 0.316 train_f1: 0.819 \t\n",
      "\n",
      "Validation 255 valid_f1: 0.757 best_f1: 0.809 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 256 train_loss: 0.315 train_f1: 0.826 \t\n",
      "\n",
      "Validation 256 valid_f1: 0.778 best_f1: 0.809 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 257 train_loss: 0.315 train_f1: 0.822 \t\n",
      "\n",
      "Validation 257 valid_f1: 0.784 best_f1: 0.809 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 258 train_loss: 0.314 train_f1: 0.827 \t\n",
      "\n",
      "Validation 258 valid_f1: 0.787 best_f1: 0.809 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 259 train_loss: 0.314 train_f1: 0.829 \t\n",
      "\n",
      "Validation 259 valid_f1: 0.779 best_f1: 0.809 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 260 train_loss: 0.314 train_f1: 0.826 \t\n",
      "\n",
      "Validation 260 valid_f1: 0.749 best_f1: 0.809 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 261 train_loss: 0.316 train_f1: 0.815 \t\n",
      "\n",
      "Validation 261 valid_f1: 0.763 best_f1: 0.809 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 262 train_loss: 0.317 train_f1: 0.816 \t\n",
      "\n",
      "Validation 262 valid_f1: 0.779 best_f1: 0.809 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 263 train_loss: 0.315 train_f1: 0.823 \t\n",
      "\n",
      "Validation 263 valid_f1: 0.786 best_f1: 0.809 mean accuracy:0.703 \t\n",
      "\n",
      "Epoch 264 train_loss: 0.316 train_f1: 0.823 \t\n",
      "\n",
      "Validation 264 valid_f1: 0.796 best_f1: 0.809 mean accuracy:0.728 \t\n",
      "\n",
      "Epoch 265 train_loss: 0.314 train_f1: 0.829 \t\n",
      "\n",
      "Validation 265 valid_f1: 0.790 best_f1: 0.809 mean accuracy:0.717 \t\n",
      "\n",
      "Epoch 266 train_loss: 0.315 train_f1: 0.831 \t\n",
      "\n",
      "Validation 266 valid_f1: 0.772 best_f1: 0.809 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 267 train_loss: 0.317 train_f1: 0.816 \t\n",
      "\n",
      "Validation 267 valid_f1: 0.778 best_f1: 0.809 mean accuracy:0.707 \t\n",
      "\n",
      "Epoch 268 train_loss: 0.315 train_f1: 0.826 \t\n",
      "\n",
      "Validation 268 valid_f1: 0.754 best_f1: 0.809 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 269 train_loss: 0.314 train_f1: 0.834 \t\n",
      "\n",
      "Validation 269 valid_f1: 0.794 best_f1: 0.809 mean accuracy:0.710 \t\n",
      "\n",
      "Epoch 270 train_loss: 0.314 train_f1: 0.835 \t\n",
      "\n",
      "Validation 270 valid_f1: 0.781 best_f1: 0.809 mean accuracy:0.699 \t\n",
      "\n",
      "Epoch 271 train_loss: 0.314 train_f1: 0.830 \t\n",
      "\n",
      "Validation 271 valid_f1: 0.791 best_f1: 0.809 mean accuracy:0.710 \t\n",
      "\n",
      "Epoch 272 train_loss: 0.315 train_f1: 0.830 \t\n",
      "\n",
      "Validation 272 valid_f1: 0.751 best_f1: 0.809 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 273 train_loss: 0.317 train_f1: 0.825 \t\n",
      "\n",
      "Validation 273 valid_f1: 0.779 best_f1: 0.809 mean accuracy:0.699 \t\n",
      "\n",
      "Epoch 274 train_loss: 0.316 train_f1: 0.825 \t\n",
      "\n",
      "Validation 274 valid_f1: 0.784 best_f1: 0.809 mean accuracy:0.699 \t\n",
      "\n",
      "Epoch 275 train_loss: 0.314 train_f1: 0.835 \t\n",
      "\n",
      "Validation 275 valid_f1: 0.771 best_f1: 0.809 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 276 train_loss: 0.314 train_f1: 0.837 \t\n",
      "\n",
      "Validation 276 valid_f1: 0.775 best_f1: 0.809 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 277 train_loss: 0.316 train_f1: 0.829 \t\n",
      "\n",
      "Validation 277 valid_f1: 0.728 best_f1: 0.809 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 278 train_loss: 0.317 train_f1: 0.822 \t\n",
      "\n",
      "Validation 278 valid_f1: 0.792 best_f1: 0.809 mean accuracy:0.707 \t\n",
      "\n",
      "Epoch 279 train_loss: 0.314 train_f1: 0.837 \t\n",
      "\n",
      "Validation 279 valid_f1: 0.767 best_f1: 0.809 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 280 train_loss: 0.314 train_f1: 0.835 \t\n",
      "\n",
      "Validation 280 valid_f1: 0.802 best_f1: 0.809 mean accuracy:0.732 \t\n",
      "\n",
      "Epoch 281 train_loss: 0.314 train_f1: 0.837 \t\n",
      "Early stopped training due to non-improved performance\n",
      "2020-07-25 01:19:41.874662\n",
      "../results_20200724_e2e_ABN_zero_gamma0.1\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, 12)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, None, 64)     2368        input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, None, 64)     256         conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, None, 64)     12352       batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, None, 64)     256         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, None, 64)     0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, None, 128)    24704       max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, None, 128)    512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, None, 128)    49280       batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, None, 128)    512         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, None, 128)    0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, None, 256)    98560       max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, None, 256)    1024        conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, None, 256)    196864      batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, None, 256)    1024        conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, None, 256)    196864      batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, None, 256)    1024        conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, None, 256)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, None, 512)    393728      max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, None, 512)    2048        conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, None, 512)    786944      batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, None, 512)    2048        conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, None, 512)    786944      batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, None, 512)    2048        conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, None, 512)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, None, 512)    786944      max_pooling1d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, None, 512)    2048        conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, None, 256)    393472      batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, None, 256)    1024        conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, None, 128)    98432       batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, None, 128)    512         conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, None, 128)    0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, None, 64)     8192        max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, None, 64)     256         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, None, 64)     0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, None, 64)     12288       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, None, 64)     256         conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, None, 64)     0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, None, 256)    16384       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, None, 256)    32768       max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, None, 256)    1024        conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, None, 256)    1024        conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, None, 256)    0           batch_normalization_58[0][0]     \n",
      "                                                                 batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, None, 256)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_bn_1 (BatchNor (None, None, 256)    1024        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_conv_1 (Conv1D (None, None, 9)      2304        attention_branch_bn_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_conv_1 (Co (None, None, 1)      9           attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_bn_1 (Batc (None, None, 1)      4           attention_branch_att_conv_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_sigmoid_1  (None, None, 1)      0           attention_branch_att_bn_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 128)    0           max_pooling1d_14[0][0]           \n",
      "                                                                 attention_branch_att_sigmoid_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, None, 64)     8192        lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, None, 64)     256         conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, None, 64)     0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, None, 64)     12288       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, None, 64)     256         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, None, 64)     0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, None, 256)    16384       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, None, 256)    32768       lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, None, 256)    1024        conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, None, 256)    1024        conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, None, 256)    0           batch_normalization_62[0][0]     \n",
      "                                                                 batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, None, 256)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_avgpool_1 (Gl (None, 256)          0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_1 (Dens (None, 512)          131584      perception_branch_avgpool_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_gap_1 (GlobalA (None, 9)            0           attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_2 (Dens (None, 9)            4617        perception_branch_dense_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_output (Activa (None, 9)            0           attention_branch_gap_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_output (Activ (None, 9)            0           perception_branch_dense_2[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 4,125,718\n",
      "Trainable params: 4,115,476\n",
      "Non-trainable params: 10,242\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 train_loss: 0.612 train_f1: 0.131 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1511: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 0 valid_f1: 0.013 best_f1: 0.013 mean accuracy:0.004 \t\n",
      "\n",
      "Epoch 1 train_loss: 0.538 train_f1: 0.156 \t\n",
      "\n",
      "Validation 1 valid_f1: 0.238 best_f1: 0.238 mean accuracy:0.116 \t\n",
      "\n",
      "Epoch 2 train_loss: 0.514 train_f1: 0.226 \t\n",
      "\n",
      "Validation 2 valid_f1: 0.272 best_f1: 0.272 mean accuracy:0.134 \t\n",
      "\n",
      "Epoch 3 train_loss: 0.499 train_f1: 0.257 \t\n",
      "\n",
      "Validation 3 valid_f1: 0.298 best_f1: 0.298 mean accuracy:0.156 \t\n",
      "\n",
      "Epoch 4 train_loss: 0.488 train_f1: 0.274 \t\n",
      "\n",
      "Validation 4 valid_f1: 0.354 best_f1: 0.354 mean accuracy:0.199 \t\n",
      "\n",
      "Epoch 5 train_loss: 0.479 train_f1: 0.299 \t\n",
      "\n",
      "Validation 5 valid_f1: 0.396 best_f1: 0.396 mean accuracy:0.236 \t\n",
      "\n",
      "Epoch 6 train_loss: 0.472 train_f1: 0.315 \t\n",
      "\n",
      "Validation 6 valid_f1: 0.423 best_f1: 0.423 mean accuracy:0.250 \t\n",
      "\n",
      "Epoch 7 train_loss: 0.465 train_f1: 0.335 \t\n",
      "\n",
      "Validation 7 valid_f1: 0.454 best_f1: 0.454 mean accuracy:0.279 \t\n",
      "\n",
      "Epoch 8 train_loss: 0.460 train_f1: 0.347 \t\n",
      "\n",
      "Validation 8 valid_f1: 0.489 best_f1: 0.489 mean accuracy:0.304 \t\n",
      "\n",
      "Epoch 9 train_loss: 0.455 train_f1: 0.362 \t\n",
      "\n",
      "Validation 9 valid_f1: 0.488 best_f1: 0.489 mean accuracy:0.308 \t\n",
      "\n",
      "Epoch 10 train_loss: 0.451 train_f1: 0.372 \t\n",
      "\n",
      "Validation 10 valid_f1: 0.507 best_f1: 0.507 mean accuracy:0.330 \t\n",
      "\n",
      "Epoch 11 train_loss: 0.447 train_f1: 0.387 \t\n",
      "\n",
      "Validation 11 valid_f1: 0.545 best_f1: 0.545 mean accuracy:0.362 \t\n",
      "\n",
      "Epoch 12 train_loss: 0.443 train_f1: 0.397 \t\n",
      "\n",
      "Validation 12 valid_f1: 0.530 best_f1: 0.545 mean accuracy:0.355 \t\n",
      "\n",
      "Epoch 13 train_loss: 0.440 train_f1: 0.408 \t\n",
      "\n",
      "Validation 13 valid_f1: 0.554 best_f1: 0.554 mean accuracy:0.370 \t\n",
      "\n",
      "Epoch 14 train_loss: 0.436 train_f1: 0.417 \t\n",
      "\n",
      "Validation 14 valid_f1: 0.573 best_f1: 0.573 mean accuracy:0.388 \t\n",
      "\n",
      "Epoch 15 train_loss: 0.433 train_f1: 0.427 \t\n",
      "\n",
      "Validation 15 valid_f1: 0.573 best_f1: 0.573 mean accuracy:0.402 \t\n",
      "\n",
      "Epoch 16 train_loss: 0.430 train_f1: 0.438 \t\n",
      "\n",
      "Validation 16 valid_f1: 0.591 best_f1: 0.591 mean accuracy:0.409 \t\n",
      "\n",
      "Epoch 17 train_loss: 0.427 train_f1: 0.447 \t\n",
      "\n",
      "Validation 17 valid_f1: 0.596 best_f1: 0.596 mean accuracy:0.428 \t\n",
      "\n",
      "Epoch 18 train_loss: 0.425 train_f1: 0.454 \t\n",
      "\n",
      "Validation 18 valid_f1: 0.594 best_f1: 0.596 mean accuracy:0.438 \t\n",
      "\n",
      "Epoch 19 train_loss: 0.422 train_f1: 0.466 \t\n",
      "\n",
      "Validation 19 valid_f1: 0.599 best_f1: 0.599 mean accuracy:0.431 \t\n",
      "\n",
      "Epoch 20 train_loss: 0.420 train_f1: 0.472 \t\n",
      "\n",
      "Validation 20 valid_f1: 0.630 best_f1: 0.630 mean accuracy:0.478 \t\n",
      "\n",
      "Epoch 21 train_loss: 0.418 train_f1: 0.479 \t\n",
      "\n",
      "Validation 21 valid_f1: 0.649 best_f1: 0.649 mean accuracy:0.493 \t\n",
      "\n",
      "Epoch 22 train_loss: 0.417 train_f1: 0.480 \t\n",
      "\n",
      "Validation 22 valid_f1: 0.646 best_f1: 0.649 mean accuracy:0.500 \t\n",
      "\n",
      "Epoch 23 train_loss: 0.415 train_f1: 0.487 \t\n",
      "\n",
      "Validation 23 valid_f1: 0.664 best_f1: 0.664 mean accuracy:0.511 \t\n",
      "\n",
      "Epoch 24 train_loss: 0.413 train_f1: 0.489 \t\n",
      "\n",
      "Validation 24 valid_f1: 0.661 best_f1: 0.664 mean accuracy:0.504 \t\n",
      "\n",
      "Epoch 25 train_loss: 0.411 train_f1: 0.495 \t\n",
      "\n",
      "Validation 25 valid_f1: 0.693 best_f1: 0.693 mean accuracy:0.554 \t\n",
      "\n",
      "Epoch 26 train_loss: 0.409 train_f1: 0.501 \t\n",
      "\n",
      "Validation 26 valid_f1: 0.667 best_f1: 0.693 mean accuracy:0.511 \t\n",
      "\n",
      "Epoch 27 train_loss: 0.409 train_f1: 0.503 \t\n",
      "\n",
      "Validation 27 valid_f1: 0.695 best_f1: 0.695 mean accuracy:0.554 \t\n",
      "\n",
      "Epoch 28 train_loss: 0.406 train_f1: 0.509 \t\n",
      "\n",
      "Validation 28 valid_f1: 0.712 best_f1: 0.712 mean accuracy:0.569 \t\n",
      "\n",
      "Epoch 29 train_loss: 0.404 train_f1: 0.516 \t\n",
      "\n",
      "Validation 29 valid_f1: 0.679 best_f1: 0.712 mean accuracy:0.533 \t\n",
      "\n",
      "Epoch 30 train_loss: 0.403 train_f1: 0.514 \t\n",
      "\n",
      "Validation 30 valid_f1: 0.705 best_f1: 0.712 mean accuracy:0.576 \t\n",
      "\n",
      "Epoch 31 train_loss: 0.401 train_f1: 0.520 \t\n",
      "\n",
      "Validation 31 valid_f1: 0.689 best_f1: 0.712 mean accuracy:0.554 \t\n",
      "\n",
      "Epoch 32 train_loss: 0.400 train_f1: 0.524 \t\n",
      "\n",
      "Validation 32 valid_f1: 0.703 best_f1: 0.712 mean accuracy:0.580 \t\n",
      "\n",
      "Epoch 33 train_loss: 0.398 train_f1: 0.526 \t\n",
      "\n",
      "Validation 33 valid_f1: 0.697 best_f1: 0.712 mean accuracy:0.580 \t\n",
      "\n",
      "Epoch 34 train_loss: 0.398 train_f1: 0.529 \t\n",
      "\n",
      "Validation 34 valid_f1: 0.700 best_f1: 0.712 mean accuracy:0.554 \t\n",
      "\n",
      "Epoch 35 train_loss: 0.396 train_f1: 0.534 \t\n",
      "\n",
      "Validation 35 valid_f1: 0.702 best_f1: 0.712 mean accuracy:0.569 \t\n",
      "\n",
      "Epoch 36 train_loss: 0.395 train_f1: 0.532 \t\n",
      "\n",
      "Validation 36 valid_f1: 0.710 best_f1: 0.712 mean accuracy:0.587 \t\n",
      "\n",
      "Epoch 37 train_loss: 0.393 train_f1: 0.540 \t\n",
      "\n",
      "Validation 37 valid_f1: 0.700 best_f1: 0.712 mean accuracy:0.583 \t\n",
      "\n",
      "Epoch 38 train_loss: 0.393 train_f1: 0.538 \t\n",
      "\n",
      "Validation 38 valid_f1: 0.705 best_f1: 0.712 mean accuracy:0.569 \t\n",
      "\n",
      "Epoch 39 train_loss: 0.391 train_f1: 0.543 \t\n",
      "\n",
      "Validation 39 valid_f1: 0.715 best_f1: 0.715 mean accuracy:0.591 \t\n",
      "\n",
      "Epoch 40 train_loss: 0.389 train_f1: 0.548 \t\n",
      "\n",
      "Validation 40 valid_f1: 0.708 best_f1: 0.715 mean accuracy:0.580 \t\n",
      "\n",
      "Epoch 41 train_loss: 0.389 train_f1: 0.551 \t\n",
      "\n",
      "Validation 41 valid_f1: 0.676 best_f1: 0.715 mean accuracy:0.543 \t\n",
      "\n",
      "Epoch 42 train_loss: 0.387 train_f1: 0.554 \t\n",
      "\n",
      "Validation 42 valid_f1: 0.693 best_f1: 0.715 mean accuracy:0.554 \t\n",
      "\n",
      "Epoch 43 train_loss: 0.386 train_f1: 0.556 \t\n",
      "\n",
      "Validation 43 valid_f1: 0.712 best_f1: 0.715 mean accuracy:0.591 \t\n",
      "\n",
      "Epoch 44 train_loss: 0.385 train_f1: 0.559 \t\n",
      "\n",
      "Validation 44 valid_f1: 0.711 best_f1: 0.715 mean accuracy:0.601 \t\n",
      "\n",
      "Epoch 45 train_loss: 0.385 train_f1: 0.557 \t\n",
      "\n",
      "Validation 45 valid_f1: 0.716 best_f1: 0.716 mean accuracy:0.601 \t\n",
      "\n",
      "Epoch 46 train_loss: 0.382 train_f1: 0.565 \t\n",
      "\n",
      "Validation 46 valid_f1: 0.724 best_f1: 0.724 mean accuracy:0.594 \t\n",
      "\n",
      "Epoch 47 train_loss: 0.382 train_f1: 0.569 \t\n",
      "\n",
      "Validation 47 valid_f1: 0.705 best_f1: 0.724 mean accuracy:0.591 \t\n",
      "\n",
      "Epoch 48 train_loss: 0.381 train_f1: 0.568 \t\n",
      "\n",
      "Validation 48 valid_f1: 0.709 best_f1: 0.724 mean accuracy:0.583 \t\n",
      "\n",
      "Epoch 49 train_loss: 0.380 train_f1: 0.571 \t\n",
      "\n",
      "Validation 49 valid_f1: 0.726 best_f1: 0.726 mean accuracy:0.594 \t\n",
      "\n",
      "Epoch 50 train_loss: 0.378 train_f1: 0.576 \t\n",
      "\n",
      "Validation 50 valid_f1: 0.711 best_f1: 0.726 mean accuracy:0.587 \t\n",
      "\n",
      "Epoch 51 train_loss: 0.379 train_f1: 0.575 \t\n",
      "\n",
      "Validation 51 valid_f1: 0.705 best_f1: 0.726 mean accuracy:0.569 \t\n",
      "\n",
      "Epoch 52 train_loss: 0.377 train_f1: 0.582 \t\n",
      "\n",
      "Validation 52 valid_f1: 0.690 best_f1: 0.726 mean accuracy:0.583 \t\n",
      "\n",
      "Epoch 53 train_loss: 0.375 train_f1: 0.583 \t\n",
      "\n",
      "Validation 53 valid_f1: 0.712 best_f1: 0.726 mean accuracy:0.605 \t\n",
      "\n",
      "Epoch 54 train_loss: 0.374 train_f1: 0.588 \t\n",
      "\n",
      "Validation 54 valid_f1: 0.722 best_f1: 0.726 mean accuracy:0.598 \t\n",
      "\n",
      "Epoch 55 train_loss: 0.373 train_f1: 0.589 \t\n",
      "\n",
      "Validation 55 valid_f1: 0.705 best_f1: 0.726 mean accuracy:0.601 \t\n",
      "\n",
      "Epoch 56 train_loss: 0.373 train_f1: 0.590 \t\n",
      "\n",
      "Validation 56 valid_f1: 0.699 best_f1: 0.726 mean accuracy:0.583 \t\n",
      "\n",
      "Epoch 57 train_loss: 0.372 train_f1: 0.594 \t\n",
      "\n",
      "Validation 57 valid_f1: 0.741 best_f1: 0.741 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 58 train_loss: 0.370 train_f1: 0.600 \t\n",
      "\n",
      "Validation 58 valid_f1: 0.714 best_f1: 0.741 mean accuracy:0.583 \t\n",
      "\n",
      "Epoch 59 train_loss: 0.370 train_f1: 0.596 \t\n",
      "\n",
      "Validation 59 valid_f1: 0.723 best_f1: 0.741 mean accuracy:0.605 \t\n",
      "\n",
      "Epoch 60 train_loss: 0.368 train_f1: 0.602 \t\n",
      "\n",
      "Validation 60 valid_f1: 0.723 best_f1: 0.741 mean accuracy:0.605 \t\n",
      "\n",
      "Epoch 61 train_loss: 0.367 train_f1: 0.607 \t\n",
      "\n",
      "Validation 61 valid_f1: 0.732 best_f1: 0.741 mean accuracy:0.598 \t\n",
      "\n",
      "Epoch 62 train_loss: 0.366 train_f1: 0.608 \t\n",
      "\n",
      "Validation 62 valid_f1: 0.740 best_f1: 0.741 mean accuracy:0.616 \t\n",
      "\n",
      "Epoch 63 train_loss: 0.365 train_f1: 0.612 \t\n",
      "\n",
      "Validation 63 valid_f1: 0.712 best_f1: 0.741 mean accuracy:0.583 \t\n",
      "\n",
      "Epoch 64 train_loss: 0.366 train_f1: 0.611 \t\n",
      "\n",
      "Validation 64 valid_f1: 0.698 best_f1: 0.741 mean accuracy:0.569 \t\n",
      "\n",
      "Epoch 65 train_loss: 0.365 train_f1: 0.613 \t\n",
      "\n",
      "Validation 65 valid_f1: 0.722 best_f1: 0.741 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 66 train_loss: 0.363 train_f1: 0.617 \t\n",
      "\n",
      "Validation 66 valid_f1: 0.729 best_f1: 0.741 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 67 train_loss: 0.363 train_f1: 0.616 \t\n",
      "\n",
      "Validation 67 valid_f1: 0.735 best_f1: 0.741 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 68 train_loss: 0.360 train_f1: 0.624 \t\n",
      "\n",
      "Validation 68 valid_f1: 0.731 best_f1: 0.741 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 69 train_loss: 0.360 train_f1: 0.622 \t\n",
      "\n",
      "Validation 69 valid_f1: 0.743 best_f1: 0.743 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 70 train_loss: 0.360 train_f1: 0.625 \t\n",
      "\n",
      "Validation 70 valid_f1: 0.722 best_f1: 0.743 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 71 train_loss: 0.359 train_f1: 0.627 \t\n",
      "\n",
      "Validation 71 valid_f1: 0.736 best_f1: 0.743 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 72 train_loss: 0.358 train_f1: 0.631 \t\n",
      "\n",
      "Validation 72 valid_f1: 0.707 best_f1: 0.743 mean accuracy:0.591 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73 train_loss: 0.356 train_f1: 0.637 \t\n",
      "\n",
      "Validation 73 valid_f1: 0.738 best_f1: 0.743 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 74 train_loss: 0.356 train_f1: 0.637 \t\n",
      "\n",
      "Validation 74 valid_f1: 0.736 best_f1: 0.743 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 75 train_loss: 0.356 train_f1: 0.637 \t\n",
      "\n",
      "Validation 75 valid_f1: 0.740 best_f1: 0.743 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 76 train_loss: 0.355 train_f1: 0.637 \t\n",
      "\n",
      "Validation 76 valid_f1: 0.718 best_f1: 0.743 mean accuracy:0.609 \t\n",
      "\n",
      "Epoch 77 train_loss: 0.355 train_f1: 0.639 \t\n",
      "\n",
      "Validation 77 valid_f1: 0.753 best_f1: 0.753 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 78 train_loss: 0.353 train_f1: 0.643 \t\n",
      "\n",
      "Validation 78 valid_f1: 0.744 best_f1: 0.753 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 79 train_loss: 0.353 train_f1: 0.643 \t\n",
      "\n",
      "Validation 79 valid_f1: 0.732 best_f1: 0.753 mean accuracy:0.601 \t\n",
      "\n",
      "Epoch 80 train_loss: 0.353 train_f1: 0.643 \t\n",
      "\n",
      "Validation 80 valid_f1: 0.725 best_f1: 0.753 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 81 train_loss: 0.351 train_f1: 0.647 \t\n",
      "\n",
      "Validation 81 valid_f1: 0.710 best_f1: 0.753 mean accuracy:0.580 \t\n",
      "\n",
      "Epoch 82 train_loss: 0.351 train_f1: 0.650 \t\n",
      "\n",
      "Validation 82 valid_f1: 0.750 best_f1: 0.753 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 83 train_loss: 0.350 train_f1: 0.650 \t\n",
      "\n",
      "Validation 83 valid_f1: 0.715 best_f1: 0.753 mean accuracy:0.609 \t\n",
      "\n",
      "Epoch 84 train_loss: 0.350 train_f1: 0.653 \t\n",
      "\n",
      "Validation 84 valid_f1: 0.716 best_f1: 0.753 mean accuracy:0.605 \t\n",
      "\n",
      "Epoch 85 train_loss: 0.349 train_f1: 0.654 \t\n",
      "\n",
      "Validation 85 valid_f1: 0.734 best_f1: 0.753 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 86 train_loss: 0.348 train_f1: 0.657 \t\n",
      "\n",
      "Validation 86 valid_f1: 0.746 best_f1: 0.753 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 87 train_loss: 0.348 train_f1: 0.657 \t\n",
      "\n",
      "Validation 87 valid_f1: 0.746 best_f1: 0.753 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 88 train_loss: 0.346 train_f1: 0.663 \t\n",
      "\n",
      "Validation 88 valid_f1: 0.732 best_f1: 0.753 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 89 train_loss: 0.347 train_f1: 0.661 \t\n",
      "\n",
      "Validation 89 valid_f1: 0.746 best_f1: 0.753 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 90 train_loss: 0.346 train_f1: 0.662 \t\n",
      "\n",
      "Validation 90 valid_f1: 0.737 best_f1: 0.753 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 91 train_loss: 0.346 train_f1: 0.664 \t\n",
      "\n",
      "Validation 91 valid_f1: 0.739 best_f1: 0.753 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 92 train_loss: 0.347 train_f1: 0.661 \t\n",
      "\n",
      "Validation 92 valid_f1: 0.758 best_f1: 0.758 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 93 train_loss: 0.344 train_f1: 0.671 \t\n",
      "\n",
      "Validation 93 valid_f1: 0.757 best_f1: 0.758 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 94 train_loss: 0.344 train_f1: 0.667 \t\n",
      "\n",
      "Validation 94 valid_f1: 0.736 best_f1: 0.758 mean accuracy:0.616 \t\n",
      "\n",
      "Epoch 95 train_loss: 0.343 train_f1: 0.674 \t\n",
      "\n",
      "Validation 95 valid_f1: 0.751 best_f1: 0.758 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 96 train_loss: 0.342 train_f1: 0.673 \t\n",
      "\n",
      "Validation 96 valid_f1: 0.753 best_f1: 0.758 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 97 train_loss: 0.343 train_f1: 0.671 \t\n",
      "\n",
      "Validation 97 valid_f1: 0.763 best_f1: 0.763 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 98 train_loss: 0.341 train_f1: 0.674 \t\n",
      "\n",
      "Validation 98 valid_f1: 0.761 best_f1: 0.763 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 99 train_loss: 0.341 train_f1: 0.681 \t\n",
      "\n",
      "Validation 99 valid_f1: 0.766 best_f1: 0.766 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 100 train_loss: 0.341 train_f1: 0.675 \t\n",
      "\n",
      "Validation 100 valid_f1: 0.761 best_f1: 0.766 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 101 train_loss: 0.341 train_f1: 0.678 \t\n",
      "\n",
      "Validation 101 valid_f1: 0.738 best_f1: 0.766 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 102 train_loss: 0.339 train_f1: 0.683 \t\n",
      "\n",
      "Validation 102 valid_f1: 0.744 best_f1: 0.766 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 103 train_loss: 0.340 train_f1: 0.683 \t\n",
      "\n",
      "Validation 103 valid_f1: 0.749 best_f1: 0.766 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 104 train_loss: 0.339 train_f1: 0.684 \t\n",
      "\n",
      "Validation 104 valid_f1: 0.747 best_f1: 0.766 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 105 train_loss: 0.338 train_f1: 0.684 \t\n",
      "\n",
      "Validation 105 valid_f1: 0.759 best_f1: 0.766 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 106 train_loss: 0.338 train_f1: 0.685 \t\n",
      "\n",
      "Validation 106 valid_f1: 0.771 best_f1: 0.771 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 107 train_loss: 0.337 train_f1: 0.686 \t\n",
      "\n",
      "Validation 107 valid_f1: 0.747 best_f1: 0.771 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 108 train_loss: 0.338 train_f1: 0.684 \t\n",
      "\n",
      "Validation 108 valid_f1: 0.740 best_f1: 0.771 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 109 train_loss: 0.336 train_f1: 0.691 \t\n",
      "\n",
      "Validation 109 valid_f1: 0.755 best_f1: 0.771 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 110 train_loss: 0.337 train_f1: 0.691 \t\n",
      "\n",
      "Validation 110 valid_f1: 0.771 best_f1: 0.771 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 111 train_loss: 0.338 train_f1: 0.689 \t\n",
      "\n",
      "Validation 111 valid_f1: 0.780 best_f1: 0.780 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 112 train_loss: 0.337 train_f1: 0.691 \t\n",
      "\n",
      "Validation 112 valid_f1: 0.756 best_f1: 0.780 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 113 train_loss: 0.335 train_f1: 0.694 \t\n",
      "\n",
      "Validation 113 valid_f1: 0.761 best_f1: 0.780 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 114 train_loss: 0.336 train_f1: 0.693 \t\n",
      "\n",
      "Validation 114 valid_f1: 0.770 best_f1: 0.780 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 115 train_loss: 0.335 train_f1: 0.694 \t\n",
      "\n",
      "Validation 115 valid_f1: 0.766 best_f1: 0.780 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 116 train_loss: 0.336 train_f1: 0.695 \t\n",
      "\n",
      "Validation 116 valid_f1: 0.750 best_f1: 0.780 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 117 train_loss: 0.335 train_f1: 0.697 \t\n",
      "\n",
      "Validation 117 valid_f1: 0.732 best_f1: 0.780 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 118 train_loss: 0.335 train_f1: 0.696 \t\n",
      "\n",
      "Validation 118 valid_f1: 0.751 best_f1: 0.780 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 119 train_loss: 0.333 train_f1: 0.700 \t\n",
      "\n",
      "Validation 119 valid_f1: 0.743 best_f1: 0.780 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 120 train_loss: 0.334 train_f1: 0.692 \t\n",
      "\n",
      "Validation 120 valid_f1: 0.759 best_f1: 0.780 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 121 train_loss: 0.334 train_f1: 0.697 \t\n",
      "\n",
      "Validation 121 valid_f1: 0.773 best_f1: 0.780 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 122 train_loss: 0.333 train_f1: 0.701 \t\n",
      "\n",
      "Validation 122 valid_f1: 0.746 best_f1: 0.780 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 123 train_loss: 0.332 train_f1: 0.704 \t\n",
      "\n",
      "Validation 123 valid_f1: 0.778 best_f1: 0.780 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 124 train_loss: 0.334 train_f1: 0.702 \t\n",
      "\n",
      "Validation 124 valid_f1: 0.748 best_f1: 0.780 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 125 train_loss: 0.332 train_f1: 0.703 \t\n",
      "\n",
      "Validation 125 valid_f1: 0.769 best_f1: 0.780 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 126 train_loss: 0.331 train_f1: 0.706 \t\n",
      "\n",
      "Validation 126 valid_f1: 0.760 best_f1: 0.780 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 127 train_loss: 0.331 train_f1: 0.708 \t\n",
      "\n",
      "Validation 127 valid_f1: 0.774 best_f1: 0.780 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 128 train_loss: 0.330 train_f1: 0.705 \t\n",
      "\n",
      "Validation 128 valid_f1: 0.772 best_f1: 0.780 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 129 train_loss: 0.334 train_f1: 0.701 \t\n",
      "\n",
      "Validation 129 valid_f1: 0.765 best_f1: 0.780 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 130 train_loss: 0.332 train_f1: 0.703 \t\n",
      "\n",
      "Validation 130 valid_f1: 0.761 best_f1: 0.780 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 131 train_loss: 0.329 train_f1: 0.713 \t\n",
      "\n",
      "Validation 131 valid_f1: 0.776 best_f1: 0.780 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 132 train_loss: 0.330 train_f1: 0.704 \t\n",
      "\n",
      "Validation 132 valid_f1: 0.760 best_f1: 0.780 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 133 train_loss: 0.330 train_f1: 0.707 \t\n",
      "\n",
      "Validation 133 valid_f1: 0.761 best_f1: 0.780 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 134 train_loss: 0.329 train_f1: 0.714 \t\n",
      "\n",
      "Validation 134 valid_f1: 0.765 best_f1: 0.780 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 135 train_loss: 0.329 train_f1: 0.711 \t\n",
      "\n",
      "Validation 135 valid_f1: 0.744 best_f1: 0.780 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 136 train_loss: 0.329 train_f1: 0.712 \t\n",
      "\n",
      "Validation 136 valid_f1: 0.785 best_f1: 0.785 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 137 train_loss: 0.328 train_f1: 0.717 \t\n",
      "\n",
      "Validation 137 valid_f1: 0.769 best_f1: 0.785 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 138 train_loss: 0.329 train_f1: 0.712 \t\n",
      "\n",
      "Validation 138 valid_f1: 0.735 best_f1: 0.785 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 139 train_loss: 0.331 train_f1: 0.709 \t\n",
      "\n",
      "Validation 139 valid_f1: 0.755 best_f1: 0.785 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 140 train_loss: 0.330 train_f1: 0.711 \t\n",
      "\n",
      "Validation 140 valid_f1: 0.764 best_f1: 0.785 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 141 train_loss: 0.330 train_f1: 0.715 \t\n",
      "\n",
      "Validation 141 valid_f1: 0.768 best_f1: 0.785 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 142 train_loss: 0.328 train_f1: 0.720 \t\n",
      "\n",
      "Validation 142 valid_f1: 0.770 best_f1: 0.785 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 143 train_loss: 0.328 train_f1: 0.719 \t\n",
      "\n",
      "Validation 143 valid_f1: 0.771 best_f1: 0.785 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 144 train_loss: 0.327 train_f1: 0.721 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 144 valid_f1: 0.750 best_f1: 0.785 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 145 train_loss: 0.328 train_f1: 0.723 \t\n",
      "\n",
      "Validation 145 valid_f1: 0.745 best_f1: 0.785 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 146 train_loss: 0.329 train_f1: 0.719 \t\n",
      "\n",
      "Validation 146 valid_f1: 0.752 best_f1: 0.785 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 147 train_loss: 0.328 train_f1: 0.721 \t\n",
      "\n",
      "Validation 147 valid_f1: 0.748 best_f1: 0.785 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 148 train_loss: 0.328 train_f1: 0.719 \t\n",
      "\n",
      "Validation 148 valid_f1: 0.754 best_f1: 0.785 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 149 train_loss: 0.328 train_f1: 0.720 \t\n",
      "\n",
      "Validation 149 valid_f1: 0.760 best_f1: 0.785 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 150 train_loss: 0.327 train_f1: 0.721 \t\n",
      "\n",
      "Validation 150 valid_f1: 0.758 best_f1: 0.785 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 151 train_loss: 0.326 train_f1: 0.720 \t\n",
      "\n",
      "Validation 151 valid_f1: 0.766 best_f1: 0.785 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 152 train_loss: 0.326 train_f1: 0.723 \t\n",
      "\n",
      "Validation 152 valid_f1: 0.753 best_f1: 0.785 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 153 train_loss: 0.326 train_f1: 0.727 \t\n",
      "\n",
      "Validation 153 valid_f1: 0.777 best_f1: 0.785 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 154 train_loss: 0.326 train_f1: 0.727 \t\n",
      "\n",
      "Validation 154 valid_f1: 0.726 best_f1: 0.785 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 155 train_loss: 0.326 train_f1: 0.727 \t\n",
      "\n",
      "Validation 155 valid_f1: 0.736 best_f1: 0.785 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 156 train_loss: 0.325 train_f1: 0.732 \t\n",
      "\n",
      "Validation 156 valid_f1: 0.739 best_f1: 0.785 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 157 train_loss: 0.326 train_f1: 0.730 \t\n",
      "\n",
      "Validation 157 valid_f1: 0.759 best_f1: 0.785 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 158 train_loss: 0.324 train_f1: 0.734 \t\n",
      "\n",
      "Validation 158 valid_f1: 0.777 best_f1: 0.785 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 159 train_loss: 0.326 train_f1: 0.729 \t\n",
      "\n",
      "Validation 159 valid_f1: 0.708 best_f1: 0.785 mean accuracy:0.569 \t\n",
      "\n",
      "Epoch 160 train_loss: 0.327 train_f1: 0.731 \t\n",
      "\n",
      "Validation 160 valid_f1: 0.759 best_f1: 0.785 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 161 train_loss: 0.326 train_f1: 0.733 \t\n",
      "\n",
      "Validation 161 valid_f1: 0.766 best_f1: 0.785 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 162 train_loss: 0.325 train_f1: 0.739 \t\n",
      "\n",
      "Validation 162 valid_f1: 0.763 best_f1: 0.785 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 163 train_loss: 0.324 train_f1: 0.740 \t\n",
      "\n",
      "Validation 163 valid_f1: 0.746 best_f1: 0.785 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 164 train_loss: 0.324 train_f1: 0.741 \t\n",
      "\n",
      "Validation 164 valid_f1: 0.768 best_f1: 0.785 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 165 train_loss: 0.325 train_f1: 0.735 \t\n",
      "\n",
      "Validation 165 valid_f1: 0.741 best_f1: 0.785 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 166 train_loss: 0.324 train_f1: 0.741 \t\n",
      "\n",
      "Validation 166 valid_f1: 0.777 best_f1: 0.785 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 167 train_loss: 0.325 train_f1: 0.740 \t\n",
      "\n",
      "Validation 167 valid_f1: 0.730 best_f1: 0.785 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 168 train_loss: 0.325 train_f1: 0.737 \t\n",
      "\n",
      "Validation 168 valid_f1: 0.766 best_f1: 0.785 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 169 train_loss: 0.323 train_f1: 0.742 \t\n",
      "\n",
      "Validation 169 valid_f1: 0.747 best_f1: 0.785 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 170 train_loss: 0.323 train_f1: 0.742 \t\n",
      "\n",
      "Validation 170 valid_f1: 0.765 best_f1: 0.785 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 171 train_loss: 0.324 train_f1: 0.740 \t\n",
      "\n",
      "Validation 171 valid_f1: 0.769 best_f1: 0.785 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 172 train_loss: 0.324 train_f1: 0.742 \t\n",
      "\n",
      "Validation 172 valid_f1: 0.768 best_f1: 0.785 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 173 train_loss: 0.323 train_f1: 0.748 \t\n",
      "\n",
      "Validation 173 valid_f1: 0.750 best_f1: 0.785 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 174 train_loss: 0.323 train_f1: 0.749 \t\n",
      "\n",
      "Validation 174 valid_f1: 0.739 best_f1: 0.785 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 175 train_loss: 0.322 train_f1: 0.749 \t\n",
      "\n",
      "Validation 175 valid_f1: 0.771 best_f1: 0.785 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 176 train_loss: 0.323 train_f1: 0.746 \t\n",
      "\n",
      "Validation 176 valid_f1: 0.715 best_f1: 0.785 mean accuracy:0.583 \t\n",
      "\n",
      "Epoch 177 train_loss: 0.324 train_f1: 0.743 \t\n",
      "\n",
      "Validation 177 valid_f1: 0.768 best_f1: 0.785 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 178 train_loss: 0.323 train_f1: 0.753 \t\n",
      "\n",
      "Validation 178 valid_f1: 0.746 best_f1: 0.785 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 179 train_loss: 0.323 train_f1: 0.747 \t\n",
      "\n",
      "Validation 179 valid_f1: 0.780 best_f1: 0.785 mean accuracy:0.707 \t\n",
      "\n",
      "Epoch 180 train_loss: 0.323 train_f1: 0.749 \t\n",
      "\n",
      "Validation 180 valid_f1: 0.759 best_f1: 0.785 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 181 train_loss: 0.322 train_f1: 0.750 \t\n",
      "\n",
      "Validation 181 valid_f1: 0.757 best_f1: 0.785 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 182 train_loss: 0.321 train_f1: 0.755 \t\n",
      "\n",
      "Validation 182 valid_f1: 0.785 best_f1: 0.785 mean accuracy:0.699 \t\n",
      "\n",
      "Epoch 183 train_loss: 0.322 train_f1: 0.749 \t\n",
      "\n",
      "Validation 183 valid_f1: 0.755 best_f1: 0.785 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 184 train_loss: 0.326 train_f1: 0.747 \t\n",
      "\n",
      "Validation 184 valid_f1: 0.767 best_f1: 0.785 mean accuracy:0.696 \t\n",
      "\n",
      "Epoch 185 train_loss: 0.321 train_f1: 0.757 \t\n",
      "\n",
      "Validation 185 valid_f1: 0.764 best_f1: 0.785 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 186 train_loss: 0.323 train_f1: 0.756 \t\n",
      "\n",
      "Validation 186 valid_f1: 0.747 best_f1: 0.785 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 187 train_loss: 0.320 train_f1: 0.758 \t\n",
      "Early stopped training due to non-improved performance\n",
      "2020-07-25 09:02:14.891209\n",
      "../results_20200724_e2e_ABN_zero_gamma1\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, 12)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, None, 64)     2368        input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, None, 64)     256         conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, None, 64)     12352       batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, None, 64)     256         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, None, 64)     0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, None, 128)    24704       max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, None, 128)    512         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_66 (Conv1D)              (None, None, 128)    49280       batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, None, 128)    512         conv1d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, None, 128)    0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_67 (Conv1D)              (None, None, 256)    98560       max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, None, 256)    1024        conv1d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_68 (Conv1D)              (None, None, 256)    196864      batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, None, 256)    1024        conv1d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_69 (Conv1D)              (None, None, 256)    196864      batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, None, 256)    1024        conv1d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, None, 256)    0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_70 (Conv1D)              (None, None, 512)    393728      max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, None, 512)    2048        conv1d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_71 (Conv1D)              (None, None, 512)    786944      batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, None, 512)    2048        conv1d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_72 (Conv1D)              (None, None, 512)    786944      batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, None, 512)    2048        conv1d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, None, 512)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_73 (Conv1D)              (None, None, 512)    786944      max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, None, 512)    2048        conv1d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_74 (Conv1D)              (None, None, 256)    393472      batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, None, 256)    1024        conv1d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_75 (Conv1D)              (None, None, 128)    98432       batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, None, 128)    512         conv1d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, None, 128)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_77 (Conv1D)              (None, None, 64)     8192        max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, None, 64)     256         conv1d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, None, 64)     0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_78 (Conv1D)              (None, None, 64)     12288       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, None, 64)     256         conv1d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, None, 64)     0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_79 (Conv1D)              (None, None, 256)    16384       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_76 (Conv1D)              (None, None, 256)    32768       max_pooling1d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, None, 256)    1024        conv1d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, None, 256)    1024        conv1d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, None, 256)    0           batch_normalization_79[0][0]     \n",
      "                                                                 batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, None, 256)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_bn_1 (BatchNor (None, None, 256)    1024        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_conv_1 (Conv1D (None, None, 9)      2304        attention_branch_bn_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_conv_1 (Co (None, None, 1)      9           attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_bn_1 (Batc (None, None, 1)      4           attention_branch_att_conv_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_sigmoid_1  (None, None, 1)      0           attention_branch_att_bn_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, None, 128)    0           max_pooling1d_19[0][0]           \n",
      "                                                                 attention_branch_att_sigmoid_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_81 (Conv1D)              (None, None, 64)     8192        lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, None, 64)     256         conv1d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, None, 64)     0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_82 (Conv1D)              (None, None, 64)     12288       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, None, 64)     256         conv1d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, None, 64)     0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_83 (Conv1D)              (None, None, 256)    16384       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_80 (Conv1D)              (None, None, 256)    32768       lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, None, 256)    1024        conv1d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, None, 256)    1024        conv1d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, None, 256)    0           batch_normalization_83[0][0]     \n",
      "                                                                 batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, None, 256)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_avgpool_1 (Gl (None, 256)          0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_1 (Dens (None, 512)          131584      perception_branch_avgpool_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_gap_1 (GlobalA (None, 9)            0           attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_2 (Dens (None, 9)            4617        perception_branch_dense_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_output (Activa (None, 9)            0           attention_branch_gap_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_output (Activ (None, 9)            0           perception_branch_dense_2[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 4,125,718\n",
      "Trainable params: 4,115,476\n",
      "Non-trainable params: 10,242\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 train_loss: 0.625 train_f1: 0.129 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1511: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 0 valid_f1: 0.046 best_f1: 0.046 mean accuracy:0.011 \t\n",
      "\n",
      "Epoch 1 train_loss: 0.541 train_f1: 0.188 \t\n",
      "\n",
      "Validation 1 valid_f1: 0.241 best_f1: 0.241 mean accuracy:0.120 \t\n",
      "\n",
      "Epoch 2 train_loss: 0.516 train_f1: 0.240 \t\n",
      "\n",
      "Validation 2 valid_f1: 0.256 best_f1: 0.256 mean accuracy:0.123 \t\n",
      "\n",
      "Epoch 3 train_loss: 0.502 train_f1: 0.255 \t\n",
      "\n",
      "Validation 3 valid_f1: 0.288 best_f1: 0.288 mean accuracy:0.149 \t\n",
      "\n",
      "Epoch 4 train_loss: 0.492 train_f1: 0.266 \t\n",
      "\n",
      "Validation 4 valid_f1: 0.299 best_f1: 0.299 mean accuracy:0.156 \t\n",
      "\n",
      "Epoch 5 train_loss: 0.484 train_f1: 0.279 \t\n",
      "\n",
      "Validation 5 valid_f1: 0.326 best_f1: 0.326 mean accuracy:0.181 \t\n",
      "\n",
      "Epoch 6 train_loss: 0.477 train_f1: 0.293 \t\n",
      "\n",
      "Validation 6 valid_f1: 0.362 best_f1: 0.362 mean accuracy:0.203 \t\n",
      "\n",
      "Epoch 7 train_loss: 0.470 train_f1: 0.313 \t\n",
      "\n",
      "Validation 7 valid_f1: 0.364 best_f1: 0.364 mean accuracy:0.214 \t\n",
      "\n",
      "Epoch 8 train_loss: 0.465 train_f1: 0.327 \t\n",
      "\n",
      "Validation 8 valid_f1: 0.410 best_f1: 0.410 mean accuracy:0.250 \t\n",
      "\n",
      "Epoch 9 train_loss: 0.460 train_f1: 0.341 \t\n",
      "\n",
      "Validation 9 valid_f1: 0.474 best_f1: 0.474 mean accuracy:0.290 \t\n",
      "\n",
      "Epoch 10 train_loss: 0.455 train_f1: 0.359 \t\n",
      "\n",
      "Validation 10 valid_f1: 0.492 best_f1: 0.492 mean accuracy:0.312 \t\n",
      "\n",
      "Epoch 11 train_loss: 0.450 train_f1: 0.372 \t\n",
      "\n",
      "Validation 11 valid_f1: 0.475 best_f1: 0.492 mean accuracy:0.301 \t\n",
      "\n",
      "Epoch 12 train_loss: 0.446 train_f1: 0.383 \t\n",
      "\n",
      "Validation 12 valid_f1: 0.530 best_f1: 0.530 mean accuracy:0.355 \t\n",
      "\n",
      "Epoch 13 train_loss: 0.442 train_f1: 0.394 \t\n",
      "\n",
      "Validation 13 valid_f1: 0.529 best_f1: 0.530 mean accuracy:0.355 \t\n",
      "\n",
      "Epoch 14 train_loss: 0.439 train_f1: 0.405 \t\n",
      "\n",
      "Validation 14 valid_f1: 0.527 best_f1: 0.530 mean accuracy:0.348 \t\n",
      "\n",
      "Epoch 15 train_loss: 0.436 train_f1: 0.414 \t\n",
      "\n",
      "Validation 15 valid_f1: 0.566 best_f1: 0.566 mean accuracy:0.388 \t\n",
      "\n",
      "Epoch 16 train_loss: 0.432 train_f1: 0.429 \t\n",
      "\n",
      "Validation 16 valid_f1: 0.597 best_f1: 0.597 mean accuracy:0.435 \t\n",
      "\n",
      "Epoch 17 train_loss: 0.429 train_f1: 0.440 \t\n",
      "\n",
      "Validation 17 valid_f1: 0.582 best_f1: 0.597 mean accuracy:0.413 \t\n",
      "\n",
      "Epoch 18 train_loss: 0.426 train_f1: 0.446 \t\n",
      "\n",
      "Validation 18 valid_f1: 0.618 best_f1: 0.618 mean accuracy:0.460 \t\n",
      "\n",
      "Epoch 19 train_loss: 0.423 train_f1: 0.459 \t\n",
      "\n",
      "Validation 19 valid_f1: 0.602 best_f1: 0.618 mean accuracy:0.431 \t\n",
      "\n",
      "Epoch 20 train_loss: 0.422 train_f1: 0.462 \t\n",
      "\n",
      "Validation 20 valid_f1: 0.633 best_f1: 0.633 mean accuracy:0.478 \t\n",
      "\n",
      "Epoch 21 train_loss: 0.420 train_f1: 0.469 \t\n",
      "\n",
      "Validation 21 valid_f1: 0.643 best_f1: 0.643 mean accuracy:0.489 \t\n",
      "\n",
      "Epoch 22 train_loss: 0.418 train_f1: 0.473 \t\n",
      "\n",
      "Validation 22 valid_f1: 0.649 best_f1: 0.649 mean accuracy:0.493 \t\n",
      "\n",
      "Epoch 23 train_loss: 0.415 train_f1: 0.481 \t\n",
      "\n",
      "Validation 23 valid_f1: 0.649 best_f1: 0.649 mean accuracy:0.493 \t\n",
      "\n",
      "Epoch 24 train_loss: 0.414 train_f1: 0.482 \t\n",
      "\n",
      "Validation 24 valid_f1: 0.671 best_f1: 0.671 mean accuracy:0.518 \t\n",
      "\n",
      "Epoch 25 train_loss: 0.412 train_f1: 0.488 \t\n",
      "\n",
      "Validation 25 valid_f1: 0.691 best_f1: 0.691 mean accuracy:0.547 \t\n",
      "\n",
      "Epoch 26 train_loss: 0.410 train_f1: 0.495 \t\n",
      "\n",
      "Validation 26 valid_f1: 0.685 best_f1: 0.691 mean accuracy:0.551 \t\n",
      "\n",
      "Epoch 27 train_loss: 0.409 train_f1: 0.496 \t\n",
      "\n",
      "Validation 27 valid_f1: 0.688 best_f1: 0.691 mean accuracy:0.554 \t\n",
      "\n",
      "Epoch 28 train_loss: 0.408 train_f1: 0.499 \t\n",
      "\n",
      "Validation 28 valid_f1: 0.700 best_f1: 0.700 mean accuracy:0.562 \t\n",
      "\n",
      "Epoch 29 train_loss: 0.405 train_f1: 0.505 \t\n",
      "\n",
      "Validation 29 valid_f1: 0.700 best_f1: 0.700 mean accuracy:0.558 \t\n",
      "\n",
      "Epoch 30 train_loss: 0.403 train_f1: 0.511 \t\n",
      "\n",
      "Validation 30 valid_f1: 0.684 best_f1: 0.700 mean accuracy:0.543 \t\n",
      "\n",
      "Epoch 31 train_loss: 0.402 train_f1: 0.512 \t\n",
      "\n",
      "Validation 31 valid_f1: 0.693 best_f1: 0.700 mean accuracy:0.569 \t\n",
      "\n",
      "Epoch 32 train_loss: 0.400 train_f1: 0.520 \t\n",
      "\n",
      "Validation 32 valid_f1: 0.693 best_f1: 0.700 mean accuracy:0.565 \t\n",
      "\n",
      "Epoch 33 train_loss: 0.400 train_f1: 0.519 \t\n",
      "\n",
      "Validation 33 valid_f1: 0.696 best_f1: 0.700 mean accuracy:0.558 \t\n",
      "\n",
      "Epoch 34 train_loss: 0.399 train_f1: 0.520 \t\n",
      "\n",
      "Validation 34 valid_f1: 0.701 best_f1: 0.701 mean accuracy:0.565 \t\n",
      "\n",
      "Epoch 35 train_loss: 0.396 train_f1: 0.528 \t\n",
      "\n",
      "Validation 35 valid_f1: 0.711 best_f1: 0.711 mean accuracy:0.591 \t\n",
      "\n",
      "Epoch 36 train_loss: 0.394 train_f1: 0.532 \t\n",
      "\n",
      "Validation 36 valid_f1: 0.706 best_f1: 0.711 mean accuracy:0.576 \t\n",
      "\n",
      "Epoch 37 train_loss: 0.394 train_f1: 0.534 \t\n",
      "\n",
      "Validation 37 valid_f1: 0.689 best_f1: 0.711 mean accuracy:0.572 \t\n",
      "\n",
      "Epoch 38 train_loss: 0.393 train_f1: 0.538 \t\n",
      "\n",
      "Validation 38 valid_f1: 0.726 best_f1: 0.726 mean accuracy:0.605 \t\n",
      "\n",
      "Epoch 39 train_loss: 0.392 train_f1: 0.538 \t\n",
      "\n",
      "Validation 39 valid_f1: 0.727 best_f1: 0.727 mean accuracy:0.594 \t\n",
      "\n",
      "Epoch 40 train_loss: 0.390 train_f1: 0.541 \t\n",
      "\n",
      "Validation 40 valid_f1: 0.716 best_f1: 0.727 mean accuracy:0.583 \t\n",
      "\n",
      "Epoch 41 train_loss: 0.389 train_f1: 0.546 \t\n",
      "\n",
      "Validation 41 valid_f1: 0.728 best_f1: 0.728 mean accuracy:0.598 \t\n",
      "\n",
      "Epoch 42 train_loss: 0.388 train_f1: 0.550 \t\n",
      "\n",
      "Validation 42 valid_f1: 0.721 best_f1: 0.728 mean accuracy:0.594 \t\n",
      "\n",
      "Epoch 43 train_loss: 0.387 train_f1: 0.550 \t\n",
      "\n",
      "Validation 43 valid_f1: 0.703 best_f1: 0.728 mean accuracy:0.587 \t\n",
      "\n",
      "Epoch 44 train_loss: 0.385 train_f1: 0.552 \t\n",
      "\n",
      "Validation 44 valid_f1: 0.728 best_f1: 0.728 mean accuracy:0.616 \t\n",
      "\n",
      "Epoch 45 train_loss: 0.384 train_f1: 0.555 \t\n",
      "\n",
      "Validation 45 valid_f1: 0.737 best_f1: 0.737 mean accuracy:0.605 \t\n",
      "\n",
      "Epoch 46 train_loss: 0.383 train_f1: 0.560 \t\n",
      "\n",
      "Validation 46 valid_f1: 0.716 best_f1: 0.737 mean accuracy:0.609 \t\n",
      "\n",
      "Epoch 47 train_loss: 0.381 train_f1: 0.563 \t\n",
      "\n",
      "Validation 47 valid_f1: 0.728 best_f1: 0.737 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 48 train_loss: 0.381 train_f1: 0.564 \t\n",
      "\n",
      "Validation 48 valid_f1: 0.721 best_f1: 0.737 mean accuracy:0.591 \t\n",
      "\n",
      "Epoch 49 train_loss: 0.380 train_f1: 0.568 \t\n",
      "\n",
      "Validation 49 valid_f1: 0.731 best_f1: 0.737 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 50 train_loss: 0.378 train_f1: 0.572 \t\n",
      "\n",
      "Validation 50 valid_f1: 0.662 best_f1: 0.737 mean accuracy:0.533 \t\n",
      "\n",
      "Epoch 51 train_loss: 0.377 train_f1: 0.575 \t\n",
      "\n",
      "Validation 51 valid_f1: 0.716 best_f1: 0.737 mean accuracy:0.591 \t\n",
      "\n",
      "Epoch 52 train_loss: 0.376 train_f1: 0.576 \t\n",
      "\n",
      "Validation 52 valid_f1: 0.704 best_f1: 0.737 mean accuracy:0.587 \t\n",
      "\n",
      "Epoch 53 train_loss: 0.376 train_f1: 0.577 \t\n",
      "\n",
      "Validation 53 valid_f1: 0.722 best_f1: 0.737 mean accuracy:0.605 \t\n",
      "\n",
      "Epoch 54 train_loss: 0.375 train_f1: 0.580 \t\n",
      "\n",
      "Validation 54 valid_f1: 0.689 best_f1: 0.737 mean accuracy:0.572 \t\n",
      "\n",
      "Epoch 55 train_loss: 0.374 train_f1: 0.585 \t\n",
      "\n",
      "Validation 55 valid_f1: 0.729 best_f1: 0.737 mean accuracy:0.601 \t\n",
      "\n",
      "Epoch 56 train_loss: 0.372 train_f1: 0.588 \t\n",
      "\n",
      "Validation 56 valid_f1: 0.734 best_f1: 0.737 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 57 train_loss: 0.370 train_f1: 0.591 \t\n",
      "\n",
      "Validation 57 valid_f1: 0.733 best_f1: 0.737 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 58 train_loss: 0.372 train_f1: 0.587 \t\n",
      "\n",
      "Validation 58 valid_f1: 0.713 best_f1: 0.737 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 59 train_loss: 0.369 train_f1: 0.595 \t\n",
      "\n",
      "Validation 59 valid_f1: 0.737 best_f1: 0.737 mean accuracy:0.616 \t\n",
      "\n",
      "Epoch 60 train_loss: 0.368 train_f1: 0.599 \t\n",
      "\n",
      "Validation 60 valid_f1: 0.741 best_f1: 0.741 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 61 train_loss: 0.367 train_f1: 0.603 \t\n",
      "\n",
      "Validation 61 valid_f1: 0.734 best_f1: 0.741 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 62 train_loss: 0.367 train_f1: 0.601 \t\n",
      "\n",
      "Validation 62 valid_f1: 0.745 best_f1: 0.745 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 63 train_loss: 0.366 train_f1: 0.602 \t\n",
      "\n",
      "Validation 63 valid_f1: 0.731 best_f1: 0.745 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 64 train_loss: 0.365 train_f1: 0.606 \t\n",
      "\n",
      "Validation 64 valid_f1: 0.756 best_f1: 0.756 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 65 train_loss: 0.364 train_f1: 0.606 \t\n",
      "\n",
      "Validation 65 valid_f1: 0.745 best_f1: 0.756 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 66 train_loss: 0.364 train_f1: 0.607 \t\n",
      "\n",
      "Validation 66 valid_f1: 0.709 best_f1: 0.756 mean accuracy:0.601 \t\n",
      "\n",
      "Epoch 67 train_loss: 0.363 train_f1: 0.610 \t\n",
      "\n",
      "Validation 67 valid_f1: 0.757 best_f1: 0.757 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 68 train_loss: 0.361 train_f1: 0.615 \t\n",
      "\n",
      "Validation 68 valid_f1: 0.746 best_f1: 0.757 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 69 train_loss: 0.360 train_f1: 0.619 \t\n",
      "\n",
      "Validation 69 valid_f1: 0.745 best_f1: 0.757 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 70 train_loss: 0.361 train_f1: 0.613 \t\n",
      "\n",
      "Validation 70 valid_f1: 0.731 best_f1: 0.757 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 71 train_loss: 0.359 train_f1: 0.621 \t\n",
      "\n",
      "Validation 71 valid_f1: 0.758 best_f1: 0.758 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 72 train_loss: 0.358 train_f1: 0.624 \t\n",
      "\n",
      "Validation 72 valid_f1: 0.731 best_f1: 0.758 mean accuracy:0.616 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73 train_loss: 0.357 train_f1: 0.625 \t\n",
      "\n",
      "Validation 73 valid_f1: 0.730 best_f1: 0.758 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 74 train_loss: 0.357 train_f1: 0.628 \t\n",
      "\n",
      "Validation 74 valid_f1: 0.723 best_f1: 0.758 mean accuracy:0.601 \t\n",
      "\n",
      "Epoch 75 train_loss: 0.356 train_f1: 0.631 \t\n",
      "\n",
      "Validation 75 valid_f1: 0.751 best_f1: 0.758 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 76 train_loss: 0.355 train_f1: 0.632 \t\n",
      "\n",
      "Validation 76 valid_f1: 0.750 best_f1: 0.758 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 77 train_loss: 0.355 train_f1: 0.634 \t\n",
      "\n",
      "Validation 77 valid_f1: 0.764 best_f1: 0.764 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 78 train_loss: 0.353 train_f1: 0.637 \t\n",
      "\n",
      "Validation 78 valid_f1: 0.722 best_f1: 0.764 mean accuracy:0.616 \t\n",
      "\n",
      "Epoch 79 train_loss: 0.354 train_f1: 0.637 \t\n",
      "\n",
      "Validation 79 valid_f1: 0.692 best_f1: 0.764 mean accuracy:0.569 \t\n",
      "\n",
      "Epoch 80 train_loss: 0.352 train_f1: 0.638 \t\n",
      "\n",
      "Validation 80 valid_f1: 0.727 best_f1: 0.764 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 81 train_loss: 0.351 train_f1: 0.643 \t\n",
      "\n",
      "Validation 81 valid_f1: 0.760 best_f1: 0.764 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 82 train_loss: 0.350 train_f1: 0.646 \t\n",
      "\n",
      "Validation 82 valid_f1: 0.725 best_f1: 0.764 mean accuracy:0.616 \t\n",
      "\n",
      "Epoch 83 train_loss: 0.350 train_f1: 0.643 \t\n",
      "\n",
      "Validation 83 valid_f1: 0.702 best_f1: 0.764 mean accuracy:0.594 \t\n",
      "\n",
      "Epoch 84 train_loss: 0.350 train_f1: 0.648 \t\n",
      "\n",
      "Validation 84 valid_f1: 0.700 best_f1: 0.764 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 85 train_loss: 0.349 train_f1: 0.649 \t\n",
      "\n",
      "Validation 85 valid_f1: 0.765 best_f1: 0.765 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 86 train_loss: 0.349 train_f1: 0.650 \t\n",
      "\n",
      "Validation 86 valid_f1: 0.735 best_f1: 0.765 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 87 train_loss: 0.348 train_f1: 0.651 \t\n",
      "\n",
      "Validation 87 valid_f1: 0.734 best_f1: 0.765 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 88 train_loss: 0.348 train_f1: 0.655 \t\n",
      "\n",
      "Validation 88 valid_f1: 0.755 best_f1: 0.765 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 89 train_loss: 0.347 train_f1: 0.656 \t\n",
      "\n",
      "Validation 89 valid_f1: 0.733 best_f1: 0.765 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 90 train_loss: 0.345 train_f1: 0.660 \t\n",
      "\n",
      "Validation 90 valid_f1: 0.746 best_f1: 0.765 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 91 train_loss: 0.345 train_f1: 0.657 \t\n",
      "\n",
      "Validation 91 valid_f1: 0.715 best_f1: 0.765 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 92 train_loss: 0.345 train_f1: 0.660 \t\n",
      "\n",
      "Validation 92 valid_f1: 0.738 best_f1: 0.765 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 93 train_loss: 0.343 train_f1: 0.663 \t\n",
      "\n",
      "Validation 93 valid_f1: 0.728 best_f1: 0.765 mean accuracy:0.605 \t\n",
      "\n",
      "Epoch 94 train_loss: 0.342 train_f1: 0.667 \t\n",
      "\n",
      "Validation 94 valid_f1: 0.736 best_f1: 0.765 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 95 train_loss: 0.342 train_f1: 0.668 \t\n",
      "\n",
      "Validation 95 valid_f1: 0.751 best_f1: 0.765 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 96 train_loss: 0.343 train_f1: 0.666 \t\n",
      "\n",
      "Validation 96 valid_f1: 0.750 best_f1: 0.765 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 97 train_loss: 0.344 train_f1: 0.662 \t\n",
      "\n",
      "Validation 97 valid_f1: 0.736 best_f1: 0.765 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 98 train_loss: 0.342 train_f1: 0.667 \t\n",
      "\n",
      "Validation 98 valid_f1: 0.723 best_f1: 0.765 mean accuracy:0.616 \t\n",
      "\n",
      "Epoch 99 train_loss: 0.342 train_f1: 0.671 \t\n",
      "\n",
      "Validation 99 valid_f1: 0.759 best_f1: 0.765 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 100 train_loss: 0.340 train_f1: 0.672 \t\n",
      "\n",
      "Validation 100 valid_f1: 0.721 best_f1: 0.765 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 101 train_loss: 0.339 train_f1: 0.679 \t\n",
      "\n",
      "Validation 101 valid_f1: 0.756 best_f1: 0.765 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 102 train_loss: 0.339 train_f1: 0.680 \t\n",
      "\n",
      "Validation 102 valid_f1: 0.719 best_f1: 0.765 mean accuracy:0.598 \t\n",
      "\n",
      "Epoch 103 train_loss: 0.339 train_f1: 0.677 \t\n",
      "\n",
      "Validation 103 valid_f1: 0.744 best_f1: 0.765 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 104 train_loss: 0.341 train_f1: 0.673 \t\n",
      "\n",
      "Validation 104 valid_f1: 0.703 best_f1: 0.765 mean accuracy:0.580 \t\n",
      "\n",
      "Epoch 105 train_loss: 0.340 train_f1: 0.676 \t\n",
      "\n",
      "Validation 105 valid_f1: 0.746 best_f1: 0.765 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 106 train_loss: 0.338 train_f1: 0.680 \t\n",
      "\n",
      "Validation 106 valid_f1: 0.728 best_f1: 0.765 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 107 train_loss: 0.337 train_f1: 0.685 \t\n",
      "\n",
      "Validation 107 valid_f1: 0.736 best_f1: 0.765 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 108 train_loss: 0.337 train_f1: 0.687 \t\n",
      "\n",
      "Validation 108 valid_f1: 0.761 best_f1: 0.765 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 109 train_loss: 0.336 train_f1: 0.687 \t\n",
      "\n",
      "Validation 109 valid_f1: 0.760 best_f1: 0.765 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 110 train_loss: 0.336 train_f1: 0.689 \t\n",
      "\n",
      "Validation 110 valid_f1: 0.765 best_f1: 0.765 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 111 train_loss: 0.337 train_f1: 0.684 \t\n",
      "\n",
      "Validation 111 valid_f1: 0.758 best_f1: 0.765 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 112 train_loss: 0.336 train_f1: 0.688 \t\n",
      "\n",
      "Validation 112 valid_f1: 0.746 best_f1: 0.765 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 113 train_loss: 0.336 train_f1: 0.692 \t\n",
      "\n",
      "Validation 113 valid_f1: 0.740 best_f1: 0.765 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 114 train_loss: 0.335 train_f1: 0.694 \t\n",
      "\n",
      "Validation 114 valid_f1: 0.750 best_f1: 0.765 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 115 train_loss: 0.335 train_f1: 0.693 \t\n",
      "\n",
      "Validation 115 valid_f1: 0.764 best_f1: 0.765 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 116 train_loss: 0.334 train_f1: 0.696 \t\n",
      "\n",
      "Validation 116 valid_f1: 0.769 best_f1: 0.769 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 117 train_loss: 0.334 train_f1: 0.696 \t\n",
      "\n",
      "Validation 117 valid_f1: 0.743 best_f1: 0.769 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 118 train_loss: 0.332 train_f1: 0.702 \t\n",
      "\n",
      "Validation 118 valid_f1: 0.773 best_f1: 0.773 mean accuracy:0.696 \t\n",
      "\n",
      "Epoch 119 train_loss: 0.332 train_f1: 0.700 \t\n",
      "\n",
      "Validation 119 valid_f1: 0.772 best_f1: 0.773 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 120 train_loss: 0.332 train_f1: 0.702 \t\n",
      "\n",
      "Validation 120 valid_f1: 0.747 best_f1: 0.773 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 121 train_loss: 0.333 train_f1: 0.697 \t\n",
      "\n",
      "Validation 121 valid_f1: 0.749 best_f1: 0.773 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 122 train_loss: 0.334 train_f1: 0.695 \t\n",
      "\n",
      "Validation 122 valid_f1: 0.762 best_f1: 0.773 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 123 train_loss: 0.332 train_f1: 0.703 \t\n",
      "\n",
      "Validation 123 valid_f1: 0.748 best_f1: 0.773 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 124 train_loss: 0.333 train_f1: 0.702 \t\n",
      "\n",
      "Validation 124 valid_f1: 0.773 best_f1: 0.773 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 125 train_loss: 0.333 train_f1: 0.699 \t\n",
      "\n",
      "Validation 125 valid_f1: 0.750 best_f1: 0.773 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 126 train_loss: 0.332 train_f1: 0.701 \t\n",
      "\n",
      "Validation 126 valid_f1: 0.768 best_f1: 0.773 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 127 train_loss: 0.331 train_f1: 0.708 \t\n",
      "\n",
      "Validation 127 valid_f1: 0.747 best_f1: 0.773 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 128 train_loss: 0.331 train_f1: 0.703 \t\n",
      "\n",
      "Validation 128 valid_f1: 0.761 best_f1: 0.773 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 129 train_loss: 0.329 train_f1: 0.708 \t\n",
      "\n",
      "Validation 129 valid_f1: 0.752 best_f1: 0.773 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 130 train_loss: 0.331 train_f1: 0.708 \t\n",
      "\n",
      "Validation 130 valid_f1: 0.758 best_f1: 0.773 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 131 train_loss: 0.331 train_f1: 0.708 \t\n",
      "\n",
      "Validation 131 valid_f1: 0.744 best_f1: 0.773 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 132 train_loss: 0.329 train_f1: 0.709 \t\n",
      "\n",
      "Validation 132 valid_f1: 0.725 best_f1: 0.773 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 133 train_loss: 0.328 train_f1: 0.712 \t\n",
      "\n",
      "Validation 133 valid_f1: 0.760 best_f1: 0.773 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 134 train_loss: 0.328 train_f1: 0.710 \t\n",
      "\n",
      "Validation 134 valid_f1: 0.738 best_f1: 0.773 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 135 train_loss: 0.329 train_f1: 0.707 \t\n",
      "\n",
      "Validation 135 valid_f1: 0.778 best_f1: 0.778 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 136 train_loss: 0.328 train_f1: 0.714 \t\n",
      "\n",
      "Validation 136 valid_f1: 0.750 best_f1: 0.778 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 137 train_loss: 0.329 train_f1: 0.709 \t\n",
      "\n",
      "Validation 137 valid_f1: 0.746 best_f1: 0.778 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 138 train_loss: 0.328 train_f1: 0.711 \t\n",
      "\n",
      "Validation 138 valid_f1: 0.749 best_f1: 0.778 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 139 train_loss: 0.328 train_f1: 0.713 \t\n",
      "\n",
      "Validation 139 valid_f1: 0.768 best_f1: 0.778 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 140 train_loss: 0.328 train_f1: 0.713 \t\n",
      "\n",
      "Validation 140 valid_f1: 0.746 best_f1: 0.778 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 141 train_loss: 0.328 train_f1: 0.716 \t\n",
      "\n",
      "Validation 141 valid_f1: 0.749 best_f1: 0.778 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 142 train_loss: 0.327 train_f1: 0.717 \t\n",
      "\n",
      "Validation 142 valid_f1: 0.737 best_f1: 0.778 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 143 train_loss: 0.331 train_f1: 0.709 \t\n",
      "\n",
      "Validation 143 valid_f1: 0.759 best_f1: 0.778 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 144 train_loss: 0.328 train_f1: 0.716 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 144 valid_f1: 0.766 best_f1: 0.778 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 145 train_loss: 0.326 train_f1: 0.722 \t\n",
      "\n",
      "Validation 145 valid_f1: 0.768 best_f1: 0.778 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 146 train_loss: 0.327 train_f1: 0.723 \t\n",
      "\n",
      "Validation 146 valid_f1: 0.783 best_f1: 0.783 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 147 train_loss: 0.327 train_f1: 0.721 \t\n",
      "\n",
      "Validation 147 valid_f1: 0.756 best_f1: 0.783 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 148 train_loss: 0.326 train_f1: 0.722 \t\n",
      "\n",
      "Validation 148 valid_f1: 0.768 best_f1: 0.783 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 149 train_loss: 0.327 train_f1: 0.720 \t\n",
      "\n",
      "Validation 149 valid_f1: 0.739 best_f1: 0.783 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 150 train_loss: 0.327 train_f1: 0.722 \t\n",
      "\n",
      "Validation 150 valid_f1: 0.778 best_f1: 0.783 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 151 train_loss: 0.324 train_f1: 0.728 \t\n",
      "\n",
      "Validation 151 valid_f1: 0.748 best_f1: 0.783 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 152 train_loss: 0.325 train_f1: 0.724 \t\n",
      "\n",
      "Validation 152 valid_f1: 0.731 best_f1: 0.783 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 153 train_loss: 0.326 train_f1: 0.720 \t\n",
      "\n",
      "Validation 153 valid_f1: 0.750 best_f1: 0.783 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 154 train_loss: 0.326 train_f1: 0.726 \t\n",
      "\n",
      "Validation 154 valid_f1: 0.692 best_f1: 0.783 mean accuracy:0.558 \t\n",
      "\n",
      "Epoch 155 train_loss: 0.326 train_f1: 0.727 \t\n",
      "\n",
      "Validation 155 valid_f1: 0.733 best_f1: 0.783 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 156 train_loss: 0.325 train_f1: 0.730 \t\n",
      "\n",
      "Validation 156 valid_f1: 0.760 best_f1: 0.783 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 157 train_loss: 0.324 train_f1: 0.727 \t\n",
      "\n",
      "Validation 157 valid_f1: 0.763 best_f1: 0.783 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 158 train_loss: 0.324 train_f1: 0.727 \t\n",
      "\n",
      "Validation 158 valid_f1: 0.753 best_f1: 0.783 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 159 train_loss: 0.325 train_f1: 0.727 \t\n",
      "\n",
      "Validation 159 valid_f1: 0.768 best_f1: 0.783 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 160 train_loss: 0.325 train_f1: 0.732 \t\n",
      "\n",
      "Validation 160 valid_f1: 0.757 best_f1: 0.783 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 161 train_loss: 0.326 train_f1: 0.727 \t\n",
      "\n",
      "Validation 161 valid_f1: 0.744 best_f1: 0.783 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 162 train_loss: 0.325 train_f1: 0.727 \t\n",
      "\n",
      "Validation 162 valid_f1: 0.760 best_f1: 0.783 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 163 train_loss: 0.323 train_f1: 0.735 \t\n",
      "\n",
      "Validation 163 valid_f1: 0.765 best_f1: 0.783 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 164 train_loss: 0.324 train_f1: 0.734 \t\n",
      "\n",
      "Validation 164 valid_f1: 0.765 best_f1: 0.783 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 165 train_loss: 0.323 train_f1: 0.737 \t\n",
      "\n",
      "Validation 165 valid_f1: 0.729 best_f1: 0.783 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 166 train_loss: 0.323 train_f1: 0.733 \t\n",
      "\n",
      "Validation 166 valid_f1: 0.744 best_f1: 0.783 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 167 train_loss: 0.324 train_f1: 0.731 \t\n",
      "\n",
      "Validation 167 valid_f1: 0.772 best_f1: 0.783 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 168 train_loss: 0.322 train_f1: 0.739 \t\n",
      "\n",
      "Validation 168 valid_f1: 0.772 best_f1: 0.783 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 169 train_loss: 0.323 train_f1: 0.735 \t\n",
      "\n",
      "Validation 169 valid_f1: 0.777 best_f1: 0.783 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 170 train_loss: 0.322 train_f1: 0.739 \t\n",
      "\n",
      "Validation 170 valid_f1: 0.744 best_f1: 0.783 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 171 train_loss: 0.323 train_f1: 0.735 \t\n",
      "\n",
      "Validation 171 valid_f1: 0.771 best_f1: 0.783 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 172 train_loss: 0.325 train_f1: 0.730 \t\n",
      "\n",
      "Validation 172 valid_f1: 0.703 best_f1: 0.783 mean accuracy:0.594 \t\n",
      "\n",
      "Epoch 173 train_loss: 0.324 train_f1: 0.738 \t\n",
      "\n",
      "Validation 173 valid_f1: 0.773 best_f1: 0.783 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 174 train_loss: 0.325 train_f1: 0.733 \t\n",
      "\n",
      "Validation 174 valid_f1: 0.744 best_f1: 0.783 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 175 train_loss: 0.322 train_f1: 0.737 \t\n",
      "\n",
      "Validation 175 valid_f1: 0.732 best_f1: 0.783 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 176 train_loss: 0.322 train_f1: 0.736 \t\n",
      "\n",
      "Validation 176 valid_f1: 0.760 best_f1: 0.783 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 177 train_loss: 0.322 train_f1: 0.741 \t\n",
      "\n",
      "Validation 177 valid_f1: 0.758 best_f1: 0.783 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 178 train_loss: 0.324 train_f1: 0.739 \t\n",
      "\n",
      "Validation 178 valid_f1: 0.686 best_f1: 0.783 mean accuracy:0.591 \t\n",
      "\n",
      "Epoch 179 train_loss: 0.323 train_f1: 0.745 \t\n",
      "\n",
      "Validation 179 valid_f1: 0.764 best_f1: 0.783 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 180 train_loss: 0.321 train_f1: 0.743 \t\n",
      "\n",
      "Validation 180 valid_f1: 0.757 best_f1: 0.783 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 181 train_loss: 0.322 train_f1: 0.738 \t\n",
      "\n",
      "Validation 181 valid_f1: 0.721 best_f1: 0.783 mean accuracy:0.616 \t\n",
      "\n",
      "Epoch 182 train_loss: 0.321 train_f1: 0.745 \t\n",
      "\n",
      "Validation 182 valid_f1: 0.770 best_f1: 0.783 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 183 train_loss: 0.320 train_f1: 0.751 \t\n",
      "\n",
      "Validation 183 valid_f1: 0.735 best_f1: 0.783 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 184 train_loss: 0.323 train_f1: 0.743 \t\n",
      "\n",
      "Validation 184 valid_f1: 0.765 best_f1: 0.783 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 185 train_loss: 0.322 train_f1: 0.751 \t\n",
      "\n",
      "Validation 185 valid_f1: 0.786 best_f1: 0.786 mean accuracy:0.710 \t\n",
      "\n",
      "Epoch 186 train_loss: 0.321 train_f1: 0.747 \t\n",
      "\n",
      "Validation 186 valid_f1: 0.765 best_f1: 0.786 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 187 train_loss: 0.321 train_f1: 0.750 \t\n",
      "\n",
      "Validation 187 valid_f1: 0.716 best_f1: 0.786 mean accuracy:0.616 \t\n",
      "\n",
      "Epoch 188 train_loss: 0.321 train_f1: 0.753 \t\n",
      "\n",
      "Validation 188 valid_f1: 0.726 best_f1: 0.786 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 189 train_loss: 0.320 train_f1: 0.748 \t\n",
      "\n",
      "Validation 189 valid_f1: 0.753 best_f1: 0.786 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 190 train_loss: 0.320 train_f1: 0.751 \t\n",
      "\n",
      "Validation 190 valid_f1: 0.756 best_f1: 0.786 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 191 train_loss: 0.319 train_f1: 0.752 \t\n",
      "\n",
      "Validation 191 valid_f1: 0.760 best_f1: 0.786 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 192 train_loss: 0.319 train_f1: 0.751 \t\n",
      "\n",
      "Validation 192 valid_f1: 0.770 best_f1: 0.786 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 193 train_loss: 0.322 train_f1: 0.747 \t\n",
      "\n",
      "Validation 193 valid_f1: 0.740 best_f1: 0.786 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 194 train_loss: 0.320 train_f1: 0.750 \t\n",
      "\n",
      "Validation 194 valid_f1: 0.745 best_f1: 0.786 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 195 train_loss: 0.321 train_f1: 0.751 \t\n",
      "\n",
      "Validation 195 valid_f1: 0.750 best_f1: 0.786 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 196 train_loss: 0.320 train_f1: 0.754 \t\n",
      "\n",
      "Validation 196 valid_f1: 0.761 best_f1: 0.786 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 197 train_loss: 0.321 train_f1: 0.750 \t\n",
      "\n",
      "Validation 197 valid_f1: 0.772 best_f1: 0.786 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 198 train_loss: 0.319 train_f1: 0.758 \t\n",
      "\n",
      "Validation 198 valid_f1: 0.752 best_f1: 0.786 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 199 train_loss: 0.321 train_f1: 0.754 \t\n",
      "\n",
      "Validation 199 valid_f1: 0.760 best_f1: 0.786 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 200 train_loss: 0.319 train_f1: 0.758 \t\n",
      "\n",
      "Validation 200 valid_f1: 0.753 best_f1: 0.786 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 201 train_loss: 0.319 train_f1: 0.759 \t\n",
      "\n",
      "Validation 201 valid_f1: 0.774 best_f1: 0.786 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 202 train_loss: 0.320 train_f1: 0.751 \t\n",
      "\n",
      "Validation 202 valid_f1: 0.744 best_f1: 0.786 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 203 train_loss: 0.318 train_f1: 0.756 \t\n",
      "\n",
      "Validation 203 valid_f1: 0.761 best_f1: 0.786 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 204 train_loss: 0.318 train_f1: 0.760 \t\n",
      "\n",
      "Validation 204 valid_f1: 0.755 best_f1: 0.786 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 205 train_loss: 0.319 train_f1: 0.758 \t\n",
      "\n",
      "Validation 205 valid_f1: 0.765 best_f1: 0.786 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 206 train_loss: 0.318 train_f1: 0.763 \t\n",
      "\n",
      "Validation 206 valid_f1: 0.702 best_f1: 0.786 mean accuracy:0.554 \t\n",
      "\n",
      "Epoch 207 train_loss: 0.320 train_f1: 0.763 \t\n",
      "\n",
      "Validation 207 valid_f1: 0.778 best_f1: 0.786 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 208 train_loss: 0.319 train_f1: 0.761 \t\n",
      "\n",
      "Validation 208 valid_f1: 0.748 best_f1: 0.786 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 209 train_loss: 0.319 train_f1: 0.760 \t\n",
      "\n",
      "Validation 209 valid_f1: 0.761 best_f1: 0.786 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 210 train_loss: 0.320 train_f1: 0.758 \t\n",
      "\n",
      "Validation 210 valid_f1: 0.726 best_f1: 0.786 mean accuracy:0.609 \t\n",
      "\n",
      "Epoch 211 train_loss: 0.321 train_f1: 0.762 \t\n",
      "\n",
      "Validation 211 valid_f1: 0.761 best_f1: 0.786 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 212 train_loss: 0.319 train_f1: 0.763 \t\n",
      "\n",
      "Validation 212 valid_f1: 0.758 best_f1: 0.786 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 213 train_loss: 0.318 train_f1: 0.764 \t\n",
      "\n",
      "Validation 213 valid_f1: 0.775 best_f1: 0.786 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 214 train_loss: 0.318 train_f1: 0.766 \t\n",
      "\n",
      "Validation 214 valid_f1: 0.763 best_f1: 0.786 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 215 train_loss: 0.316 train_f1: 0.766 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 215 valid_f1: 0.768 best_f1: 0.786 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 216 train_loss: 0.318 train_f1: 0.767 \t\n",
      "\n",
      "Validation 216 valid_f1: 0.761 best_f1: 0.786 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 217 train_loss: 0.318 train_f1: 0.768 \t\n",
      "\n",
      "Validation 217 valid_f1: 0.725 best_f1: 0.786 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 218 train_loss: 0.317 train_f1: 0.770 \t\n",
      "\n",
      "Validation 218 valid_f1: 0.755 best_f1: 0.786 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 219 train_loss: 0.318 train_f1: 0.763 \t\n",
      "\n",
      "Validation 219 valid_f1: 0.745 best_f1: 0.786 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 220 train_loss: 0.319 train_f1: 0.765 \t\n",
      "\n",
      "Validation 220 valid_f1: 0.761 best_f1: 0.786 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 221 train_loss: 0.322 train_f1: 0.760 \t\n",
      "\n",
      "Validation 221 valid_f1: 0.777 best_f1: 0.786 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 222 train_loss: 0.318 train_f1: 0.775 \t\n",
      "\n",
      "Validation 222 valid_f1: 0.786 best_f1: 0.786 mean accuracy:0.696 \t\n",
      "\n",
      "Epoch 223 train_loss: 0.316 train_f1: 0.781 \t\n",
      "\n",
      "Validation 223 valid_f1: 0.760 best_f1: 0.786 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 224 train_loss: 0.316 train_f1: 0.778 \t\n",
      "\n",
      "Validation 224 valid_f1: 0.751 best_f1: 0.786 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 225 train_loss: 0.316 train_f1: 0.777 \t\n",
      "\n",
      "Validation 225 valid_f1: 0.765 best_f1: 0.786 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 226 train_loss: 0.316 train_f1: 0.774 \t\n",
      "\n",
      "Validation 226 valid_f1: 0.759 best_f1: 0.786 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 227 train_loss: 0.316 train_f1: 0.776 \t\n",
      "\n",
      "Validation 227 valid_f1: 0.746 best_f1: 0.786 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 228 train_loss: 0.316 train_f1: 0.778 \t\n",
      "\n",
      "Validation 228 valid_f1: 0.733 best_f1: 0.786 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 229 train_loss: 0.320 train_f1: 0.762 \t\n",
      "\n",
      "Validation 229 valid_f1: 0.733 best_f1: 0.786 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 230 train_loss: 0.319 train_f1: 0.766 \t\n",
      "\n",
      "Validation 230 valid_f1: 0.782 best_f1: 0.786 mean accuracy:0.692 \t\n",
      "\n",
      "Epoch 231 train_loss: 0.316 train_f1: 0.777 \t\n",
      "\n",
      "Validation 231 valid_f1: 0.748 best_f1: 0.786 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 232 train_loss: 0.317 train_f1: 0.777 \t\n",
      "\n",
      "Validation 232 valid_f1: 0.755 best_f1: 0.786 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 233 train_loss: 0.318 train_f1: 0.772 \t\n",
      "\n",
      "Validation 233 valid_f1: 0.777 best_f1: 0.786 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 234 train_loss: 0.316 train_f1: 0.783 \t\n",
      "\n",
      "Validation 234 valid_f1: 0.756 best_f1: 0.786 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 235 train_loss: 0.315 train_f1: 0.784 \t\n",
      "\n",
      "Validation 235 valid_f1: 0.734 best_f1: 0.786 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 236 train_loss: 0.318 train_f1: 0.778 \t\n",
      "Early stopped training due to non-improved performance\n",
      "2020-07-25 18:42:44.543887\n",
      "../results_20200724_e2e_ABN_zero_gamma10\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, 12)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_84 (Conv1D)              (None, None, 64)     2368        input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, None, 64)     256         conv1d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_85 (Conv1D)              (None, None, 64)     12352       batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, None, 64)     256         conv1d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, None, 64)     0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_86 (Conv1D)              (None, None, 128)    24704       max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, None, 128)    512         conv1d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_87 (Conv1D)              (None, None, 128)    49280       batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, None, 128)    512         conv1d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling1D) (None, None, 128)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_88 (Conv1D)              (None, None, 256)    98560       max_pooling1d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, None, 256)    1024        conv1d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_89 (Conv1D)              (None, None, 256)    196864      batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, None, 256)    1024        conv1d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_90 (Conv1D)              (None, None, 256)    196864      batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, None, 256)    1024        conv1d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, None, 256)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_91 (Conv1D)              (None, None, 512)    393728      max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, None, 512)    2048        conv1d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_92 (Conv1D)              (None, None, 512)    786944      batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, None, 512)    2048        conv1d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_93 (Conv1D)              (None, None, 512)    786944      batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, None, 512)    2048        conv1d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, None, 512)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_94 (Conv1D)              (None, None, 512)    786944      max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, None, 512)    2048        conv1d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_95 (Conv1D)              (None, None, 256)    393472      batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, None, 256)    1024        conv1d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_96 (Conv1D)              (None, None, 128)    98432       batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, None, 128)    512         conv1d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, None, 128)    0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_98 (Conv1D)              (None, None, 64)     8192        max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, None, 64)     256         conv1d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, None, 64)     0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_99 (Conv1D)              (None, None, 64)     12288       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, None, 64)     256         conv1d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, None, 64)     0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_100 (Conv1D)             (None, None, 256)    16384       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_97 (Conv1D)              (None, None, 256)    32768       max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, None, 256)    1024        conv1d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, None, 256)    1024        conv1d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, None, 256)    0           batch_normalization_100[0][0]    \n",
      "                                                                 batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, None, 256)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_bn_1 (BatchNor (None, None, 256)    1024        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_conv_1 (Conv1D (None, None, 9)      2304        attention_branch_bn_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_conv_1 (Co (None, None, 1)      9           attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_bn_1 (Batc (None, None, 1)      4           attention_branch_att_conv_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_sigmoid_1  (None, None, 1)      0           attention_branch_att_bn_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, None, 128)    0           max_pooling1d_24[0][0]           \n",
      "                                                                 attention_branch_att_sigmoid_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_102 (Conv1D)             (None, None, 64)     8192        lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, None, 64)     256         conv1d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, None, 64)     0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_103 (Conv1D)             (None, None, 64)     12288       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, None, 64)     256         conv1d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, None, 64)     0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_104 (Conv1D)             (None, None, 256)    16384       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_101 (Conv1D)             (None, None, 256)    32768       lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, None, 256)    1024        conv1d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, None, 256)    1024        conv1d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, None, 256)    0           batch_normalization_104[0][0]    \n",
      "                                                                 batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, None, 256)    0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_avgpool_1 (Gl (None, 256)          0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_1 (Dens (None, 512)          131584      perception_branch_avgpool_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_gap_1 (GlobalA (None, 9)            0           attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_2 (Dens (None, 9)            4617        perception_branch_dense_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_output (Activa (None, 9)            0           attention_branch_gap_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_output (Activ (None, 9)            0           perception_branch_dense_2[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 4,125,718\n",
      "Trainable params: 4,115,476\n",
      "Non-trainable params: 10,242\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 train_loss: 0.625 train_f1: 0.156 \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1511: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 0 valid_f1: 0.026 best_f1: 0.026 mean accuracy:0.004 \t\n",
      "\n",
      "Epoch 1 train_loss: 0.544 train_f1: 0.168 \t\n",
      "\n",
      "Validation 1 valid_f1: 0.210 best_f1: 0.210 mean accuracy:0.101 \t\n",
      "\n",
      "Epoch 2 train_loss: 0.519 train_f1: 0.219 \t\n",
      "\n",
      "Validation 2 valid_f1: 0.265 best_f1: 0.265 mean accuracy:0.134 \t\n",
      "\n",
      "Epoch 3 train_loss: 0.505 train_f1: 0.245 \t\n",
      "\n",
      "Validation 3 valid_f1: 0.282 best_f1: 0.282 mean accuracy:0.145 \t\n",
      "\n",
      "Epoch 4 train_loss: 0.495 train_f1: 0.259 \t\n",
      "\n",
      "Validation 4 valid_f1: 0.281 best_f1: 0.282 mean accuracy:0.145 \t\n",
      "\n",
      "Epoch 5 train_loss: 0.486 train_f1: 0.276 \t\n",
      "\n",
      "Validation 5 valid_f1: 0.305 best_f1: 0.305 mean accuracy:0.167 \t\n",
      "\n",
      "Epoch 6 train_loss: 0.480 train_f1: 0.288 \t\n",
      "\n",
      "Validation 6 valid_f1: 0.340 best_f1: 0.340 mean accuracy:0.196 \t\n",
      "\n",
      "Epoch 7 train_loss: 0.473 train_f1: 0.304 \t\n",
      "\n",
      "Validation 7 valid_f1: 0.372 best_f1: 0.372 mean accuracy:0.217 \t\n",
      "\n",
      "Epoch 8 train_loss: 0.467 train_f1: 0.321 \t\n",
      "\n",
      "Validation 8 valid_f1: 0.402 best_f1: 0.402 mean accuracy:0.246 \t\n",
      "\n",
      "Epoch 9 train_loss: 0.462 train_f1: 0.333 \t\n",
      "\n",
      "Validation 9 valid_f1: 0.426 best_f1: 0.426 mean accuracy:0.261 \t\n",
      "\n",
      "Epoch 10 train_loss: 0.457 train_f1: 0.343 \t\n",
      "\n",
      "Validation 10 valid_f1: 0.478 best_f1: 0.478 mean accuracy:0.293 \t\n",
      "\n",
      "Epoch 11 train_loss: 0.453 train_f1: 0.353 \t\n",
      "\n",
      "Validation 11 valid_f1: 0.506 best_f1: 0.506 mean accuracy:0.326 \t\n",
      "\n",
      "Epoch 12 train_loss: 0.449 train_f1: 0.367 \t\n",
      "\n",
      "Validation 12 valid_f1: 0.519 best_f1: 0.519 mean accuracy:0.337 \t\n",
      "\n",
      "Epoch 13 train_loss: 0.444 train_f1: 0.376 \t\n",
      "\n",
      "Validation 13 valid_f1: 0.508 best_f1: 0.519 mean accuracy:0.330 \t\n",
      "\n",
      "Epoch 14 train_loss: 0.441 train_f1: 0.391 \t\n",
      "\n",
      "Validation 14 valid_f1: 0.543 best_f1: 0.543 mean accuracy:0.359 \t\n",
      "\n",
      "Epoch 15 train_loss: 0.438 train_f1: 0.405 \t\n",
      "\n",
      "Validation 15 valid_f1: 0.550 best_f1: 0.550 mean accuracy:0.370 \t\n",
      "\n",
      "Epoch 16 train_loss: 0.434 train_f1: 0.414 \t\n",
      "\n",
      "Validation 16 valid_f1: 0.559 best_f1: 0.559 mean accuracy:0.380 \t\n",
      "\n",
      "Epoch 17 train_loss: 0.431 train_f1: 0.429 \t\n",
      "\n",
      "Validation 17 valid_f1: 0.602 best_f1: 0.602 mean accuracy:0.424 \t\n",
      "\n",
      "Epoch 18 train_loss: 0.427 train_f1: 0.440 \t\n",
      "\n",
      "Validation 18 valid_f1: 0.613 best_f1: 0.613 mean accuracy:0.442 \t\n",
      "\n",
      "Epoch 19 train_loss: 0.425 train_f1: 0.449 \t\n",
      "\n",
      "Validation 19 valid_f1: 0.606 best_f1: 0.613 mean accuracy:0.435 \t\n",
      "\n",
      "Epoch 20 train_loss: 0.422 train_f1: 0.456 \t\n",
      "\n",
      "Validation 20 valid_f1: 0.616 best_f1: 0.616 mean accuracy:0.457 \t\n",
      "\n",
      "Epoch 21 train_loss: 0.418 train_f1: 0.468 \t\n",
      "\n",
      "Validation 21 valid_f1: 0.637 best_f1: 0.637 mean accuracy:0.486 \t\n",
      "\n",
      "Epoch 22 train_loss: 0.418 train_f1: 0.470 \t\n",
      "\n",
      "Validation 22 valid_f1: 0.660 best_f1: 0.660 mean accuracy:0.496 \t\n",
      "\n",
      "Epoch 23 train_loss: 0.415 train_f1: 0.475 \t\n",
      "\n",
      "Validation 23 valid_f1: 0.661 best_f1: 0.661 mean accuracy:0.493 \t\n",
      "\n",
      "Epoch 24 train_loss: 0.414 train_f1: 0.478 \t\n",
      "\n",
      "Validation 24 valid_f1: 0.682 best_f1: 0.682 mean accuracy:0.533 \t\n",
      "\n",
      "Epoch 25 train_loss: 0.412 train_f1: 0.486 \t\n",
      "\n",
      "Validation 25 valid_f1: 0.674 best_f1: 0.682 mean accuracy:0.504 \t\n",
      "\n",
      "Epoch 26 train_loss: 0.410 train_f1: 0.491 \t\n",
      "\n",
      "Validation 26 valid_f1: 0.673 best_f1: 0.682 mean accuracy:0.525 \t\n",
      "\n",
      "Epoch 27 train_loss: 0.408 train_f1: 0.496 \t\n",
      "\n",
      "Validation 27 valid_f1: 0.692 best_f1: 0.692 mean accuracy:0.547 \t\n",
      "\n",
      "Epoch 28 train_loss: 0.405 train_f1: 0.500 \t\n",
      "\n",
      "Validation 28 valid_f1: 0.693 best_f1: 0.693 mean accuracy:0.543 \t\n",
      "\n",
      "Epoch 29 train_loss: 0.405 train_f1: 0.503 \t\n",
      "\n",
      "Validation 29 valid_f1: 0.623 best_f1: 0.693 mean accuracy:0.475 \t\n",
      "\n",
      "Epoch 30 train_loss: 0.402 train_f1: 0.509 \t\n",
      "\n",
      "Validation 30 valid_f1: 0.687 best_f1: 0.693 mean accuracy:0.525 \t\n",
      "\n",
      "Epoch 31 train_loss: 0.401 train_f1: 0.511 \t\n",
      "\n",
      "Validation 31 valid_f1: 0.698 best_f1: 0.698 mean accuracy:0.562 \t\n",
      "\n",
      "Epoch 32 train_loss: 0.399 train_f1: 0.517 \t\n",
      "\n",
      "Validation 32 valid_f1: 0.659 best_f1: 0.698 mean accuracy:0.504 \t\n",
      "\n",
      "Epoch 33 train_loss: 0.398 train_f1: 0.519 \t\n",
      "\n",
      "Validation 33 valid_f1: 0.693 best_f1: 0.698 mean accuracy:0.551 \t\n",
      "\n",
      "Epoch 34 train_loss: 0.396 train_f1: 0.521 \t\n",
      "\n",
      "Validation 34 valid_f1: 0.707 best_f1: 0.707 mean accuracy:0.562 \t\n",
      "\n",
      "Epoch 35 train_loss: 0.397 train_f1: 0.520 \t\n",
      "\n",
      "Validation 35 valid_f1: 0.700 best_f1: 0.707 mean accuracy:0.554 \t\n",
      "\n",
      "Epoch 36 train_loss: 0.394 train_f1: 0.529 \t\n",
      "\n",
      "Validation 36 valid_f1: 0.709 best_f1: 0.709 mean accuracy:0.598 \t\n",
      "\n",
      "Epoch 37 train_loss: 0.393 train_f1: 0.531 \t\n",
      "\n",
      "Validation 37 valid_f1: 0.712 best_f1: 0.712 mean accuracy:0.580 \t\n",
      "\n",
      "Epoch 38 train_loss: 0.391 train_f1: 0.538 \t\n",
      "\n",
      "Validation 38 valid_f1: 0.732 best_f1: 0.732 mean accuracy:0.616 \t\n",
      "\n",
      "Epoch 39 train_loss: 0.391 train_f1: 0.538 \t\n",
      "\n",
      "Validation 39 valid_f1: 0.679 best_f1: 0.732 mean accuracy:0.529 \t\n",
      "\n",
      "Epoch 40 train_loss: 0.389 train_f1: 0.542 \t\n",
      "\n",
      "Validation 40 valid_f1: 0.705 best_f1: 0.732 mean accuracy:0.576 \t\n",
      "\n",
      "Epoch 41 train_loss: 0.388 train_f1: 0.545 \t\n",
      "\n",
      "Validation 41 valid_f1: 0.701 best_f1: 0.732 mean accuracy:0.576 \t\n",
      "\n",
      "Epoch 42 train_loss: 0.387 train_f1: 0.546 \t\n",
      "\n",
      "Validation 42 valid_f1: 0.716 best_f1: 0.732 mean accuracy:0.598 \t\n",
      "\n",
      "Epoch 43 train_loss: 0.385 train_f1: 0.550 \t\n",
      "\n",
      "Validation 43 valid_f1: 0.711 best_f1: 0.732 mean accuracy:0.569 \t\n",
      "\n",
      "Epoch 44 train_loss: 0.384 train_f1: 0.555 \t\n",
      "\n",
      "Validation 44 valid_f1: 0.714 best_f1: 0.732 mean accuracy:0.572 \t\n",
      "\n",
      "Epoch 45 train_loss: 0.382 train_f1: 0.559 \t\n",
      "\n",
      "Validation 45 valid_f1: 0.713 best_f1: 0.732 mean accuracy:0.601 \t\n",
      "\n",
      "Epoch 46 train_loss: 0.382 train_f1: 0.561 \t\n",
      "\n",
      "Validation 46 valid_f1: 0.701 best_f1: 0.732 mean accuracy:0.601 \t\n",
      "\n",
      "Epoch 47 train_loss: 0.380 train_f1: 0.564 \t\n",
      "\n",
      "Validation 47 valid_f1: 0.735 best_f1: 0.735 mean accuracy:0.601 \t\n",
      "\n",
      "Epoch 48 train_loss: 0.380 train_f1: 0.564 \t\n",
      "\n",
      "Validation 48 valid_f1: 0.723 best_f1: 0.735 mean accuracy:0.609 \t\n",
      "\n",
      "Epoch 49 train_loss: 0.378 train_f1: 0.570 \t\n",
      "\n",
      "Validation 49 valid_f1: 0.698 best_f1: 0.735 mean accuracy:0.565 \t\n",
      "\n",
      "Epoch 50 train_loss: 0.377 train_f1: 0.572 \t\n",
      "\n",
      "Validation 50 valid_f1: 0.723 best_f1: 0.735 mean accuracy:0.594 \t\n",
      "\n",
      "Epoch 51 train_loss: 0.376 train_f1: 0.577 \t\n",
      "\n",
      "Validation 51 valid_f1: 0.711 best_f1: 0.735 mean accuracy:0.565 \t\n",
      "\n",
      "Epoch 52 train_loss: 0.374 train_f1: 0.579 \t\n",
      "\n",
      "Validation 52 valid_f1: 0.709 best_f1: 0.735 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 53 train_loss: 0.372 train_f1: 0.582 \t\n",
      "\n",
      "Validation 53 valid_f1: 0.698 best_f1: 0.735 mean accuracy:0.594 \t\n",
      "\n",
      "Epoch 54 train_loss: 0.372 train_f1: 0.584 \t\n",
      "\n",
      "Validation 54 valid_f1: 0.721 best_f1: 0.735 mean accuracy:0.587 \t\n",
      "\n",
      "Epoch 55 train_loss: 0.372 train_f1: 0.582 \t\n",
      "\n",
      "Validation 55 valid_f1: 0.736 best_f1: 0.736 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 56 train_loss: 0.371 train_f1: 0.591 \t\n",
      "\n",
      "Validation 56 valid_f1: 0.716 best_f1: 0.736 mean accuracy:0.594 \t\n",
      "\n",
      "Epoch 57 train_loss: 0.370 train_f1: 0.593 \t\n",
      "\n",
      "Validation 57 valid_f1: 0.749 best_f1: 0.749 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 58 train_loss: 0.369 train_f1: 0.595 \t\n",
      "\n",
      "Validation 58 valid_f1: 0.731 best_f1: 0.749 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 59 train_loss: 0.368 train_f1: 0.595 \t\n",
      "\n",
      "Validation 59 valid_f1: 0.740 best_f1: 0.749 mean accuracy:0.601 \t\n",
      "\n",
      "Epoch 60 train_loss: 0.367 train_f1: 0.598 \t\n",
      "\n",
      "Validation 60 valid_f1: 0.734 best_f1: 0.749 mean accuracy:0.601 \t\n",
      "\n",
      "Epoch 61 train_loss: 0.366 train_f1: 0.602 \t\n",
      "\n",
      "Validation 61 valid_f1: 0.715 best_f1: 0.749 mean accuracy:0.572 \t\n",
      "\n",
      "Epoch 62 train_loss: 0.364 train_f1: 0.603 \t\n",
      "\n",
      "Validation 62 valid_f1: 0.736 best_f1: 0.749 mean accuracy:0.605 \t\n",
      "\n",
      "Epoch 63 train_loss: 0.364 train_f1: 0.607 \t\n",
      "\n",
      "Validation 63 valid_f1: 0.690 best_f1: 0.749 mean accuracy:0.558 \t\n",
      "\n",
      "Epoch 64 train_loss: 0.363 train_f1: 0.608 \t\n",
      "\n",
      "Validation 64 valid_f1: 0.725 best_f1: 0.749 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 65 train_loss: 0.362 train_f1: 0.613 \t\n",
      "\n",
      "Validation 65 valid_f1: 0.733 best_f1: 0.749 mean accuracy:0.616 \t\n",
      "\n",
      "Epoch 66 train_loss: 0.362 train_f1: 0.613 \t\n",
      "\n",
      "Validation 66 valid_f1: 0.704 best_f1: 0.749 mean accuracy:0.583 \t\n",
      "\n",
      "Epoch 67 train_loss: 0.360 train_f1: 0.618 \t\n",
      "\n",
      "Validation 67 valid_f1: 0.743 best_f1: 0.749 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 68 train_loss: 0.360 train_f1: 0.617 \t\n",
      "\n",
      "Validation 68 valid_f1: 0.657 best_f1: 0.749 mean accuracy:0.536 \t\n",
      "\n",
      "Epoch 69 train_loss: 0.360 train_f1: 0.617 \t\n",
      "\n",
      "Validation 69 valid_f1: 0.726 best_f1: 0.749 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 70 train_loss: 0.358 train_f1: 0.624 \t\n",
      "\n",
      "Validation 70 valid_f1: 0.721 best_f1: 0.749 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 71 train_loss: 0.358 train_f1: 0.621 \t\n",
      "\n",
      "Validation 71 valid_f1: 0.733 best_f1: 0.749 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 72 train_loss: 0.356 train_f1: 0.628 \t\n",
      "\n",
      "Validation 72 valid_f1: 0.714 best_f1: 0.749 mean accuracy:0.580 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73 train_loss: 0.355 train_f1: 0.630 \t\n",
      "\n",
      "Validation 73 valid_f1: 0.717 best_f1: 0.749 mean accuracy:0.612 \t\n",
      "\n",
      "Epoch 74 train_loss: 0.354 train_f1: 0.636 \t\n",
      "\n",
      "Validation 74 valid_f1: 0.726 best_f1: 0.749 mean accuracy:0.616 \t\n",
      "\n",
      "Epoch 75 train_loss: 0.354 train_f1: 0.633 \t\n",
      "\n",
      "Validation 75 valid_f1: 0.746 best_f1: 0.749 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 76 train_loss: 0.354 train_f1: 0.634 \t\n",
      "\n",
      "Validation 76 valid_f1: 0.736 best_f1: 0.749 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 77 train_loss: 0.353 train_f1: 0.640 \t\n",
      "\n",
      "Validation 77 valid_f1: 0.734 best_f1: 0.749 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 78 train_loss: 0.352 train_f1: 0.639 \t\n",
      "\n",
      "Validation 78 valid_f1: 0.759 best_f1: 0.759 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 79 train_loss: 0.352 train_f1: 0.641 \t\n",
      "\n",
      "Validation 79 valid_f1: 0.753 best_f1: 0.759 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 80 train_loss: 0.351 train_f1: 0.640 \t\n",
      "\n",
      "Validation 80 valid_f1: 0.759 best_f1: 0.759 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 81 train_loss: 0.350 train_f1: 0.644 \t\n",
      "\n",
      "Validation 81 valid_f1: 0.753 best_f1: 0.759 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 82 train_loss: 0.349 train_f1: 0.646 \t\n",
      "\n",
      "Validation 82 valid_f1: 0.757 best_f1: 0.759 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 83 train_loss: 0.349 train_f1: 0.645 \t\n",
      "\n",
      "Validation 83 valid_f1: 0.752 best_f1: 0.759 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 84 train_loss: 0.349 train_f1: 0.649 \t\n",
      "\n",
      "Validation 84 valid_f1: 0.760 best_f1: 0.760 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 85 train_loss: 0.349 train_f1: 0.647 \t\n",
      "\n",
      "Validation 85 valid_f1: 0.740 best_f1: 0.760 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 86 train_loss: 0.348 train_f1: 0.653 \t\n",
      "\n",
      "Validation 86 valid_f1: 0.764 best_f1: 0.764 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 87 train_loss: 0.347 train_f1: 0.653 \t\n",
      "\n",
      "Validation 87 valid_f1: 0.668 best_f1: 0.764 mean accuracy:0.533 \t\n",
      "\n",
      "Epoch 88 train_loss: 0.347 train_f1: 0.655 \t\n",
      "\n",
      "Validation 88 valid_f1: 0.772 best_f1: 0.772 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 89 train_loss: 0.345 train_f1: 0.659 \t\n",
      "\n",
      "Validation 89 valid_f1: 0.750 best_f1: 0.772 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 90 train_loss: 0.346 train_f1: 0.660 \t\n",
      "\n",
      "Validation 90 valid_f1: 0.758 best_f1: 0.772 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 91 train_loss: 0.345 train_f1: 0.661 \t\n",
      "\n",
      "Validation 91 valid_f1: 0.733 best_f1: 0.772 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 92 train_loss: 0.345 train_f1: 0.665 \t\n",
      "\n",
      "Validation 92 valid_f1: 0.738 best_f1: 0.772 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 93 train_loss: 0.343 train_f1: 0.665 \t\n",
      "\n",
      "Validation 93 valid_f1: 0.750 best_f1: 0.772 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 94 train_loss: 0.343 train_f1: 0.667 \t\n",
      "\n",
      "Validation 94 valid_f1: 0.744 best_f1: 0.772 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 95 train_loss: 0.342 train_f1: 0.671 \t\n",
      "\n",
      "Validation 95 valid_f1: 0.728 best_f1: 0.772 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 96 train_loss: 0.342 train_f1: 0.668 \t\n",
      "\n",
      "Validation 96 valid_f1: 0.749 best_f1: 0.772 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 97 train_loss: 0.341 train_f1: 0.674 \t\n",
      "\n",
      "Validation 97 valid_f1: 0.747 best_f1: 0.772 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 98 train_loss: 0.341 train_f1: 0.671 \t\n",
      "\n",
      "Validation 98 valid_f1: 0.705 best_f1: 0.772 mean accuracy:0.569 \t\n",
      "\n",
      "Epoch 99 train_loss: 0.341 train_f1: 0.669 \t\n",
      "\n",
      "Validation 99 valid_f1: 0.761 best_f1: 0.772 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 100 train_loss: 0.342 train_f1: 0.672 \t\n",
      "\n",
      "Validation 100 valid_f1: 0.738 best_f1: 0.772 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 101 train_loss: 0.341 train_f1: 0.675 \t\n",
      "\n",
      "Validation 101 valid_f1: 0.749 best_f1: 0.772 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 102 train_loss: 0.338 train_f1: 0.680 \t\n",
      "\n",
      "Validation 102 valid_f1: 0.771 best_f1: 0.772 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 103 train_loss: 0.340 train_f1: 0.677 \t\n",
      "\n",
      "Validation 103 valid_f1: 0.722 best_f1: 0.772 mean accuracy:0.616 \t\n",
      "\n",
      "Epoch 104 train_loss: 0.339 train_f1: 0.678 \t\n",
      "\n",
      "Validation 104 valid_f1: 0.750 best_f1: 0.772 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 105 train_loss: 0.338 train_f1: 0.681 \t\n",
      "\n",
      "Validation 105 valid_f1: 0.768 best_f1: 0.772 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 106 train_loss: 0.337 train_f1: 0.686 \t\n",
      "\n",
      "Validation 106 valid_f1: 0.754 best_f1: 0.772 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 107 train_loss: 0.337 train_f1: 0.683 \t\n",
      "\n",
      "Validation 107 valid_f1: 0.762 best_f1: 0.772 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 108 train_loss: 0.338 train_f1: 0.683 \t\n",
      "\n",
      "Validation 108 valid_f1: 0.745 best_f1: 0.772 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 109 train_loss: 0.339 train_f1: 0.685 \t\n",
      "\n",
      "Validation 109 valid_f1: 0.727 best_f1: 0.772 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 110 train_loss: 0.336 train_f1: 0.692 \t\n",
      "\n",
      "Validation 110 valid_f1: 0.727 best_f1: 0.772 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 111 train_loss: 0.336 train_f1: 0.689 \t\n",
      "\n",
      "Validation 111 valid_f1: 0.773 best_f1: 0.773 mean accuracy:0.681 \t\n",
      "\n",
      "Epoch 112 train_loss: 0.335 train_f1: 0.692 \t\n",
      "\n",
      "Validation 112 valid_f1: 0.766 best_f1: 0.773 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 113 train_loss: 0.337 train_f1: 0.689 \t\n",
      "\n",
      "Validation 113 valid_f1: 0.711 best_f1: 0.773 mean accuracy:0.609 \t\n",
      "\n",
      "Epoch 114 train_loss: 0.335 train_f1: 0.691 \t\n",
      "\n",
      "Validation 114 valid_f1: 0.747 best_f1: 0.773 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 115 train_loss: 0.335 train_f1: 0.696 \t\n",
      "\n",
      "Validation 115 valid_f1: 0.741 best_f1: 0.773 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 116 train_loss: 0.335 train_f1: 0.694 \t\n",
      "\n",
      "Validation 116 valid_f1: 0.758 best_f1: 0.773 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 117 train_loss: 0.334 train_f1: 0.697 \t\n",
      "\n",
      "Validation 117 valid_f1: 0.710 best_f1: 0.773 mean accuracy:0.609 \t\n",
      "\n",
      "Epoch 118 train_loss: 0.333 train_f1: 0.695 \t\n",
      "\n",
      "Validation 118 valid_f1: 0.752 best_f1: 0.773 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 119 train_loss: 0.333 train_f1: 0.699 \t\n",
      "\n",
      "Validation 119 valid_f1: 0.766 best_f1: 0.773 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 120 train_loss: 0.332 train_f1: 0.698 \t\n",
      "\n",
      "Validation 120 valid_f1: 0.775 best_f1: 0.775 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 121 train_loss: 0.333 train_f1: 0.699 \t\n",
      "\n",
      "Validation 121 valid_f1: 0.775 best_f1: 0.775 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 122 train_loss: 0.331 train_f1: 0.703 \t\n",
      "\n",
      "Validation 122 valid_f1: 0.748 best_f1: 0.775 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 123 train_loss: 0.332 train_f1: 0.701 \t\n",
      "\n",
      "Validation 123 valid_f1: 0.750 best_f1: 0.775 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 124 train_loss: 0.334 train_f1: 0.696 \t\n",
      "\n",
      "Validation 124 valid_f1: 0.736 best_f1: 0.775 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 125 train_loss: 0.331 train_f1: 0.706 \t\n",
      "\n",
      "Validation 125 valid_f1: 0.758 best_f1: 0.775 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 126 train_loss: 0.331 train_f1: 0.708 \t\n",
      "\n",
      "Validation 126 valid_f1: 0.763 best_f1: 0.775 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 127 train_loss: 0.331 train_f1: 0.709 \t\n",
      "\n",
      "Validation 127 valid_f1: 0.744 best_f1: 0.775 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 128 train_loss: 0.332 train_f1: 0.708 \t\n",
      "\n",
      "Validation 128 valid_f1: 0.765 best_f1: 0.775 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 129 train_loss: 0.330 train_f1: 0.710 \t\n",
      "\n",
      "Validation 129 valid_f1: 0.764 best_f1: 0.775 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 130 train_loss: 0.331 train_f1: 0.708 \t\n",
      "\n",
      "Validation 130 valid_f1: 0.763 best_f1: 0.775 mean accuracy:0.688 \t\n",
      "\n",
      "Epoch 131 train_loss: 0.331 train_f1: 0.711 \t\n",
      "\n",
      "Validation 131 valid_f1: 0.738 best_f1: 0.775 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 132 train_loss: 0.332 train_f1: 0.706 \t\n",
      "\n",
      "Validation 132 valid_f1: 0.747 best_f1: 0.775 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 133 train_loss: 0.330 train_f1: 0.712 \t\n",
      "\n",
      "Validation 133 valid_f1: 0.772 best_f1: 0.775 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 134 train_loss: 0.329 train_f1: 0.714 \t\n",
      "\n",
      "Validation 134 valid_f1: 0.762 best_f1: 0.775 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 135 train_loss: 0.330 train_f1: 0.711 \t\n",
      "\n",
      "Validation 135 valid_f1: 0.751 best_f1: 0.775 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 136 train_loss: 0.329 train_f1: 0.717 \t\n",
      "\n",
      "Validation 136 valid_f1: 0.774 best_f1: 0.775 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 137 train_loss: 0.329 train_f1: 0.718 \t\n",
      "\n",
      "Validation 137 valid_f1: 0.765 best_f1: 0.775 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 138 train_loss: 0.329 train_f1: 0.716 \t\n",
      "\n",
      "Validation 138 valid_f1: 0.753 best_f1: 0.775 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 139 train_loss: 0.329 train_f1: 0.716 \t\n",
      "\n",
      "Validation 139 valid_f1: 0.733 best_f1: 0.775 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 140 train_loss: 0.328 train_f1: 0.719 \t\n",
      "\n",
      "Validation 140 valid_f1: 0.763 best_f1: 0.775 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 141 train_loss: 0.328 train_f1: 0.721 \t\n",
      "\n",
      "Validation 141 valid_f1: 0.754 best_f1: 0.775 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 142 train_loss: 0.328 train_f1: 0.721 \t\n",
      "\n",
      "Validation 142 valid_f1: 0.752 best_f1: 0.775 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 143 train_loss: 0.328 train_f1: 0.720 \t\n",
      "\n",
      "Validation 143 valid_f1: 0.748 best_f1: 0.775 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 144 train_loss: 0.329 train_f1: 0.719 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation 144 valid_f1: 0.753 best_f1: 0.775 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 145 train_loss: 0.328 train_f1: 0.724 \t\n",
      "\n",
      "Validation 145 valid_f1: 0.743 best_f1: 0.775 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 146 train_loss: 0.327 train_f1: 0.722 \t\n",
      "\n",
      "Validation 146 valid_f1: 0.747 best_f1: 0.775 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 147 train_loss: 0.330 train_f1: 0.719 \t\n",
      "\n",
      "Validation 147 valid_f1: 0.741 best_f1: 0.775 mean accuracy:0.630 \t\n",
      "\n",
      "Epoch 148 train_loss: 0.327 train_f1: 0.721 \t\n",
      "\n",
      "Validation 148 valid_f1: 0.776 best_f1: 0.776 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 149 train_loss: 0.326 train_f1: 0.727 \t\n",
      "\n",
      "Validation 149 valid_f1: 0.738 best_f1: 0.776 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 150 train_loss: 0.326 train_f1: 0.728 \t\n",
      "\n",
      "Validation 150 valid_f1: 0.747 best_f1: 0.776 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 151 train_loss: 0.326 train_f1: 0.730 \t\n",
      "\n",
      "Validation 151 valid_f1: 0.766 best_f1: 0.776 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 152 train_loss: 0.327 train_f1: 0.723 \t\n",
      "\n",
      "Validation 152 valid_f1: 0.782 best_f1: 0.782 mean accuracy:0.685 \t\n",
      "\n",
      "Epoch 153 train_loss: 0.326 train_f1: 0.727 \t\n",
      "\n",
      "Validation 153 valid_f1: 0.739 best_f1: 0.782 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 154 train_loss: 0.326 train_f1: 0.730 \t\n",
      "\n",
      "Validation 154 valid_f1: 0.754 best_f1: 0.782 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 155 train_loss: 0.326 train_f1: 0.734 \t\n",
      "\n",
      "Validation 155 valid_f1: 0.744 best_f1: 0.782 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 156 train_loss: 0.326 train_f1: 0.733 \t\n",
      "\n",
      "Validation 156 valid_f1: 0.741 best_f1: 0.782 mean accuracy:0.638 \t\n",
      "\n",
      "Epoch 157 train_loss: 0.326 train_f1: 0.728 \t\n",
      "\n",
      "Validation 157 valid_f1: 0.761 best_f1: 0.782 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 158 train_loss: 0.326 train_f1: 0.737 \t\n",
      "\n",
      "Validation 158 valid_f1: 0.758 best_f1: 0.782 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 159 train_loss: 0.325 train_f1: 0.736 \t\n",
      "\n",
      "Validation 159 valid_f1: 0.762 best_f1: 0.782 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 160 train_loss: 0.324 train_f1: 0.737 \t\n",
      "\n",
      "Validation 160 valid_f1: 0.748 best_f1: 0.782 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 161 train_loss: 0.326 train_f1: 0.734 \t\n",
      "\n",
      "Validation 161 valid_f1: 0.774 best_f1: 0.782 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 162 train_loss: 0.324 train_f1: 0.741 \t\n",
      "\n",
      "Validation 162 valid_f1: 0.758 best_f1: 0.782 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 163 train_loss: 0.324 train_f1: 0.741 \t\n",
      "\n",
      "Validation 163 valid_f1: 0.771 best_f1: 0.782 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 164 train_loss: 0.327 train_f1: 0.734 \t\n",
      "\n",
      "Validation 164 valid_f1: 0.752 best_f1: 0.782 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 165 train_loss: 0.325 train_f1: 0.738 \t\n",
      "\n",
      "Validation 165 valid_f1: 0.744 best_f1: 0.782 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 166 train_loss: 0.325 train_f1: 0.739 \t\n",
      "\n",
      "Validation 166 valid_f1: 0.760 best_f1: 0.782 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 167 train_loss: 0.324 train_f1: 0.740 \t\n",
      "\n",
      "Validation 167 valid_f1: 0.765 best_f1: 0.782 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 168 train_loss: 0.324 train_f1: 0.744 \t\n",
      "\n",
      "Validation 168 valid_f1: 0.751 best_f1: 0.782 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 169 train_loss: 0.323 train_f1: 0.747 \t\n",
      "\n",
      "Validation 169 valid_f1: 0.749 best_f1: 0.782 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 170 train_loss: 0.323 train_f1: 0.744 \t\n",
      "\n",
      "Validation 170 valid_f1: 0.749 best_f1: 0.782 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 171 train_loss: 0.323 train_f1: 0.745 \t\n",
      "\n",
      "Validation 171 valid_f1: 0.765 best_f1: 0.782 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 172 train_loss: 0.324 train_f1: 0.742 \t\n",
      "\n",
      "Validation 172 valid_f1: 0.734 best_f1: 0.782 mean accuracy:0.620 \t\n",
      "\n",
      "Epoch 173 train_loss: 0.324 train_f1: 0.745 \t\n",
      "\n",
      "Validation 173 valid_f1: 0.755 best_f1: 0.782 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 174 train_loss: 0.323 train_f1: 0.752 \t\n",
      "\n",
      "Validation 174 valid_f1: 0.760 best_f1: 0.782 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 175 train_loss: 0.323 train_f1: 0.748 \t\n",
      "\n",
      "Validation 175 valid_f1: 0.772 best_f1: 0.782 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 176 train_loss: 0.322 train_f1: 0.751 \t\n",
      "\n",
      "Validation 176 valid_f1: 0.762 best_f1: 0.782 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 177 train_loss: 0.322 train_f1: 0.751 \t\n",
      "\n",
      "Validation 177 valid_f1: 0.766 best_f1: 0.782 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 178 train_loss: 0.322 train_f1: 0.754 \t\n",
      "\n",
      "Validation 178 valid_f1: 0.743 best_f1: 0.782 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 179 train_loss: 0.323 train_f1: 0.755 \t\n",
      "\n",
      "Validation 179 valid_f1: 0.759 best_f1: 0.782 mean accuracy:0.674 \t\n",
      "\n",
      "Epoch 180 train_loss: 0.322 train_f1: 0.752 \t\n",
      "\n",
      "Validation 180 valid_f1: 0.747 best_f1: 0.782 mean accuracy:0.649 \t\n",
      "\n",
      "Epoch 181 train_loss: 0.323 train_f1: 0.751 \t\n",
      "\n",
      "Validation 181 valid_f1: 0.762 best_f1: 0.782 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 182 train_loss: 0.323 train_f1: 0.750 \t\n",
      "\n",
      "Validation 182 valid_f1: 0.779 best_f1: 0.782 mean accuracy:0.678 \t\n",
      "\n",
      "Epoch 183 train_loss: 0.321 train_f1: 0.760 \t\n",
      "\n",
      "Validation 183 valid_f1: 0.753 best_f1: 0.782 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 184 train_loss: 0.321 train_f1: 0.761 \t\n",
      "\n",
      "Validation 184 valid_f1: 0.765 best_f1: 0.782 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 185 train_loss: 0.321 train_f1: 0.759 \t\n",
      "\n",
      "Validation 185 valid_f1: 0.771 best_f1: 0.782 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 186 train_loss: 0.323 train_f1: 0.755 \t\n",
      "\n",
      "Validation 186 valid_f1: 0.750 best_f1: 0.782 mean accuracy:0.627 \t\n",
      "\n",
      "Epoch 187 train_loss: 0.324 train_f1: 0.757 \t\n",
      "\n",
      "Validation 187 valid_f1: 0.764 best_f1: 0.782 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 188 train_loss: 0.326 train_f1: 0.747 \t\n",
      "\n",
      "Validation 188 valid_f1: 0.727 best_f1: 0.782 mean accuracy:0.623 \t\n",
      "\n",
      "Epoch 189 train_loss: 0.321 train_f1: 0.761 \t\n",
      "\n",
      "Validation 189 valid_f1: 0.761 best_f1: 0.782 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 190 train_loss: 0.322 train_f1: 0.757 \t\n",
      "\n",
      "Validation 190 valid_f1: 0.759 best_f1: 0.782 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 191 train_loss: 0.320 train_f1: 0.766 \t\n",
      "\n",
      "Validation 191 valid_f1: 0.768 best_f1: 0.782 mean accuracy:0.663 \t\n",
      "\n",
      "Epoch 192 train_loss: 0.319 train_f1: 0.769 \t\n",
      "\n",
      "Validation 192 valid_f1: 0.754 best_f1: 0.782 mean accuracy:0.670 \t\n",
      "\n",
      "Epoch 193 train_loss: 0.320 train_f1: 0.770 \t\n",
      "\n",
      "Validation 193 valid_f1: 0.752 best_f1: 0.782 mean accuracy:0.645 \t\n",
      "\n",
      "Epoch 194 train_loss: 0.320 train_f1: 0.774 \t\n",
      "\n",
      "Validation 194 valid_f1: 0.730 best_f1: 0.782 mean accuracy:0.634 \t\n",
      "\n",
      "Epoch 195 train_loss: 0.320 train_f1: 0.770 \t\n",
      "\n",
      "Validation 195 valid_f1: 0.761 best_f1: 0.782 mean accuracy:0.659 \t\n",
      "\n",
      "Epoch 196 train_loss: 0.320 train_f1: 0.768 \t\n",
      "\n",
      "Validation 196 valid_f1: 0.746 best_f1: 0.782 mean accuracy:0.641 \t\n",
      "\n",
      "Epoch 197 train_loss: 0.321 train_f1: 0.771 \t\n",
      "\n",
      "Validation 197 valid_f1: 0.768 best_f1: 0.782 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 198 train_loss: 0.321 train_f1: 0.765 \t\n",
      "\n",
      "Validation 198 valid_f1: 0.760 best_f1: 0.782 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 199 train_loss: 0.321 train_f1: 0.768 \t\n",
      "\n",
      "Validation 199 valid_f1: 0.765 best_f1: 0.782 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 200 train_loss: 0.322 train_f1: 0.769 \t\n",
      "\n",
      "Validation 200 valid_f1: 0.747 best_f1: 0.782 mean accuracy:0.652 \t\n",
      "\n",
      "Epoch 201 train_loss: 0.322 train_f1: 0.762 \t\n",
      "\n",
      "Validation 201 valid_f1: 0.763 best_f1: 0.782 mean accuracy:0.656 \t\n",
      "\n",
      "Epoch 202 train_loss: 0.320 train_f1: 0.777 \t\n",
      "\n",
      "Validation 202 valid_f1: 0.754 best_f1: 0.782 mean accuracy:0.667 \t\n",
      "\n",
      "Epoch 203 train_loss: 0.320 train_f1: 0.770 \t\n",
      "Early stopped training due to non-improved performance\n",
      "2020-07-26 03:07:09.729540\n",
      "../results_20200724_e2e_ABN_zero_gamma100\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, 12)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_105 (Conv1D)             (None, None, 64)     2368        input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, None, 64)     256         conv1d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_106 (Conv1D)             (None, None, 64)     12352       batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, None, 64)     256         conv1d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, None, 64)     0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_107 (Conv1D)             (None, None, 128)    24704       max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, None, 128)    512         conv1d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_108 (Conv1D)             (None, None, 128)    49280       batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, None, 128)    512         conv1d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, None, 128)    0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_109 (Conv1D)             (None, None, 256)    98560       max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, None, 256)    1024        conv1d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_110 (Conv1D)             (None, None, 256)    196864      batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, None, 256)    1024        conv1d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_111 (Conv1D)             (None, None, 256)    196864      batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, None, 256)    1024        conv1d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, None, 256)    0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_112 (Conv1D)             (None, None, 512)    393728      max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, None, 512)    2048        conv1d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_113 (Conv1D)             (None, None, 512)    786944      batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, None, 512)    2048        conv1d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_114 (Conv1D)             (None, None, 512)    786944      batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, None, 512)    2048        conv1d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling1D) (None, None, 512)    0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_115 (Conv1D)             (None, None, 512)    786944      max_pooling1d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, None, 512)    2048        conv1d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_116 (Conv1D)             (None, None, 256)    393472      batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, None, 256)    1024        conv1d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_117 (Conv1D)             (None, None, 128)    98432       batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, None, 128)    512         conv1d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling1D) (None, None, 128)    0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_119 (Conv1D)             (None, None, 64)     8192        max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, None, 64)     256         conv1d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, None, 64)     0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_120 (Conv1D)             (None, None, 64)     12288       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, None, 64)     256         conv1d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, None, 64)     0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_121 (Conv1D)             (None, None, 256)    16384       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_118 (Conv1D)             (None, None, 256)    32768       max_pooling1d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, None, 256)    1024        conv1d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, None, 256)    1024        conv1d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, None, 256)    0           batch_normalization_121[0][0]    \n",
      "                                                                 batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, None, 256)    0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_bn_1 (BatchNor (None, None, 256)    1024        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_conv_1 (Conv1D (None, None, 9)      2304        attention_branch_bn_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_conv_1 (Co (None, None, 1)      9           attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_bn_1 (Batc (None, None, 1)      4           attention_branch_att_conv_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_att_sigmoid_1  (None, None, 1)      0           attention_branch_att_bn_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, None, 128)    0           max_pooling1d_29[0][0]           \n",
      "                                                                 attention_branch_att_sigmoid_1[0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d_123 (Conv1D)             (None, None, 64)     8192        lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, None, 64)     256         conv1d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, None, 64)     0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_124 (Conv1D)             (None, None, 64)     12288       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, None, 64)     256         conv1d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, None, 64)     0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_125 (Conv1D)             (None, None, 256)    16384       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_122 (Conv1D)             (None, None, 256)    32768       lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, None, 256)    1024        conv1d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, None, 256)    1024        conv1d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, None, 256)    0           batch_normalization_125[0][0]    \n",
      "                                                                 batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, None, 256)    0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_avgpool_1 (Gl (None, 256)          0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_1 (Dens (None, 512)          131584      perception_branch_avgpool_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_gap_1 (GlobalA (None, 9)            0           attention_branch_conv_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_dense_2 (Dens (None, 9)            4617        perception_branch_dense_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "attention_branch_output (Activa (None, 9)            0           attention_branch_gap_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "perception_branch_output (Activ (None, 9)            0           perception_branch_dense_2[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 4,125,718\n",
      "Trainable params: 4,115,476\n",
      "Non-trainable params: 10,242\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 train_loss: 0.625 train_f1: 0.170 \t\n",
      "Early stopped training due to non-improved performance\n",
      "2020-07-26 03:09:50.511529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1511: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    }
   ],
   "source": [
    "# DISP DATETIME FOR CHECKING TIME\n",
    "print(dt.datetime.now())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "patience = 50\n",
    "gammas = [0.01, 0.1, 1, 10, 100] # finished: 0, 0.001, \n",
    "for gamma in gammas:\n",
    "    train_loss_sum=[]\n",
    "    train_f1_sum=[]\n",
    "    val_acc_sum=[]\n",
    "    val_f1_sum=[]\n",
    "    val_f1_classes=[]\n",
    "    val_f1_min = 0\n",
    "    results_directory = os.path.join(rootdir, 'results_'+date+'_e2e_ABN_zero_gamma' + str(gamma))\n",
    "    print(results_directory)\n",
    "    if not os.path.isdir(results_directory):\n",
    "        os.mkdir(results_directory)\n",
    "    model, edit_loss = endtoend_edit_ABN_model_loss((None,12), len(unique_classes), minimum_len,n=1, gamma = gamma)\n",
    "    model.compile(loss=loss_function,\n",
    "              optimizer=optimizers.Adam(lr=1e-5),           \n",
    "              metrics=[score_f1])\n",
    "    model.summary()\n",
    "    for num_epoch in range(epochs):\n",
    "    #     num_epoch += 32 # 중단된 코드 돌리기 위해 임의로 사용\n",
    "        random.shuffle(data_train)\n",
    "        train_loss, train_f1 = train(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final, model)\n",
    "    #     train_loss, train_f1 = train_edit(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final, model)\n",
    "\n",
    "        train_loss_sum.append(train_loss)\n",
    "        train_f1_sum.append(train_f1)\n",
    "\n",
    "        print('\\nEpoch',num_epoch,'train_loss:',f'{train_loss:.3f}','train_f1:',f'{train_f1:.3f}',\"\\t\")\n",
    "        model_output = \"ECG_ABN_E%02dL%.2f\" % (num_epoch, train_loss)\n",
    "        save_name = os.path.join(results_directory, model_output)\n",
    "\n",
    "    #     val_acc, f1_classes, f1,_,_,_,_,_,_ = test(data_val, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)\n",
    "    #     val_acc, f1_classes, f1,_,_,_ = test_edit(data_val, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)\n",
    "        val_acc, f1_classes, f1,_,_,_,_,_,_ = test_zero(data_val, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)    \n",
    "\n",
    "\n",
    "        val_acc_sum.append(val_acc)\n",
    "        val_f1_sum.append(f1)\n",
    "        val_f1_classes.append(f1_classes)\n",
    "\n",
    "        if f1 > val_f1_min:\n",
    "            val_f1_min = f1\n",
    "            earlystop = 0\n",
    "            if val_acc > 0.6: # 너무 낮아서 어차피 안쓸것들은 제외하기 위한 것\n",
    "                model.save_weights(save_name.format(epoch=0))\n",
    "        else: \n",
    "            earlystop += 1\n",
    "            if earlystop > patience: \n",
    "                print(\"Early stopped training due to non-improved performance\")\n",
    "                break\n",
    "\n",
    "        print('\\nValidation', num_epoch, 'valid_f1:',f'{f1:.3f}', 'best_f1:',f'{val_f1_min:.3f}', 'mean accuracy:' f'{val_acc:.3f}',\"\\t\")\n",
    "    #     print('\\nValidation mean accuracy: ', f'{val_acc:.3f}', \"\\t\")\n",
    "    #     print('\\nValidation mean f1 by classes: ',f1_classes, \"\\t\")\n",
    "\n",
    "    np.save(os.path.join(results_directory, 'train_loss_sum'), train_loss_sum)\n",
    "    np.save(os.path.join(results_directory, 'train_f1_sum'), train_f1_sum)\n",
    "    np.save(os.path.join(results_directory, 'val_acc_sum'), val_acc_sum)\n",
    "    np.save(os.path.join(results_directory, 'val_f1_sum'), val_f1_sum)\n",
    "    np.save(os.path.join(results_directory, 'val_f1_classes'), val_f1_classes)\n",
    "    del model\n",
    "    del edit_loss\n",
    "    print(dt.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_directory=os.path.join(rootdir, 'results_20200720_just_ABN_zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.001\n",
    "results_directory = os.path.join(rootdir, 'results_'+date+'_e2e_ABN_zero_gamma' + str(gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f2a21f3d908>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gamma = 0.0001\n",
    "# test_model = ABN_model((None,12), len(unique_classes), minimum_len, n=1)\n",
    "# test_model = primitive_ABN((None,12), len(unique_classes), minimum_len, n=1)\n",
    "# test_model = edit_ABN_model((None,12), len(unique_classes), minimum_len, n=1)\n",
    "# test_model, _ = edit_ABN_model_loss((None,12), len(unique_classes), minimum_len,n=1, gamma = gamma)\n",
    "test_model,_ = endtoend_edit_ABN_model_loss((None,12), len(unique_classes), minimum_len,n=1, gamma = gamma)\n",
    "\n",
    "# results_directory = os.path.join(rootdir, 'results_20200626_V4_ABN_primitive_editL1_gamma0.0001_primitive_updated')\n",
    "# results_directory = os.path.join(rootdir, 'results_20200622_ABN_multiclass_V4_ABN')\n",
    "# results_directory = os.path.join(rootdir, 'results_20200622_ABN_multiclass_V4_ABN_edit')\n",
    "# results_directory = os.path.join(rootdir, 'results_'+date+'_ABN_primitive_n=7')\n",
    "# results_directory = os.path.join(rootdir, 'results_20200626_V4_ABN_primitive_editL2_gamma0.0001_primitive_updated')\n",
    "# results_directory = os.path.join(rootdir, 'results_20200709_ABN_dense512')\n",
    "# test_model =  Model(inputs=main_input, outputs=main_output)\n",
    "# test_model.load_weights(os.path.join(results_directory, 'ecg_mel_E829L0.17'))\n",
    "# results_directory = os.path.join(rootdir, 'results_20200624_V4_ABN_edit_loss')\n",
    "\n",
    "\n",
    "# test_model.load_weights(os.path.join(results_directory, 'ECG_ABN_E227L0.04'))\n",
    "results_directory = os.path.join(rootdir, 'results_20200724_e2e_ABN_zero_gamma0.01')\n",
    "latest_test = tf.train.latest_checkpoint(results_directory)\n",
    "test_model.load_weights(latest_test)\n",
    "# test_model.load_weights(os.path.join(results_directory, 'ECG_ABN_E220L0.32'))\n",
    "# test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_acc, f1_classes, f1, m_acc, m_f1, m_sum, s_acc, s_f1, s_sum = test(data_test, mel_directory, input_directory, class2index, minimum_len, test_model, x_mean_final, x_std_final)\n",
    "# test_acc, f1_classes, f1, m_acc, m_f1, m_sum, s_acc, s_f1, s_sum = test_edit_final(data_test, mel_directory, input_directory, class2index, minimum_len, test_model, x_mean_final, x_std_final, p_model)\n",
    "test_acc, f1_classes, f1,m_acc, m_f1, m_sum, s_acc, s_f1, s_sum = test_zero(data_val, mel_directory, input_directory, class2index, minimum_len, test_model, x_mean_final, x_std_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../results_20200724_e2e_ABN_zero_gamma0.01/ECG_ABN_E230L0.32'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.732"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(test_acc, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94 , 0.814, 0.963, 0.691, 0.5  , 0.741, 0.892, 0.8  , 0.6  ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_classes.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AF': 0,\n",
       " 'I-AVB': 1,\n",
       " 'LBBB': 2,\n",
       " 'Normal': 3,\n",
       " 'PAC': 4,\n",
       " 'PVC': 5,\n",
       " 'RBBB': 6,\n",
       " 'STD': 7,\n",
       " 'STE': 8}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.809"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.895"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(m_acc, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.   , 0.   , 1.   , 0.   , 1.   , 1.   , 0.957, 0.909, 1.   ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_f1.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.973"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_sum.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(s_acc, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.925, 0.814, 0.957, 0.691, 0.458, 0.682, 0.879, 0.781, 0.5  ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_f1.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.785"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_sum.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "617.813px",
    "left": "1545.27px",
    "right": "20px",
    "top": "80.75px",
    "width": "296.719px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
