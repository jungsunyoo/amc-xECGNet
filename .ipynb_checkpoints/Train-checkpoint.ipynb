{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import datetime as dt\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set TF random seed to improve reproducibility\n",
    "tf.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "batch_size = 32\n",
    "minimum_len = 128\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#currdir= os.getcwd()\n",
    "rootdir = '/home/taejoon/PhysioNetChallenge'\n",
    "input_directory = os.path.join(rootdir, 'Training_WFDB')\n",
    "mel_name = 'Mel_data_20200402_128' \n",
    "mel_directory = os.path.join(rootdir, mel_name)\n",
    "#save_directory = os.path.join(currdir, '')\n",
    "if not os.path.isdir(input_directory):\n",
    "        os.mkdir(input_directory)\n",
    "if not os.path.isdir(mel_directory):\n",
    "        os.mkdir(mel_directory)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find files\n",
    "input_files = []\n",
    "for f in os.listdir(input_directory):\n",
    "    if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('mat'):\n",
    "        input_files.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_names = sorted(input_files)\n",
    "input_file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and divide files into train/eval/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, data_test = train_test_split(input_file_names, test_size = 0.2, train_size = 0.8, shuffle=True)\n",
    "data_train, data_val = train_test_split(data, test_size = 0.25, train_size = 0.75, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(data_train), np.shape(data_val), np.shape(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess labels (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique number of classes  \n",
    "def get_unique_classes(input_directory,files):\n",
    "\n",
    "    unique_classes=set()\n",
    "    for f in files:\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    for c in tmp:\n",
    "                        unique_classes.add(c.strip())\n",
    "\n",
    "    return sorted(unique_classes)\n",
    "\n",
    "unique_classes = get_unique_classes(input_directory, input_files)\n",
    "# Creating one-hot vector for Y\n",
    "# num = np.unique(classes, axis=0)\n",
    "class2index = {}\n",
    "for a, b in enumerate(unique_classes):\n",
    "    class2index[b] = a\n",
    "#class2index\n",
    "\n",
    "def one_hot_encoding(y, class2index):\n",
    "       one_hot_vector = [0]*(len(class2index))\n",
    "       ind=class2index[y]\n",
    "       one_hot_vector[ind]=1\n",
    "       return one_hot_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checkinc which x is minimum\n",
    "# minimum = 300\n",
    "# for file in input_file_names:\n",
    "#     tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "#     print(np.shape(tmp_file))\n",
    "#     if len(tmp_file) < minimum:\n",
    "#         minimum = tmp_file.shape[0]\n",
    "# #print(minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes= np.asarray(classes)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mel_files = np.asarray(mel_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# dataset = dataset.batch(batch_size)\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, x_test, y, y_test  = train_test_split(mel_files, classes, test_size=0.2, train_size = 0.8)\n",
    "# x_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.25, train_size = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(np.shape(x_train), np.shape(x_val), np.shape(x_test), np.shape(y_train), np.shape(y_val), np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(minimum_len, minimum_len, 12)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(9, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_feature(sequence_en, minimum_len): \n",
    "    new_en = []\n",
    "    if len(sequence_en) > minimum_len:  # 길이가 minimum보다 긴 경우\n",
    "        start = random.randint(0,len(sequence_en)-minimum_len)    \n",
    "        new_en = sequence_en[start:start+minimum_len]\n",
    "    elif len(sequence_en) == minimum_len: # 길이가 minimum\n",
    "        new_en = sequence_en\n",
    "    else: \n",
    "        assert len(sequence_en) <= minimum_len\n",
    "    return new_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classes of sorted file names\n",
    "def get_labels(input_directory,file, class2index):\n",
    "    \n",
    "    classes = []\n",
    "\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                for c in tmp:\n",
    "                    curr_label = one_hot_encoding(c.strip(), class2index)\n",
    "                classes.append(curr_label)\n",
    "\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randextract_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len): # step = 0, 1, 2, 3....\n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    start = batch_size*curr_step\n",
    "    end = batch_size*(curr_step+1)-1\n",
    "    curr_file_indices = data_train[start:end]\n",
    "    for file in curr_file_indices:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        clip_file = block_feature(tmp_file, minimum_len)\n",
    "        mel_files.append(clip_file)\n",
    "        \n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        classes.append(label)\n",
    "    return mel_files, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randextract_mels_val(curr_range_start, curr_range_end, data_val, mel_directory, class2index, minimum_len): # step = 0, 1, 2, 3....\n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    start = curr_range_start\n",
    "    end = curr_range_end\n",
    "    start = start.astype(int)\n",
    "    end = end.astype(int)\n",
    "    curr_file_indices = data_val[start:end]\n",
    "    for file in curr_file_indices:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        clip_file = block_feature(tmp_file, minimum_len)\n",
    "        mel_files.append(clip_file)\n",
    "        \n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        classes.append(label)\n",
    "    return mel_files, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_train, mel_directory, batch_size, class2index, minimum_len): \n",
    "    \n",
    "    loss=[]\n",
    "    acc = []\n",
    "\n",
    "    total_steps = int(np.ceil(len(data_train)/batch_size))\n",
    "    for curr_step in range(total_steps):\n",
    "        batch_mels, batch_labels = randextract_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len)\n",
    "        batch_mels = np.asarray(batch_mels)\n",
    "        batch_labels = np.asarray(np.squeeze(batch_labels))\n",
    "        train_loss_tmp = model.train_on_batch(batch_mels, batch_labels)\n",
    "        loss.append(train_loss_tmp[0])\n",
    "        acc.append(train_loss_tmp[1])\n",
    "\n",
    "    loss = np.mean(np.array(loss))\n",
    "    acc = np.mean(np.array(acc))\n",
    "#     metrics = np.mean(np.array(metrics))\n",
    "#     return metrics\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(ct, data_val, mel_directory, class2index, minimum_len, epochs): \n",
    "    loss = []\n",
    "    acc = []\n",
    "    \n",
    "    per_epoch = epochs/20 # how many validation sets we need: divide total epochs (1000) by 20\n",
    "    per_val = np.floor(len(data_val)/per_epoch)\n",
    "    curr_range_start = (ct-1)*per_val\n",
    "    curr_range_end = ct*per_val - 1\n",
    "    batch_mels, batch_labels = randextract_mels_val(curr_range_start, curr_range_end, data_val, mel_directory, class2index, minimum_len)\n",
    "    batch_mels = np.asarray(batch_mels)\n",
    "    batch_labels = np.asarray(np.squeeze(batch_labels))\n",
    "    val_loss_tmp = model.test_on_batch(batch_mels, batch_labels)\n",
    "    loss.append(val_loss_tmp[0])\n",
    "    acc.append(val_loss_tmp[1])\n",
    "#     metrics.append(val_loss_tmp)\n",
    "#     metrics = np.mean(np.array(metrics))\n",
    "#     return metrics\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ct = 0\n",
    "for num_epoch in range(epochs):\n",
    "    if (num_epoch+1)%20 == 0: # Validation for every 20 epochs\n",
    "        ct += 1\n",
    "        val_loss, val_acc = validation(ct,data_val, mel_directory, class2index, minimum_len, epochs)\n",
    "        curr = (num_epoch+1)/20\n",
    "        print('\\nValidation', curr.astype(int),'valid_loss:',f'{valid_loss:.3f}', 'valid_acc:',f'{valid_acc:.3f}',\"\\t\", dt.datetime.now())\n",
    "    else: \n",
    "        train_loss, train_acc = train(data_train, mel_directory, batch_size, class2index, minimum_len)\n",
    "        print('\\nEpoch',num_epoch+1,'train_loss:',f'{train_loss:.3f}','train_acc:',f'{train_acc:.3f}',\"\\t\", dt.datetime.now())\n",
    "    \n",
    "    model.save('MEL.h5')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, test with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_test, mel_directory, class2index, minimum_len): \n",
    "    \n",
    "    metrics = []\n",
    "    batch_mels, batch_labels = randextract_mels_val(0, len(data_test)-1, data_val, mel_directory, class2index, minimum_len)\n",
    "    # although rendextract_mels_val, you can use the same function fpr test\n",
    "    batch_mels = np.asarray(batch_mels)\n",
    "    batch_labels = np.asarray(np.squeeze(batch_labels))\n",
    "    test_loss_tmp = model.test_on_batch(batch_mels, batch_labels)\n",
    "    metrics.append(test_loss_tmp)\n",
    "\n",
    "    metrics = np.mean(np.array(metrics))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = test(data_test, mel_directory, class2index, minimum_len)\n",
    "print('\\nTest result: test_metrics:',f'{test_metrics:.3f}',\"\\t\", dt.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(\n",
    "#       mel_files, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import optimizers\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "#               metrics=['acc'])\n",
    "# nepochs=1000\n",
    "# model.fit(x_train, y_train, batch_size=batch_size, epochs=nepochs, validation_data=(x_val, y_val), verbose=2)\n",
    "# model.save('ECG1.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 481,
   "position": {
    "height": "503px",
    "left": "1544px",
    "right": "20px",
    "top": "247px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
