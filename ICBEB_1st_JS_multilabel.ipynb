{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, GRU, TimeDistributed, Bidirectional, LeakyReLU\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten,  Input, Reshape, GRU, CuDNNGRU\n",
    "from keras.layers import Convolution1D, MaxPool1D, GlobalAveragePooling1D,concatenate,AveragePooling1D\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numba/errors.py:137: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0012702   0.00038759  0.00402317 -0.0029567   0.00051957  0.00116427\n",
      " -0.0008405  -0.00222516 -0.0019119  -0.00093502  0.00129789  0.00177598]\n",
      "(5225,) (276,) (1376,)\n",
      "A2355.mat A5949.mat A0349.mat\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 72000, 12)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 72000, 12)         444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 72000, 12)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 72000, 12)         444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 72000, 12)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 36000, 12)         3468      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 36000, 12)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 36000, 12)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 36000, 12)         444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 36000, 12)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 36000, 12)         444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 36000, 12)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 18000, 12)         3468      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 18000, 12)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 18000, 12)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 18000, 12)         444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 18000, 12)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 18000, 12)         444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 18000, 12)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 9000, 12)          3468      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 9000, 12)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 9000, 12)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 9000, 12)          444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 9000, 12)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 9000, 12)          444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 9000, 12)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 4500, 12)          3468      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 4500, 12)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4500, 12)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 4500, 12)          444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 4500, 12)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 4500, 12)          444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 4500, 12)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 2250, 12)          6924      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 2250, 12)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2250, 12)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 2250, 24)          1872      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 2250, 24)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2250, 24)          0         \n",
      "_________________________________________________________________\n",
      "attention_with_context_1 (At (None, 24)                624       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 24)                96        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 9)                 225       \n",
      "=================================================================\n",
      "Total params: 28,053\n",
      "Trainable params: 28,005\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import keras\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras.applications.densenet import DenseNet121, DenseNet169\n",
    "#from keras.applications.nasnet import NASNetLarge\n",
    "# from keras_efficientnets import EfficientNetB7\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers.normalization import BatchNormalization \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from keras import backend as K\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "random.seed(100)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "def score_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "class AttentionWithContext(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "            self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "        a = K.exp(ait)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "\n",
    "    \n",
    "def cce_f1_loss(y_true, y_pred):\n",
    "    return 1 + 0.1*keras.losses.categorical_crossentropy(y_true, y_pred) - keras.metrics.categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "# Find unique number of classes  \n",
    "def get_unique_classes(input_directory,files):\n",
    "\n",
    "    unique_classes=set()\n",
    "    for f in files:\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    for c in tmp:\n",
    "                        unique_classes.add(c.strip())\n",
    "    return sorted(unique_classes)\n",
    "\n",
    "def one_hot_encoding(one_hot_vector,y, class2index):\n",
    "    ind=class2index[y]\n",
    "    one_hot_vector[ind]=1\n",
    "    return one_hot_vector\n",
    "\n",
    "# Search for multi-label subjects\n",
    "def searching_overlap(input_directory,class2index, input_file_names):\n",
    "    multiclasses=[]\n",
    "    multisubjects=[]\n",
    "    number = []\n",
    "    for file in input_file_names:\n",
    "        f=file\n",
    "        g = f.replace('.mat','.hea')\n",
    "        input_file = os.path.join(input_directory,g)\n",
    "        with open(input_file,'r') as f:\n",
    "            for lines in f:\n",
    "                if lines.startswith('#Dx'):\n",
    "                    tmp = lines.split(': ')[1].split(',')\n",
    "                    if len(tmp)>1:\n",
    "                        one_hot_vector = [0]*(len(class2index))\n",
    "                        for c in tmp:\n",
    "                            one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                        multiclasses.append(one_hot_vector)\n",
    "                        multisubjects.append(g)\n",
    "                        number.append(len(tmp))\n",
    "    return multisubjects, multiclasses, number\n",
    "\n",
    "def block_feature(sequence_en, minimum_len): \n",
    "    new_en = []\n",
    "    if len(sequence_en) > minimum_len:  # 길이가 minimum보다 긴 경우\n",
    "        start = random.randint(0,len(sequence_en)-minimum_len)\n",
    "        #print(start)\n",
    "        new_en = sequence_en[start:start+minimum_len]\n",
    "    elif len(sequence_en) == minimum_len: # 길이가 minimum\n",
    "        new_en = sequence_en\n",
    "    else: \n",
    "        assert len(sequence_en) <= minimum_len\n",
    "    return new_en\n",
    "\n",
    "def exploratory_look(input_directory,file, class2index):\n",
    "    classes = []\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                print(tmp, len(tmp))\n",
    "    return tmp     \n",
    "\n",
    "# Get classes of sorted file names\n",
    "def get_labels(input_directory,file, class2index):\n",
    "    f = file\n",
    "    g = f.replace('.mat','.hea')\n",
    "    input_file = os.path.join(input_directory,g)\n",
    "    with open(input_file,'r') as f:\n",
    "        for lines in f:\n",
    "            if lines.startswith('#Dx'):\n",
    "                tmp = lines.split(': ')[1].split(',')\n",
    "                one_hot_vector = [0]*(len(class2index))\n",
    "                for c in tmp:\n",
    "                    one_hot_vector = one_hot_encoding(one_hot_vector, c.strip(), class2index)\n",
    "                \n",
    "    return one_hot_vector\n",
    "\n",
    "def randextract_mels(curr_step, batch_size, data, mel_directory, class2index, minimum_len, x_mean_final, x_std_final):\n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    start = batch_size*curr_step\n",
    "    end = batch_size*(curr_step+1)\n",
    "    curr_file_indices = data[start:end]\n",
    "    for file in curr_file_indices:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        clip_file = block_feature(tmp_file, minimum_len)\n",
    "        #print(clip_file.shape)\n",
    "        #clip_file = tmp_file[:minimum_len]\n",
    "        clip_file -= x_mean_final\n",
    "        clip_file /= x_std_final\n",
    "        mel_files.append(clip_file)\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        classes.append(label)\n",
    "    concat = list(zip(mel_files, classes))\n",
    "    random.shuffle(concat)\n",
    "    mel_files, classes = zip(*concat)\n",
    "    return mel_files, classes\n",
    "\n",
    "def zeropadding_mels(curr_step, batch_size, data, mel_directory, class2index, minimum_len, x_mean_final, x_std_final):\n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    start = batch_size*curr_step\n",
    "    end = batch_size*(curr_step+1)\n",
    "    curr_file_indices = data[start:end]    \n",
    "    for file in curr_file_indices:\n",
    "        \n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        tmp_file -= x_mean_final\n",
    "        tmp_file /= x_std_final        \n",
    "#         print(tmp_file.shape)\n",
    "        zero_padding = np.zeros((minimum_len-len(tmp_file), 12))\n",
    "#         print(zero_padding.shape)\n",
    "        clip_file = np.concatenate((zero_padding, tmp_file), axis=0)\n",
    "#         print(clip_file)\n",
    "#         clip_file = zero_padding + tmp_file\n",
    "#         print(clip_file.shape)\n",
    "#         clip_file = block_feature(tmp_file, minimum_len)\n",
    "        #print(clip_file.shape)\n",
    "        #clip_file = tmp_file[:minimum_len]\n",
    "\n",
    "        mel_files.append(clip_file)\n",
    "        label = get_labels(input_directory, file, class2index)\n",
    "        classes.append(label)\n",
    "    concat = list(zip(mel_files, classes))\n",
    "    random.shuffle(concat)\n",
    "    mel_files, classes = zip(*concat)\n",
    "    return mel_files, classes    \n",
    "\n",
    "def train(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final): \n",
    "    loss=[]\n",
    "    acc = []\n",
    "    f1 = []\n",
    "\n",
    "    total_steps = int(np.ceil(len(data_train)/batch_size))\n",
    "    for curr_step in range(total_steps):\n",
    "        batch_mels, batch_labels = zeropadding_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "#         batch_mels, batch_labels = randextract_mels(curr_step, batch_size, data_train, mel_directory, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "        batch_mels = np.asarray(batch_mels)\n",
    "        batch_labels = np.asarray(np.squeeze(batch_labels))\n",
    "        train_tmp = model.train_on_batch(batch_mels, batch_labels)\n",
    "        loss.append(train_tmp[0])\n",
    "        acc.append(train_tmp[1])\n",
    "        f1.append(train_tmp[2])\n",
    "\n",
    "    loss = np.mean(np.array(loss))\n",
    "    acc = np.mean(np.array(acc))\n",
    "    f1 = np.mean(np.array(f1))\n",
    "    return loss, acc, f1\n",
    "\n",
    "def test(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    y_true = []\n",
    "    y_pred=[]    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    \n",
    "    \n",
    "    multi_files, _, _ = searching_overlap(input_directory,class2index, data)\n",
    "    multi_acc = 0\n",
    "    multi_y_true=[]\n",
    "    multi_y_pred=[]       \n",
    "    single_acc = 0\n",
    "    single_y_true=[]\n",
    "    single_y_pred=[]   \n",
    "    \n",
    "    for file in data:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "        for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            start = block*minimum_len\n",
    "            end = (block+1)*minimum_len\n",
    "            clip_file = tmp_file[start:end]\n",
    "            clip_file -= x_mean_final\n",
    "            clip_file /= x_std_final\n",
    "            mel_files.append(clip_file)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "        logit = model.predict(mel_files)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        pred = np.zeros(len(logit))\n",
    "        for ii, label in enumerate(logit):\n",
    "            if label >= 0.5:\n",
    "                pred[ii] = 1\n",
    "            else:\n",
    "                pred[ii] = 0\n",
    "        pred = pred.tolist()\n",
    "        y_pred.append(pred)        \n",
    "        label = get_labels(input_directory,file,class2index)\n",
    "        y_true.append(label)\n",
    "        \n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        if file.replace('.mat', '.hea') in multi_files:\n",
    "            multi_y_true.append(label)            \n",
    "            multi_y_pred.append(pred)\n",
    "            multi_acc += acc   \n",
    "        else: # for calculating single_label accuracy\n",
    "            single_y_true.append(label)            \n",
    "            single_y_pred.append(pred)\n",
    "            single_acc += acc         \n",
    "        #total_f1 += f1\n",
    "    final_acc = total_acc / len(data)\n",
    "    f1_classes = f1_score(y_true, y_pred, average=None)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    \n",
    "    multi_final_acc = multi_acc / len(multi_files)\n",
    "    multi_f1_classes = f1_score(multi_y_true, multi_y_pred, average=None)\n",
    "    multi_f1_micro = f1_score(multi_y_true, multi_y_pred, average='micro')\n",
    "    \n",
    "    single_final_acc = single_acc / (len (data) - len(multi_files))\n",
    "    single_f1_classes = f1_score(single_y_true, single_y_pred, average=None)\n",
    "    single_f1_micro = f1_score(single_y_true, single_y_pred, average='micro')    \n",
    "    \n",
    "    return final_acc, f1_classes, f1_micro, multi_final_acc, multi_f1_classes, multi_f1_micro, single_final_acc, single_f1_classes, single_f1_micro\n",
    "def test_zero(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    y_true = []\n",
    "    y_pred=[]    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    \n",
    "    \n",
    "    multi_files, _, _ = searching_overlap(input_directory,class2index, data)\n",
    "    multi_acc = 0\n",
    "    multi_y_true=[]\n",
    "    multi_y_pred=[]    \n",
    "    single_acc = 0\n",
    "    single_y_true=[]\n",
    "    single_y_pred=[]  \n",
    "    for file in data:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        tmp_file -= x_mean_final\n",
    "        tmp_file /= x_std_final\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "#         for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            \n",
    "        zero_padding = np.zeros((minimum_len-len(tmp_file), 12))\n",
    "        clip_file = np.concatenate((zero_padding, tmp_file), axis=0)\n",
    "#         print(clip_file.shape)\n",
    "        \n",
    "\n",
    "        mel_files.append(clip_file)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "        logit = model.predict(mel_files)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        pred = np.zeros(len(logit))\n",
    "        for ii, label in enumerate(logit):\n",
    "            if label >= 0.5:\n",
    "                pred[ii] = 1\n",
    "            else:\n",
    "                pred[ii] = 0\n",
    "        pred = pred.tolist()\n",
    "        y_pred.append(pred)        \n",
    "        label = get_labels(input_directory,file,class2index)\n",
    "        y_true.append(label)\n",
    "        \n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        if file.replace('.mat', '.hea') in multi_files:\n",
    "            multi_y_true.append(label)            \n",
    "            multi_y_pred.append(pred)\n",
    "            multi_acc += acc         \n",
    "        else: # for calculating single_label accuracy\n",
    "            single_y_true.append(label)            \n",
    "            single_y_pred.append(pred)\n",
    "            single_acc += acc         \n",
    "        #total_f1 += f1\n",
    "    final_acc = total_acc / len(data)\n",
    "    f1_classes = f1_score(y_true, y_pred, average=None)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    \n",
    "    multi_final_acc = multi_acc / len(multi_files)\n",
    "    multi_f1_classes = f1_score(multi_y_true, multi_y_pred, average=None)\n",
    "    multi_f1_micro = f1_score(multi_y_true, multi_y_pred, average='micro')\n",
    "    \n",
    "    single_final_acc = single_acc / (len (data) - len(multi_files))\n",
    "    single_f1_classes = f1_score(single_y_true, single_y_pred, average=None)\n",
    "    single_f1_micro = f1_score(single_y_true, single_y_pred, average='micro')    \n",
    "    \n",
    "    return final_acc, f1_classes, f1_micro, multi_final_acc, multi_f1_classes, multi_f1_micro, single_final_acc, single_f1_classes, single_f1_micro\n",
    "\n",
    "def test_zero_magic(data, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final, magic):\n",
    "    scores = []\n",
    "    predicted_labels=[]\n",
    "    accuracy=np.zeros(len(data))\n",
    "    #total_loss=[]\n",
    "    total_acc = 0\n",
    "    total_f1 = 0\n",
    "    y_true = []\n",
    "    y_pred=[]    \n",
    "    mel_files = []\n",
    "    classes = []\n",
    "    \n",
    "    \n",
    "    multi_files, _, _ = searching_overlap(input_directory,class2index, data)\n",
    "    multi_acc = 0\n",
    "    multi_y_true=[]\n",
    "    multi_y_pred=[]    \n",
    "    single_acc = 0\n",
    "    single_y_true=[]\n",
    "    single_y_pred=[]  \n",
    "    for file in data:\n",
    "        tmp_file = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "        tmp_file -= x_mean_final\n",
    "        tmp_file /= x_std_final\n",
    "        steps = int(np.floor(tmp_file.shape[0]/minimum_len))\n",
    "        mel_files = []\n",
    "#         for block in range(steps): # 128개씩 쪼갠 블럭 단위로 predict\n",
    "            \n",
    "        zero_padding = np.zeros((minimum_len-len(tmp_file), 12))\n",
    "        clip_file = np.concatenate((zero_padding, tmp_file), axis=0)\n",
    "#         print(clip_file.shape)\n",
    "        \n",
    "\n",
    "        mel_files.append(clip_file)\n",
    "        mel_files = np.asarray(mel_files)\n",
    "        logit = model.predict(mel_files)\n",
    "        logit = np.mean(logit, axis=0)\n",
    "#         print(logit)\n",
    "        logit = logit * magic\n",
    "#         print(logit)\n",
    "        \n",
    "        pred = np.zeros(len(logit))\n",
    "#         print(pred)\n",
    "        for ii, label in enumerate(logit):\n",
    "            if label >= 0.5:\n",
    "                pred[ii] = 1\n",
    "            else:\n",
    "                pred[ii] = 0\n",
    "        pred = pred.tolist()\n",
    "        y_pred.append(pred)        \n",
    "        label = get_labels(input_directory,file,class2index)\n",
    "        y_true.append(label)\n",
    "        \n",
    "        if pred == label:\n",
    "            acc = 1\n",
    "        else:\n",
    "            acc = 0\n",
    "        total_acc += acc\n",
    "        if file.replace('.mat', '.hea') in multi_files:\n",
    "            multi_y_true.append(label)            \n",
    "            multi_y_pred.append(pred)\n",
    "            multi_acc += acc         \n",
    "        else: # for calculating single_label accuracy\n",
    "            single_y_true.append(label)            \n",
    "            single_y_pred.append(pred)\n",
    "            single_acc += acc         \n",
    "        #total_f1 += f1\n",
    "    final_acc = total_acc / len(data)\n",
    "    f1_classes = f1_score(y_true, y_pred, average=None)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    \n",
    "    multi_final_acc = multi_acc / len(multi_files)\n",
    "    multi_f1_classes = f1_score(multi_y_true, multi_y_pred, average=None)\n",
    "    multi_f1_micro = f1_score(multi_y_true, multi_y_pred, average='micro')\n",
    "    \n",
    "    single_final_acc = single_acc / (len (data) - len(multi_files))\n",
    "    single_f1_classes = f1_score(single_y_true, single_y_pred, average=None)\n",
    "    single_f1_micro = f1_score(single_y_true, single_y_pred, average='micro')    \n",
    "    \n",
    "    return final_acc, f1_classes, f1_micro, multi_final_acc, multi_f1_classes, multi_f1_micro, single_final_acc, single_f1_classes, single_f1_micro\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "minimum_len = 72000 #2880\n",
    "epochs = 1000\n",
    "# loss_function = 'binary_crossentropy'\n",
    "loss_function = 'categorical_crossentropy'\n",
    "# activation_function = 'softmax'\n",
    "rootdir = '../'\n",
    "date = datetime.today().strftime(\"%Y%m%d\")\n",
    "input_directory = os.path.join(rootdir, 'Training_WFDB')\n",
    "mel_name = 'Raw_data_20200424' \n",
    "mel_directory = os.path.join(rootdir, mel_name)\n",
    "results_directory = os.path.join(rootdir, 'results_'+date+'1stModel')\n",
    "if not os.path.isdir(input_directory):\n",
    "    os.mkdir(input_directory)\n",
    "if not os.path.isdir(mel_directory):\n",
    "    os.mkdir(mel_directory)\n",
    "if not os.path.isdir(results_directory):\n",
    "    os.mkdir(results_directory)\n",
    "        \n",
    "input_files = []\n",
    "for f in os.listdir(input_directory):\n",
    "    if os.path.isfile(os.path.join(input_directory, f)) and not f.lower().startswith('.') and f.lower().endswith('mat'):\n",
    "        input_files.append(f)\n",
    "input_file_names = sorted(input_files)\n",
    "\n",
    "unique_classes = get_unique_classes(input_directory, input_files)\n",
    "class2index = {}\n",
    "for a, b in enumerate(unique_classes):\n",
    "    class2index[b] = a\n",
    "    \n",
    "# classes_orig= [x.replace('.mat', '.hea') for x in input_file_names] # total subjects\n",
    "# classes_multi, _, _ = searching_overlap(input_directory,class2index, input_file_names)\n",
    "# classes_single = [x for x in classes_orig if x not in classes_multi]\n",
    "# classes_single = [x.replace('.hea', '.mat') for x in classes_single]\n",
    "\n",
    "# double-checking if classes_single have single-label\n",
    "# a, b, c  = searching_overlap(input_directory,class2index,classes_single)\n",
    "\n",
    "# we can safely use classes_single as input_file_names\n",
    "# input_file_names = classes_single\n",
    "\n",
    "np.shape(input_file_names)\n",
    "\n",
    "x_mean_all = []\n",
    "x_std_all = []\n",
    "for file in input_file_names:\n",
    "    x = np.load(mel_directory + '/' + file.replace('.mat', '.npy'))\n",
    "    x_mean = [np.mean(x[:,0]), np.mean(x[:,1]), np.mean(x[:,2]), np.mean(x[:,3]), np.mean(x[:,4]), np.mean(x[:,5]),\n",
    "             np.mean(x[:,6]), np.mean(x[:,7]), np.mean(x[:,8]), np.mean(x[:,9]), np.mean(x[:,10]), np.mean(x[:,11])]\n",
    "    \n",
    "    x_std = [np.std(x[:,0]), np.std(x[:,1]), np.std(x[:,2]), np.std(x[:,3]), np.std(x[:,4]), np.std(x[:,5]),\n",
    "             np.std(x[:,6]), np.std(x[:,7]), np.std(x[:,8]), np.std(x[:,9]), np.std(x[:,10]), np.std(x[:,11])]\n",
    "    #print(x_mean)\n",
    "    x_mean_all.append(x_mean)\n",
    "    x_std_all.append(x_std)\n",
    "x_mean_final = np.mean(x_mean_all, axis=0)\n",
    "x_std_final = np.mean(x_std_all, axis=0)\n",
    "print(x_mean_final)\n",
    "\n",
    "data, data_test = train_test_split(input_file_names, test_size = 0.2, train_size = 0.8, shuffle=True, random_state=1004)\n",
    "data_train, data_val = train_test_split(data, test_size = 0.05, train_size = 0.95, shuffle=True, random_state=1004)\n",
    "print(np.shape(data_train), np.shape(data_val), np.shape(data_test))\n",
    "print(data_train[0], data_val[0], data_test[0])\n",
    "\n",
    "main_input = Input(shape=(minimum_len,12), dtype='float32', name='main_input')\n",
    "num_classes= 9 \n",
    "branch_pred = []\n",
    "x = Convolution1D(12, 3, padding='same')(main_input)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 3, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 24, strides = 2, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Convolution1D(12, 3, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 3, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 24, strides = 2, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Convolution1D(12, 3, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 3, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 24, strides = 2, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Convolution1D(12, 3, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 3, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 24, strides = 2, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Convolution1D(12, 3, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 3, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Convolution1D(12, 48, strides = 2, padding='same')(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "cnnout = Dropout(0.2)(x)\n",
    "x = Bidirectional(CuDNNGRU(12, input_shape=(2250,12),return_sequences=True,return_state=False))(cnnout)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = AttentionWithContext()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.3)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "main_output = Dense(num_classes,activation='sigmoid')(x)\n",
    "model = Model(inputs=main_input, outputs=main_output)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=optimizers.Adam(lr=1e-5),\n",
    "              metrics=['acc', score_f1])\n",
    "\n",
    "val_acc_sum=[]\n",
    "train_loss_sum=[]\n",
    "train_acc_sum=[]\n",
    "val_loss_sum=[]\n",
    "val_acc_min = 0\n",
    "patience = 50\n",
    "earlystop = 0\n",
    "# for num_epoch in range(epochs):\n",
    "#     random.shuffle(data_train)\n",
    "#     train_loss, train_acc, train_f1 = train(data_train, mel_directory, batch_size, class2index, minimum_len, x_mean_final, x_std_final)\n",
    "#     print('\\nEpoch',num_epoch+1,'train_loss:',f'{train_loss:.3f}','train_acc:',f'{train_acc:.3f}','train_f1:',f'{train_f1:.3f}',\"\\t\")\n",
    "#     model_output = \"ecg_mel_E%02dL%.2f\" % (num_epoch, train_loss)\n",
    "#     save_name = os.path.join(results_directory, model_output)\n",
    "#     final_acc, f1_classes, f1_micro,_,_,_ = test_zero(data_val, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)\n",
    "# #     final_acc, f1_classes, f1_micro,_,_,_ = test(data_val, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)\n",
    "#     if f1_micro > val_acc_min:\n",
    "#         val_acc_min = f1_micro\n",
    "#         earlystop = 0\n",
    "#         model.save(save_name)\n",
    "#     else: \n",
    "#         earlystop += 1 \n",
    "#         if earlystop > patience:\n",
    "#             print(\"************Early stopped training due to non-improved performance**********\")\n",
    "# #             break         \n",
    "            \n",
    "#     print('\\nValidation', num_epoch+1, 'valid_acc:',f'{f1_micro:.3f}', 'best_acc:',f'{val_acc_min:.3f}', \"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')\n",
    "# results_directory = os.path.join(rootdir, 'results_1st'+date+'_0')\n",
    "# results_directory = os.path.join(rootdir, 'results_202007011stModel')\n",
    "results_directory = os.path.join(rootdir, 'results_202006291stModel_FINAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model = edit_ABN_model((None,12), len(unique_classes), minimum_len, n=1)\n",
    "\n",
    "# results_directory = os.path.join(rootdir, 'results_20200622_ABN_multiclass_V4_primitiveABN')\n",
    "# results_directory = os.path.join(rootdir, 'results_20200622_ABN_multiclass_V4_ABN')\n",
    "# results_directory = os.path.join(rootdir, 'results_20200622_ABN_multiclass_V4_ABN_edit')\n",
    "# results_directory = os.path.join(rootdir, 'results_'+date+'_ABN_primitive_n=7')\n",
    "# latest_test = tf.train.latest_checkpoint(results_directory)\n",
    "# test_model.load_weights(latest_test)\n",
    "\n",
    "# test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_test = tf.train.latest_checkpoint(results_directory)\n",
    "latest_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 72000, 12)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 72000, 12)         444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 72000, 12)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 72000, 12)         444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 72000, 12)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 36000, 12)         3468      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 36000, 12)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 36000, 12)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 36000, 12)         444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 36000, 12)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 36000, 12)         444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 36000, 12)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 18000, 12)         3468      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 18000, 12)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 18000, 12)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 18000, 12)         444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 18000, 12)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 18000, 12)         444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 18000, 12)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 9000, 12)          3468      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 9000, 12)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 9000, 12)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 9000, 12)          444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 9000, 12)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 9000, 12)          444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 9000, 12)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 4500, 12)          3468      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 4500, 12)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4500, 12)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 4500, 12)          444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 4500, 12)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 4500, 12)          444       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 4500, 12)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 2250, 12)          6924      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 2250, 12)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2250, 12)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 2250, 24)          1872      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 2250, 24)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2250, 24)          0         \n",
      "_________________________________________________________________\n",
      "attention_with_context_1 (At (None, 24)                624       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 24)                96        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 9)                 225       \n",
      "=================================================================\n",
      "Total params: 28,053\n",
      "Trainable params: 28,005\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_model =  Model(inputs=main_input, outputs=main_output)\n",
    "# test_model.load_weights(latest_test)\n",
    "test_model.load_weights(os.path.join(results_directory, 'ecg_mel_E979L0.14'))\n",
    "\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "magicVector = np.array([19, 19, 20, 14, 30, 19, 24, 21, 39])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, f1_classes, f1, m_acc, m_f1, m_sum, s_acc, s_f1, s_sum = test_zero_magic(data_test, mel_directory, input_directory, class2index, minimum_len, test_model, x_mean_final, x_std_final, magicVector)\n",
    "# test_acc, f1_classes, f1, m_acc, m_f1, m_sum, s_acc, s_f1, s_sum = test_zero(data_test, mel_directory, input_directory, class2index, minimum_len, test_model, x_mean_final, x_std_final)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "final_acc, f1_classes, f1_micro,m_acc,m_f1,m_sum = test_zero(data_val, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)\n",
    "# final_acc, f1_classes, f1_micro,m_acc,m_f1,m_sum = test(data_val, mel_directory, input_directory, class2index, minimum_len, model, x_mean_final, x_std_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2311046511627907"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.735, 0.59 , 0.738, 0.501, 0.266, 0.61 , 0.823, 0.443, 0.159])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_classes.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5322364645111919"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0989010989010989"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.876, 0.378, 0.7  , 0.   , 0.37 , 0.52 , 0.928, 0.5  , 0.276])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_f1.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6438631790744467"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(s_acc, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.713, 0.606, 0.745, 0.505, 0.253, 0.622, 0.803, 0.44 , 0.15 ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_f1.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.520507399577167"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AF': 0,\n",
       " 'I-AVB': 1,\n",
       " 'LBBB': 2,\n",
       " 'Normal': 3,\n",
       " 'PAC': 4,\n",
       " 'PVC': 5,\n",
       " 'RBBB': 6,\n",
       " 'STD': 7,\n",
       " 'STE': 8}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "665px",
    "left": "1599px",
    "right": "20px",
    "top": "120px",
    "width": "301px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
